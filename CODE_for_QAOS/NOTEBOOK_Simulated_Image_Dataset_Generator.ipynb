{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5108d7c",
   "metadata": {},
   "source": [
    "# Generate a Grid of Matrices (labelled by $w_0,R_0,Z$) that can be used to get the CR ring for Arbitrary LP and CP \n",
    "---\n",
    "\n",
    "**Contact: oiangu9@gmail.com**\n",
    "\n",
    "In this notebook we explain how to generate a dataset of matrices that yield simulated non-noisy/theoretical conical refraction (CR) rings. \n",
    "\n",
    "**Concept**\n",
    "\n",
    "As explained in A.2 and A.3 of the bachelor thesis, the standard set-up to observe CR (where the input light beam is a cylindrically symmetric gaussian beam and a convergent lens is placed before the crystal), has explicit mathematical formulas that allow us to simulate the phenomenon as a function of some laboratory parameters (or their idelaizaitons). \n",
    "\n",
    "The parameters that allow to sweep all the possible rings are (all in pixel units):\n",
    "\n",
    "- $Z$: The theoretical distance from the camera plane to the focal plane of the CR ring.\n",
    "- $w_0$: The theoretical waist of the input beam (the diameter of the beam at the focal plane in the abscense of crystal).\n",
    "- $R_0$: The geometric radius of the ring in the focal plane.\n",
    "\n",
    "Then, as explained in A.3, using equations (37)-(42) from A.2, we can compute the intensity profile for any input polarization of light.\n",
    "\n",
    "The key point is that computing the matrices that depend on $Z,w_0,R_0$ is very computationally costly because some Bessel function computations and big numerical integrals need to be taken. Fortunately, it so happens that we only need to make explicit the polarization of the input light after this costly computations, and using the same matrix we can obtain the CR ring image for any polarization very cheaply. Therefore, the idea is to pre-compute the matrices for different $Z,w_0,R_0$ settings, store them in a hard drive and then, whenever we need them, instead of computing them, retrieve them from the storage and apply the last steps to get the image for a particular polarization.\n",
    "\n",
    "This then allows to generate cheap CR images in real time, instead of needing to wait until the integrals are computed for each $Z,w_0,R_0$ we want to check (very useful to do a real time simulation fitting etc.). With the drawback that only those combinations of $Z,w_0,R_0$ available in the precomputed dataset will be accessible.\n",
    "\n",
    "Hence, the idea is to generate a dataset of all the $Z,w_0,R_0$ we can. Certainly we cannot pre-compute them for the whole continuum of values. Instead, we will generate them for a lattice of interesting $Z,w_0,R_0$. \n",
    "\n",
    "Hereafter, we will only be interested in circularly and linearly polarized light.\n",
    "\n",
    "**Modus Operandi**\n",
    "\n",
    " - You will need to specify a box for the space of $Z,w_0,R_0$ to be considered. This box will be discretized into a lattice of $N_Z\\times N_{w_0}\\times N_{R_0}$ points (also specified by you), ideally as high as feasible. <p> \n",
    "\n",
    "\n",
    "- Then the idea is to generate a file of format `.h5` that will store the precomputation matrices (as numpy matrices), indexed by some label ID, and a `.json` file (hence, a Python dictionary), that will tell which is the `Z,w_0,R_0` combination of each label ID in the `.h5`. <p> \n",
    "\n",
    "\n",
    "- The calculations are done using the GPU (if available $-$make sure it is) and the library `jax`, which employs exactly the same syntax as numpy but uses the GPU for calculations. <p> \n",
    "\n",
    "\n",
    " We will do the calculations in parallel (if wished, otherwise just set the number of workers to 1). The parallel executions can run in the same or different computers. The way the notebook works is the following one:\n",
    " \n",
    " 1. Fix all the parameters for the lattice in Section 1.<p> \n",
    " \n",
    " 2. Choose the number of parts in which you want to split the task (so-called, the number of workers). Each worker (in the same or different computers) will compute a portion of the lattice, independently of whether the other parts have been computed or not. Note that if you put too many workers using the same GPU, it might run out of memory (the python script will let you know if such is the case).<p> \n",
    " \n",
    " 2. Make a copy of this notebook for each worker.<p> \n",
    " \n",
    " 3. For each worker, change the variable telling the script which worker it is in Section 2, and run the generation of matrices of Section 3.<p> \n",
    " \n",
    " 4. When every worker is done, move all the generated `.h5` and `.json` pairs to the same directory (using a pen drive if needed to move files from another computer). Now using a single worker notebook, run Section 4 to merge all the dataset into a full dataset.\n",
    "\n",
    "It is simpler than it seems at first read, really!\n",
    "     \n",
    "**What exactly these datasets contain and How to use them**\n",
    "\n",
    "Go to Section 5.\n",
    "     \n",
    "**What if I wanted a dataset of actual `.png` images and not the matrices?**\n",
    "\n",
    "Go to Section 6.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffbcb1e",
   "metadata": {},
   "source": [
    "## Section 1: Common Parameters for all Workers\n",
    "Set these parameters before creating the different workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d8a34da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a name to the dataset #############################################################\n",
    "experiment_name = \"test\"\n",
    "\n",
    "# Choose the directory where the dataset will be outputed\n",
    "# (if non-existent, it will be created by the script)\n",
    "output_directory = f\"./OUTPUT/\"\n",
    "\n",
    "# The simulated images will be square (length=width): #####################################\n",
    "# Choose the size of the images (and the output matrices)\n",
    "image_side = 521   # for tests use 81\n",
    "# Warning: it should be ODD (2X+1), and ideally should match \n",
    "# the short side of the camera images with which they will be compared\n",
    "\n",
    "\n",
    "# Choose the bound for the box in the R0,w0,Z space #######################################\n",
    "# (recall, they are in pixel units)\n",
    "min_R0 = 100 # for tests use 10\n",
    "max_R0 = 180 # for tests use 30\n",
    "\n",
    "min_w0 = 8  # for tests 1\n",
    "max_w0 = 40 # for tests 8\n",
    "\n",
    "min_Z = 0 \n",
    "max_Z = 0.1\n",
    "# choose the bounds for Z in the positive range, because the \n",
    "# ring at Z=a and Z=-a is the same!\n",
    "\n",
    "\n",
    "# Choose the number of points per axis in the lattice ########################################\n",
    "N_R0 = 100   # number of points along the R0 axis  \n",
    "N_w0 = 100    # number of points along the w0 axis\n",
    "N_Z = 4      # number of points in the Z axis\n",
    "# for tests use e.g. N_R0=5 N_w0=5 N_z=2\n",
    "# Note: the time to be waited and the weight of the resulting files\n",
    "# will increase exponentially as you increase any of the Ns\n",
    "\n",
    "# Choose how often you want the progess bar to be updated ###################################\n",
    "# (if too small, this will slow down the generation)\n",
    "output_info_every = 25 #many lattice points\n",
    "\n",
    "# Choose how often the generated matrices and json should be dumped to the external file ######\n",
    "# (if too small, this will slow down the generation)\n",
    "dump_every = 25 #many lattice points\n",
    "# The trick is that if we keep dumping to the external file, halting manually the calculations\n",
    "# by clicking the square button will not erase the made progress. Even if we shut down the computer,\n",
    "# just click run again to continue the generation of the dataset\n",
    "\n",
    "\n",
    "# The following need not be changed a priori ###############################################\n",
    "# Maximum frequency magnitude for the integration in the Fourier space\n",
    "max_k = 50\n",
    "# Number of points to consider for the Fourier space integral \n",
    "num_k = 1200\n",
    "# Randomization seed (unless this changes, the same random \n",
    "# decisions will be taken at each run of the script)\n",
    "randomization_seed = 666"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d08135",
   "metadata": {},
   "source": [
    "## Section 2: Tell this script which is their worker number and their worker ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b62952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am worker number 0 and out of the 200 matrices to be computed,\n",
      "I am going to compute from the 0-th one to the 67-th one.\n"
     ]
    }
   ],
   "source": [
    "# Which is the total number of workers that will compute this?\n",
    "TOTAL_WORKERS = 3\n",
    "# Who am I (which is my worker ID)?\n",
    "WORKER_ID = 0\n",
    "# Warning: begin from 0 till TOTAL_WORKERS-1\n",
    "\n",
    "################################################################\n",
    "def beg_index(N,j,K):\n",
    "    '''N: number of tasks (R0_w0_Z triplets to compute);\n",
    "       j: a worker ID;    K: total number of workers'''\n",
    "    return (N//K)*j + j*((N%K-j)>0) + (N%K)*(j>=N%K)\n",
    "def end_index(N,j,K):\n",
    "    return (N//K)*(j+1) + (j+1)*((N%K-j-1)>0)+ (N%K)*(j+1>=N%K)\n",
    "\n",
    "print(f\"I am worker number {WORKER_ID} and out of the {N_R0*N_w0*N_Z} matrices to be computed,\")\n",
    "print(f\"I am going to compute from the {beg_index(N_R0*N_w0*N_Z,WORKER_ID,TOTAL_WORKERS)}-th one to the {end_index(N_R0*N_w0*N_Z,WORKER_ID,TOTAL_WORKERS)}-th one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f6de5",
   "metadata": {},
   "source": [
    "## Section 3: Run the Calculations of this Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3fe13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages ######################################\n",
    "import os\n",
    "# This .py file should be in the same directory!\n",
    "from CLASS_GPU_Simulator import *\n",
    "import numpy as np\n",
    "import json\n",
    "from IPython import display\n",
    "from time import time\n",
    "import h5py\n",
    "\n",
    "# General preambles ######################################\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "np.random.seed(randomization_seed)\n",
    "\n",
    "# Generate lattice axes\n",
    "R0_s= np.linspace(min_R0, max_R0, N_R0)#np.linspace(110, 180, N_R0) #np.linspace(70,180,40) # in pxels 153\n",
    "w0_s= np.linspace(min_w0, max_w0, N_w0)#np.linspace( 8, 35, N_w0) #np.linspace(8,50,40) 11\n",
    "Z_s=np.linspace(min_Z, max_Z, N_Z)\n",
    "\n",
    "# This will tell the worker how to compute only those matrices in a fixed range:\n",
    "# we distribute/chop one of the axes of the lattice among the workers \n",
    "# (If you do it with all axes you do not get the whole lattice covered!)\n",
    "R0_s = R0_s[beg_index(N_R0, WORKER_ID, TOTAL_WORKERS):end_index(N_R0, WORKER_ID, TOTAL_WORKERS)]\n",
    "\n",
    "i=1 # Counter\n",
    "total=Z_s.shape[0]*R0_s.shape[0]*w0_s.shape[0] # total number of elements to compute\n",
    "elapsed=0  # time elapsed since computations started\n",
    "beg=time() # initial time\n",
    "\n",
    "\n",
    "\n",
    "# Initialize index bookkeeper ###########################\n",
    "try: # If the computation was halted by the user, then instead of starting from zero, continue wherever we were before\n",
    "    index_bookkeeper = json.load(open(f\"{output_directory}/BOOKKEEPER_PART_{WORKER_ID}_{experiment_name}.json\"))\n",
    "except: # Else, start from scratch\n",
    "    index_bookkeeper = {'R0s':[], 'w0s':[], 'Zs':[], 'IDs':[]}\n",
    "\n",
    "# Initialize Simulator object ############################\n",
    "simulator =RingSimulator_GPU( n=1.5, a0=1.0, max_k=max_k, num_k=num_k, nx=image_side, \n",
    "                                      sim_chunk_x=image_side, sim_chunk_y=image_side)\n",
    "\n",
    "# Initialize the hdf5 dataset storing file ###############\n",
    "# If it already existed (because the user halted the generation),\n",
    "# we will append the next images, else create a new file\n",
    "h5f = h5py.File(f\"{output_directory}/DATASET_PART_{WORKER_ID}_{experiment_name}.h5\", 'a') \n",
    "\n",
    "\n",
    "try: # Store in the h5f the angle at which each pixel is located relative to the central pixel\n",
    "    h5f.create_dataset('phis', data=simulator.phis[:,:,0], compression=\"lzf\", shuffle=True) #, compression_opts=9)\n",
    "except: \n",
    "    h5f['phis'][:] = simulator.phis[:,:,0]\n",
    "\n",
    "# Run the generation loop #####################\n",
    "for Z in Z_s:\n",
    "    for R0 in R0_s:\n",
    "        for w0 in w0_s:\n",
    "            ID = f\"R0_{str(R0)}_w0_{str(w0)}_Z_{str(Z)}\" # this will be the ID for the matrices in the json\n",
    "            if ID not in index_bookkeeper['IDs']: # then it has not been simulated yet\n",
    "                # simulate matrix with this R0,w0,Z\n",
    "                D_matrix = simulator.compute_pieces_for_I_LP_and_CP(R0_pixels=R0, Z=Z, w0_pixels=w0)\n",
    "                \n",
    "                if D_matrix is None: # there has been some error. Stop the execution\n",
    "                    raise ValueError\n",
    "                    \n",
    "                try: # Store the generated matrix in the hdf5 file\n",
    "                    h5f.create_dataset(ID, data=D_matrix, compression=\"lzf\", shuffle=True) #, compression_opts=9)\n",
    "                except: # in case the index_bookkeeper did not record it, but it was already in h5f\n",
    "                    print(f\"{ID} was already in h5f but not in index_bookkeeper\")\n",
    "                    h5f[ID][:] = D_matrix\n",
    "\n",
    "                # Append the index and features of the generated matrix to the bookkeeping dicitonary\n",
    "                index_bookkeeper['IDs'].append(ID)\n",
    "                index_bookkeeper['R0s'].append(str(R0))\n",
    "                index_bookkeeper['Zs'].append(str(Z))\n",
    "                index_bookkeeper['w0s'].append(str(w0))\n",
    "                \n",
    "                # Print state to user\n",
    "                if i%output_info_every==0:\n",
    "                    display.clear_output(wait=True)\n",
    "                    elapsed=time()-beg\n",
    "                    print(f\"[\"+'#'*(int(100*i/total))+' '*(100-int(100*i/total))+f\"] {100*i/total:3.4}% \\n\\nSimulated: {i}/{total}\\nElapsed time: {elapsed//3600} h {elapsed//60-(elapsed//3600)*60} min {elapsed-(elapsed//60)*60-(elapsed//3600)*60:2.4} s\")\n",
    "                \n",
    "                # Store the progress in the external files (in order to be able to quit and resume)\n",
    "                if i%dump_every==0:\n",
    "                    h5f.flush() # technically the matrices are saved already, but need to flush just in case\n",
    "                    json.dump(index_bookkeeper, open( f\"{output_directory}/BOOKKEEPER_PART_{WORKER_ID}_{experiment_name}.json\", \"w\"))\n",
    "            i+=1\n",
    "            \n",
    "# Print Final log details\n",
    "display.clear_output(wait=True)\n",
    "elapsed=time()-beg\n",
    "print(f\"[\"+'#'*(int(100*i/total))+' '*(100-int(100*i/total))+f\"] {100*i/total:3.4}% \\n\\nSimulated: {i}/{total}\\nElapsed time: {elapsed//3600} h {elapsed//60-(elapsed//3600)*60} min {elapsed-(elapsed//60)*60-(elapsed//3600)*60:2.4} s\")               \n",
    "\n",
    "# Finall flush of data to files\n",
    "h5f.flush()\n",
    "json.dump(index_bookkeeper, open( f\"{output_directory}/BOOKKEEPER_PART_{WORKER_ID}_{experiment_name}.json\", \"w\"))\n",
    "\n",
    "# Close hdf5 properly\n",
    "h5f.close()\n",
    "print(f\"\\n\\nWORKER {WORKER_ID} FINISHED!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93f1fa",
   "metadata": {},
   "source": [
    "**Important**: While the previous cell is running, you CAN interrupt the calculations (e.g., by clicking the square button above). The matrices calculated until the last update of the progress bar are safely stored if you run the next cell to close the dataset. Then, even if you shutdown the computer, the next time you run the code, the calculations will continue where they were left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will give error if you waited the whole computation of the previous cell\n",
    "h5f.flush()\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3295f",
   "metadata": {},
   "source": [
    "## Section 4: Join the pieces calculated by the different workers!\n",
    "**Only ONE of the workers must run the following!**\n",
    "\n",
    "Run Section 1 and 2 and then the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import h5py\n",
    "\n",
    "# Create the big h5f and bookkeeper that will contain all of the parts\n",
    "h5f = h5py.File(f\"{output_directory}/DATASET_R0_{N_R0}_w0_{N_w0}_Z_{N_Z}_{experiment_name}.h5\", 'w') # append if exists, create if not\n",
    "total_bookkeeper = {'R0s':[], 'w0s':[], 'Zs':[], 'IDs':[]}\n",
    "\n",
    "for j in range(TOTAL_WORKERS):\n",
    "    # open worker j's part of the dataset\n",
    "    h5f_worker = h5py.File(f\"{output_directory}/DATASET_PART_{j}_{experiment_name}.h5\", 'r')\n",
    "    worker_bookkeeper = json.load(open(f\"{output_directory}/BOOKKEEPER_PART_{j}_{experiment_name}.json\"))\n",
    "    if j==0:\n",
    "        h5f.create_dataset('phis', data=h5f_worker['phis'][:], compression=\"lzf\", shuffle=True)\n",
    "    for ID in worker_bookkeeper['IDs']:\n",
    "        h5f.create_dataset(ID, data=h5f_worker[ID][:], compression=\"lzf\", shuffle=True)\n",
    "    # dump all the results to the total dataset holders\n",
    "    total_bookkeeper['R0s'] = total_bookkeeper['R0s'] + worker_bookkeeper['R0s']\n",
    "    total_bookkeeper['w0s'] = total_bookkeeper['w0s'] + worker_bookkeeper['w0s']\n",
    "    total_bookkeeper['Zs'] = total_bookkeeper['Zs'] + worker_bookkeeper['Zs']\n",
    "    total_bookkeeper['IDs'] = total_bookkeeper['IDs'] + worker_bookkeeper['IDs']\n",
    "    h5f_worker.close()\n",
    "    print(f\"Worker {j}'s job transferred to total dataset files!\")\n",
    "json.dump(total_bookkeeper, open( f\"{output_directory}/BOOKKEEPER_R0_{N_R0}_w0_{N_w0}_Z_{N_Z}_{experiment_name}.json\", \"w\"))\n",
    "h5f.close()\n",
    "print(f\"Total dataset generated! It has two files: \\n  >{output_directory}/DATASET_R0_{N_R0}_w0_{N_w0}_Z_{N_Z}_{experiment_name}.h5\\n  >{output_directory}/BOOKKEEPER_R0_{N_R0}_w0_{N_w0}_Z_{N_Z}_{experiment_name}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb47894",
   "metadata": {},
   "source": [
    "Run the following cell if you want to erase all the partial calculation files (not needed anymore, so you can safely erase them!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1d8da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(TOTAL_WORKERS):\n",
    "    os.remove(f\"{output_directory}/DATASET_PART_{j}_{experiment_name}.h5\")\n",
    "    os.remove(f\"{output_directory}/BOOKKEEPER_PART_{j}_{experiment_name}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc824dd9",
   "metadata": {},
   "source": [
    "## Section 5: How to use the dataset and what it contains exactly\n",
    "\n",
    "You have generated two files:\n",
    "\n",
    "- (a) `BOOKKEEPER_....json` (which is quite light)\n",
    "\n",
    "    You can open it in python using\n",
    "    \n",
    "    `import json`\n",
    "    \n",
    "    `index_bookkeeper = json.load(open(\"BOOKKEEPER_....json\"))`\n",
    "    \n",
    "    It is just a python dictionary that has four lists inside:\n",
    "    \n",
    "    `'R0s':[], 'w0s':[], 'Zs':[], 'IDs':[]`<p>\n",
    "    \n",
    "    \n",
    "    - The first three give the $R_0,w_0,Z$ parameters of each matrix in the (b) file. The values of these lists are saved as strings instead of numbers, so if you want to use them as numbers do not forget to put `float()` around.\n",
    "    \n",
    "    - The last list (IDs) gives the exact label used to index the matrices in the (b) file (corresponding to the $R0,w_0,Z$ of the same \"row\" $-$if you imagine the dictionary as a table).<p>\n",
    "    \n",
    "    \n",
    "\n",
    "- (b) `DATASET_....h5` (which is quite heavy)\n",
    "        \n",
    "    You can open it using:\n",
    "        \n",
    "        \n",
    "    `h5_dataset = h5py.File(\"DATASET_....h5\", 'r')`\n",
    "        \n",
    "        \n",
    "    Let's say you are interested in the matrix with index `ID` (e.g., `index_bookkeeper['IDs'][k]` for the `k`-th matrix). \n",
    "        You can extract it to a numpy array as:\n",
    "        \n",
    "        \n",
    "    `my_matrix = h5_dataset[ID][:]`\n",
    "        \n",
    "    In addition, the `.h5` file also contains a precomputed matrix of size `[image_size, image_size]` giving the polar angle (in radians) of each pixel in an image of size `[image_size, image_size]`, relative to the central pixel (recall `image_size` was required to be odd!). This matrix can be retrieved as:\n",
    "        \n",
    "        \n",
    "    `my_matrix = h5_dataset['phis'][:]`\n",
    "        \n",
    "    When you are done using the dataset close it using\n",
    "        \n",
    "    `h5_dataset.close()`\n",
    "\n",
    "**What do these matrices contain?**\n",
    "        \n",
    "For the fixed target image resolution `[image_side, image_side]` that you chose and for all $R_0,w_0,Z$ in the given lattice, each matrix contains the result for a particular combiantion of $R_0,w_0,Z$ of \n",
    "\n",
    "    np.stack((\n",
    "        B0.real**2+B0.imag**2 + B1.real**2+B1.imag**2, 2*(B1.conjugate()*B0).real\n",
    "    ))\n",
    "\n",
    "where `B0`, `B1` are the terms with the same names of equation (40) and (42) in the bachelor's thesis. Here, `B0` and `B1` are 2D matrices of size `[image_side, image_side]`. \n",
    "\n",
    "Then, if we denote by `D` the result of the last computation (not to confuse with the D of the thesis), and if we denote by `phi_grid` a 2D matrix with shape `[image_side, image_side]` that tells the polar angle of each pixel relative to the central pixel (e.g., `h5_dataset['phis'][:]`), one can then obtain the linear polarization CR ring image for an angle `phiCR` in radians by computing, following equation (42):\n",
    "\n",
    "     D[0]+D[1]*np.cos(phiCRs-phis) #[image_side, image_side]\n",
    "    \n",
    "On the other hand, `D[0]` already gives the circular polarization's image.\n",
    "        \n",
    "Note that it might be interesting (depending of what you are going to do), to normalize the resulting images, so that the maximum intensity pixel takes the maximum value (i.e., divide the image matrix element-wise by the maximum intensity value).\n",
    "    \n",
    "That is, what **the \"D-matrices\" dataset contains is exactly the precursor to the last (silly) calculation to obtain a simulated image** with linear polarization, and the full calculation for circular polarization.\n",
    "\n",
    "\n",
    "Here some example usage, retrieval of images etc. (run Section 1 and 2 and then the next cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e585537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# write here the path to the datasets\n",
    "path_to_h5 =      # f\"{output_directory}/DATASET_R0_{N_R0}_w0_{N_w0}_Z_{N_Z}_{experiment_name}.h5\"\n",
    "path_to_json =      # f\"{output_directory}/BOOKKEEPER_R0_{N_R0}_w0_{N_w0}_Z_{N_Z}_{experiment_name}.json\"\n",
    "\n",
    "# open the two files\n",
    "h5_dataset = h5py.File(path_to_h5, 'r')\n",
    "index_bookkeeper = json.load(open(path_to_json))\n",
    "\n",
    "# retrieve the matrix with the polar angles at each pixel\n",
    "phis = h5_dataset['phis'][:]\n",
    "\n",
    "# to retrieve the first element of the dataset\n",
    "# first take its ID label\n",
    "ID_first_element = index_bookkeeper['IDs'][20]\n",
    "\n",
    "# now access it\n",
    "example_D_matrix = h5_dataset[ID_first_element][:]\n",
    "\n",
    "# generate the circular polarization image\n",
    "CP_image = example_D_matrix[0] #[image_side, image_side]\n",
    "\n",
    "\n",
    "# fix a linear polarization angle\n",
    "example_LP_angle = 3.1415/4\n",
    "LP_image = example_D_matrix[0]+example_D_matrix[1]*np.cos(example_LP_angle-phis) #[image_side, image_side]\n",
    "\n",
    "# plot resulting images\n",
    "print(f\"Circular Polarization\\nR0={index_bookkeeper['R0s'][0]} pix; w0={index_bookkeeper['w0s'][0]} pix; Z={index_bookkeeper['Zs'][0]} pix\")\n",
    "plt.imshow(CP_image)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Linear Polarization at phi={example_LP_angle}rad\\nR0={index_bookkeeper['R0s'][0]} pix; w0={index_bookkeeper['w0s'][0]} pix; Z={index_bookkeeper['Zs'][0]} pix\")\n",
    "plt.imshow(LP_image)\n",
    "plt.show()\n",
    "\n",
    "# close the dataset when finished!\n",
    "h5_dataset.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d682634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little loop to show how the same D matrix allows the generation of any linear polarization\n",
    "for LP_angle in np.linspace(0,2*3.1415, 10):\n",
    "    # generate the image for this angle using the same D matrix\n",
    "    LP_image = example_D_matrix[0]+example_D_matrix[1]*np.cos(LP_angle-phis) #[image_side, image_side]\n",
    "    print(f\"Linear Polarization at phi={LP_angle:.5} rad\\nR0={index_bookkeeper['R0s'][0]} pix; w0={index_bookkeeper['w0s'][0]} pix; Z={index_bookkeeper['Zs'][0]} pix\")\n",
    "    plt.imshow(LP_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee88e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What to do if I wanted to save the resulting image as a .png for some other stuff?\n",
    "import cv2\n",
    "\n",
    "image_with_8bits_per_pixel = ((2**8-1)*LP_image/np.max(LP_image)).astype(np.uint8) \n",
    "cv2.imwrite(\"example_image_8bits.png\", image_with_8bits_per_pixel)\n",
    "\n",
    "image_with_16bits_per_pixel = ((2**16-1)*LP_image/np.max(LP_image)).astype(np.uint16) \n",
    "cv2.imwrite(\"example_image_16bits.png\", image_with_16bits_per_pixel)\n",
    "# you can check that the file of 16 bits occupies more memory than the 8 bit one\n",
    "\n",
    "# What to do if I want to read such an image back to python?\n",
    "image_with_8bits_per_pixel = cv2.imread(\"example_image_8bits.png\", flags=cv2.IMREAD_ANYDEPTH + cv2.IMREAD_GRAYSCALE)\n",
    "image_with_16bits_per_pixel = cv2.imread(\"example_image_16bits.png\", flags=cv2.IMREAD_ANYDEPTH + cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "print(image_with_16bits_per_pixel.dtype, image_with_16bits_per_pixel.shape)\n",
    "print(image_with_8bits_per_pixel.dtype, image_with_8bits_per_pixel.shape)\n",
    "\n",
    "# Take them to floats if you want to do operations with them!\n",
    "image_8_in_floats = image_with_8bits_per_pixel.astype(np.float32) # or float64 is double precision required!\n",
    "image_16_in_floats = image_with_16bits_per_pixel.astype(np.float32)\n",
    "\n",
    "print(image_with_8bits_per_pixel[:3,:3])\n",
    "print(image_8_in_floats[:3,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74043c67",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the test images we just generated!\n",
    "import os\n",
    "os.remove(\"example_image_8bits.png\")\n",
    "os.remove(\"example_image_16bits.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7b603",
   "metadata": {},
   "source": [
    "## Section 6: How to make a `.png` dataset out of this?\n",
    "\n",
    "Maybe you prefer to have a folder full of `.png` of simulated CR ring images. I warn you that:\n",
    "\n",
    "1. You will need to discretize also the linear polarization, so you will loose the possibility to access the full continuum, and since the angle of polarization is the parameter we are interested on, this might not be a good idea unless you make the angle grid dense enough.<p>\n",
    "\n",
    "2. This will occupy way more space than the previous dataset, because you will need to consider now all the previous dataset for each point in the angle grid (so multiply the weight of the `h5`by the number of angle points you want to consider, to estimate the storage space needed for that).\n",
    "    \n",
    "If you are okay with that, just fill the parameters in the next cell and run the following one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd6a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the OUTPUT path. The images will be generated to a sub-folder (if non-existent path, it will be created)\n",
    "output_directory = \"./OUTPUT/\"\n",
    "experiment_name = \"Test\"\n",
    "\n",
    "# Select if you want to generate images of circular and/or linear polarization ###############\n",
    "circular_polarization = True\n",
    "linear_polarization = True\n",
    "\n",
    "# Choose the grid in linear polarization angles you are looking for ##########################\n",
    "N_phiCR = 10\n",
    "\n",
    "# write here the path to the datasets ########################################################\n",
    "path_to_h5 =    # f\"{output_directory}/DATASET_R0_{N_R0}_w0_{N_w0}_Z_{N_Z}_{experiment_name}.h5\"\n",
    "path_to_json =   # f\"{output_directory}/BOOKKEEPER_R0_{N_R0}_w0_{N_w0}_Z_{N_Z}_{experiment_name}.json\"\n",
    "\n",
    "# print progess bar every x many generated images\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c17a2c",
   "metadata": {},
   "source": [
    "Run following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "730c943c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#################################################################################################### ] 100.0% \n",
      "\\Generated Images: 200/200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import h5py\n",
    "import cv2\n",
    "import os\n",
    "from IPython import display\n",
    "\n",
    "# open the two files\n",
    "h5_dataset = h5py.File(path_to_h5, 'r')\n",
    "index_bookkeeper = json.load(open(path_to_json))\n",
    "\n",
    "# generate grid of angles\n",
    "phiCRs = np.linspace( 0, 2*np.pi, N_phiCR )\n",
    "\n",
    "# retrieve the matrix with the polar angles at each pixel\n",
    "phis = h5_dataset['phis'][:]\n",
    "\n",
    "total = len(index_bookkeeper['IDs'])\n",
    "# make output directory if non-existent\n",
    "output_path = output_directory + f\"/IMAGE_DATASET_{experiment_name}_N_R0_{len(set(index_bookkeeper['R0s']))}_N_w0_{len(set(index_bookkeeper['w0s']))}_N_Z_{len(set(index_bookkeeper['Zs']))}_N_phiCR_{N_phiCR}\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for i,ID in enumerate(index_bookkeeper['IDs']):\n",
    "    # retrieve D matrix\n",
    "    D_mat = h5_dataset[ID][:]\n",
    "    if linear_polarization:\n",
    "        for LP_angle in phiCRs:\n",
    "            LP_image = D_mat[0]+D_mat[1]*np.cos(LP_angle-phis) #[image_side, image_side]\n",
    "            cv2.imwrite(f\"{output_path}/{ID}_LP_angle_{str(LP_angle)}.png\", ((2**8-1)*LP_image/np.max(LP_image)).astype(np.uint8))\n",
    "    if circular_polarization:\n",
    "        cv2.imwrite(f\"{output_path}/{ID}_CP.png\", ((2**8-1)*D_mat[0]/np.max(D_mat[0])).astype(np.uint8))\n",
    "    if i%log_every==0:\n",
    "        display.clear_output(wait=True)\n",
    "        print(f\"[\"+'#'*(int(100*(i+1)/total))+' '*(100-int(100*i/total))+f\"] {100*(i+1)/total:3.4}% \\n\\Generated Images: {i+1}/{total}\\n\")               \n",
    "\n",
    "display.clear_output(wait=True)\n",
    "print(f\"[\"+'#'*(int(100*(i+1)/total))+' '*(100-int(100*i/total))+f\"] {100*(i+1)/total:3.4}% \\n\\Generated Images: {i+1}/{total}\\n\")               \n",
    "h5_dataset.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
