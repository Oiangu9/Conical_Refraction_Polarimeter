{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Residual Autoencoder for $\\phi_{CR}, R_0, w_0, Z$  of a CR image\n",
    "---\n",
    "CR-CRAE = CR2AE\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #should be installed by default in any colab notebook\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as display_IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from time import time\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
    "\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the functions and routines for the DL\n",
    "### Define the model and its constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CR2Autoencoder(nn.Module):\n",
    "    def __init__(self, X=302, feats_1=15, feats_2=20, feats_3=20, feats_4=20,\n",
    "                 prop1=3, prop2=2, prop3=1, av_pool1_div=4, conv4_feat_size=15, av_pool2_div=10, \n",
    "                 out_fc_1=10,\n",
    "                 dropout_p1=0.2, dropout_p2=0.1,\n",
    "                 latent_representation_dims=10,\n",
    "                 out_fc_3 = 10\n",
    "                ): \n",
    "        # propj is such that the_ image getting out from stage j is propj/prop_{j-1}-ths of the previous (with j=0 being 5)\n",
    "        # clearly, prop_{j-1}>prop_{j}>...\n",
    "        # 2X+1 will be assumed to be divisible by 5\n",
    "        assert((2*X+1)%5==0)\n",
    "        assert(prop1>prop2)\n",
    "        assert(prop2>prop3)\n",
    "        assert((int((prop3*(2*X+1)/5)/av_pool1_div)-conv4_feat_size)>0)\n",
    "        \n",
    "        \n",
    "        super(CR2Autoencoder, self).__init__()\n",
    "        # in is [epoch_size, 1, 2X+1, 2X+1]\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=feats_1, \n",
    "                               kernel_size = int((2*X+1)/5*(5-prop1)+1), bias=True) \n",
    "        # out conv1 [epoch_size, feats_1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "        self.conv2 = nn.Conv2d(in_channels=feats_1, out_channels=feats_2, \n",
    "                               kernel_size = int((2*X+1)/5*(prop1-prop2)+1), bias=True) \n",
    "        # out conv1 [epoch_size, feats_2, prop2*(prop1*(2X+1)/5)/prop1, prop2*(prop1*(2X+1)/5)/prop1]\n",
    "        # that is [epoch_size, feats_2, prop2*(2X+1)/5), prop2*(2X+1)/5)]\n",
    "        self.conv3 = nn.Conv2d(in_channels=feats_2, out_channels=feats_3, \n",
    "                               kernel_size = int((2*X+1)/5*(prop2-prop3)+1), bias=True)\n",
    "        # out conv3 is [epoch_size, feats_3, prop3*(2X+1)/5), prop3*(2X+1)/5)]\n",
    "\n",
    "        self.avPool1 = nn.AvgPool2d(kernel_size= int((prop3*(2*X+1)/5)*(1-1/av_pool1_div)) +1, stride=1)\n",
    "        # out avpool1 is [epoch_size, feats_3, prop3*(2X+1)/5)/av_pool1_div, prop3*(2X+1)/5)/av_pool1_div]\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=feats_3, out_channels=feats_4, \n",
    "                              kernel_size= int((prop3*(2*X+1)/5)/av_pool1_div+1)-conv4_feat_size+1, bias=True)\n",
    "        # [epoch_size, feats_4, conv4_feat_size, conv4_feat_size]\n",
    "        \n",
    "        self.avPool2 = nn.AvgPool2d(kernel_size= int(conv4_feat_size*(1-1/av_pool2_div)) +1, stride=1)\n",
    "        # out avpool1 is [epoch_size, feats_4, conv4_feat_size/av_pool2_div+1, conv4_feat_size/av_pool2_div+1]\n",
    "        \n",
    "        self.in_fc = feats_4*int(conv4_feat_size/av_pool2_div+1)**2\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=self.in_fc, out_features=out_fc_1, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=out_fc_1, out_features=latent_representation_dims, bias=True)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=dropout_p1, inplace=False)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p2, inplace=False)\n",
    "        self.relu = torch.nn.functional.relu\n",
    "\n",
    "        self.batchNorm2 = nn.BatchNorm2d(num_features=feats_2)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(num_features=feats_4)\n",
    "        \n",
    "        self.batchNorm22 = nn.BatchNorm2d(num_features=feats_2)\n",
    "        self.batchNorm44 = nn.BatchNorm2d(num_features=feats_4)\n",
    "        \n",
    "        self.fc3 = nn.Linear(in_features=latent_representation_dims, out_features=out_fc_3, bias=True)\n",
    "        self.fc4 = nn.Linear(in_features=out_fc_3, out_features=4, bias=True)\n",
    "        \n",
    "        self.fc5 = nn.Linear(in_features=latent_representation_dims, out_features=self.in_fc, bias=True)\n",
    "        self.deConv5 = torch.nn.ConvTranspose2d(in_channels=2*feats_4, out_channels=feats_4, \n",
    "                                                kernel_size = int(conv4_feat_size*(1-1/av_pool2_div)) +1)\n",
    "        self.deConv6 = torch.nn.ConvTranspose2d(in_channels=2*feats_4, out_channels=feats_2, \n",
    "                                                kernel_size = int(prop2*(2*X+1)/5-conv4_feat_size+1 ))\n",
    "        self.deConv7 = torch.nn.ConvTranspose2d(in_channels=2*feats_2, out_channels=feats_1, \n",
    "                                                kernel_size = int((2*X+1)/5*(prop1-prop2)+1))\n",
    "        self.deConv8 = torch.nn.ConvTranspose2d(in_channels=2*feats_1, out_channels=1, \n",
    "                                                kernel_size = int((2*X+1)/5*(5-prop1)+1))\n",
    "        \n",
    "\n",
    "    def encoder(self, x): # [batch_size, 2X+1, 2X+1] or [batch_size, 1, 2X+1, 2X+1]\n",
    "        x = x.view(x.shape[0], 1, x.shape[-2], x.shape[-1]).float() # [batch_size, 1, 2X+1, 2X+1]\n",
    "        \n",
    "        encoded4 = self.relu( self.conv1(x) ) # [batch_size, feats_1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "        \n",
    "        encoded3 = self.batchNorm2( self.relu( self.conv2(self.dropout1(encoded4)) )) # [batch_size, feats_2, prop2*(2X+1)/5, prop2*(2X+1)/5]\n",
    "\n",
    "        \n",
    "        x = self.relu( self.conv3(self.dropout2(encoded3)) ) # [batch_size, feats_3, prop3*(2X+1)/5, prop3*(2X+1)/5]\n",
    "\n",
    "        \n",
    "        x = self.avPool1(x) # [batch_size, feats_3, prop3*(2X+1)/5)/av_pool1_div, prop3*(2X+1)/5)/av_pool1_div]\n",
    "\n",
    "        \n",
    "        encoded2 = self.batchNorm4(self.conv4(self.dropout2(x))) # [batch_size, feats_4, conv4_feat_size, conv4_feat_size]\n",
    "\n",
    "        \n",
    "        encoded1 = self.relu( self.avPool2(encoded2) ) # [batch_size, feats_4, conv4_feat_size/av_pool2_div, conv4_feat_size/av_pool2_div]\n",
    "        \n",
    "        x = encoded1.view(encoded1.shape[0], self.in_fc) #[batch_size, feats_4*int(conv4_feat_size/av_pool2_div)**2]\n",
    "\n",
    "        \n",
    "        x = self.fc2( self.relu( self.fc1(x) ) ) #[batch_size, latent_representation_dims]\n",
    "        \n",
    "        return x, encoded1, encoded2, encoded3, encoded4\n",
    "    \n",
    "    \n",
    "    \n",
    "    def image_decoder(self, latent_representation, encoded1, encoded2, encoded3, encoded4 ):        \n",
    "        # latent_representation is [batch_size, latent_representation_dims]\n",
    "        # first a fc to turn it back to the shape tkhe last convolution of the encoder left\n",
    "        x = self.relu(self.fc5(latent_representation)) # [batch_size, feats_4*int(conv4_feat_size/av_pool2_div+1)**2]\n",
    "        \n",
    "        x = x.view( encoded1.shape ) # [batch_size, feats_4, conv4_feat_size/av_pool2_div, conv4_feat_size/av_pool2_div]\n",
    "        \n",
    "        x = torch.hstack( (x, encoded1)) # [batch_size, 2*feats_4, conv4_feat_size/av_pool2_div, conv4_feat_size/av_pool2_div]\n",
    "        \n",
    "        x = self.batchNorm44(self.dropout2(self.deConv5(x))) # [batch_size, feats4, conv4_feat_size]\n",
    "\n",
    "        x = torch.hstack( (x, encoded2) ) # [batch_size, 2*feats4, conv4_feat_size]\n",
    "        \n",
    "        x = self.batchNorm22(self.relu(self.dropout2(self.deConv6(x)))) # [batch_size, feats2, prop2*(2X+1)/5, prop2*(2X+1)/5]\n",
    "        \n",
    "        x = torch.hstack( (x, encoded3) ) # [batch_size, feats2*2, prop2*(2X+1)/5, prop2*(2X+1)/5]\n",
    "        \n",
    "        x = self.relu(self.dropout1(self.deConv7(x))) # [batch_size, feats1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "\n",
    "        x = torch.hstack( (x, encoded4) ) # [batch_size, feats1*2, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "        \n",
    "        return self.deConv8(x).squeeze(1) # [batch_size, 2X+1, 2X+1]\n",
    "      \n",
    "    def label_decoder(self, latent_representation):\n",
    "        return self.fc4(self.relu(self.fc3(latent_representation)))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent, encoded1, encoded2, encoded3, encoded4 = self.encoder(x)\n",
    "        predicted_labels = self.label_decoder(latent)\n",
    "        reconstructed_images = self.image_decoder(latent, encoded1, encoded2, encoded3, encoded4)\n",
    "        return predicted_labels, reconstructed_images\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subroutine to count number of parameters in the model\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.numel()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The routines to validate and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # prevent this function from computing gradients \n",
    "def validate_epoch(label_criterion, reconstruction_criterion, model, sampler, dataset): #show_confusion_matrix = False):\n",
    "\n",
    "    val_loss = 0\n",
    "    max_abs_im_error = torch.Tensor([0]).to(device)\n",
    "    mean_abs_im_error = 0\n",
    "    max_abs_lb_error = torch.Tensor([0]).to(device)\n",
    "    mean_abs_lb_error = 0\n",
    "    preds = torch.Tensor().to(device)\n",
    "    targets = torch.Tensor().to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch_id, sample_batch_idxs in enumerate(sampler):\n",
    "        \n",
    "        data, target_labels, target_images = dataset[sample_batch_idxs] # each data, target is a whole batch of samples\n",
    "        # data is [batch_size, 2X+1, 2X+1], target_labels is [batch_size, 4], target_images is [batch_size, 2X+1, 2X+1]\n",
    "        \n",
    "        predicted_labels, predicted_images = model(data)\n",
    "        target_images = target_images.view(predicted_images.shape) # to add the 1 dimension of the single channel\n",
    "        loss = label_criterion(predicted_images, target_images.float())+reconstruction_criterion(\n",
    "                                                                predicted_labels, target_labels)\n",
    "        val_loss += loss.item()                                                              \n",
    "        max_abs_im_error = torch.maximum(torch.max(torch.abs(predicted_images-target_images), 0).values, max_abs_im_error)\n",
    "        mean_abs_im_error += torch.sum(torch.abs(predicted_images-target_images), 0)\n",
    "        max_abs_lb_error = torch.maximum(torch.max(torch.abs(predicted_labels-target_labels), 0).values, max_abs_lb_error)\n",
    "        mean_abs_lb_error += torch.sum(torch.abs(predicted_labels-target_labels), 0)\n",
    "        \n",
    "\n",
    "    val_loss /= len(dataset)\n",
    "    mean_abs_im_error /= len(dataset)\n",
    "    mean_abs_lb_error /= len(dataset)\n",
    "    #accuracy = 100. * correct / len(loader.dataset)\n",
    "    print(f'\\nValidation set: Average loss: {val_loss:.4f}\\n\\n Average Abs Image Error: {np.array(mean_abs_im_error.cpu()/len(dataset))}, Maximum Abs Image Error: {np.array(max_abs_im_error.cpu())} \\n\\n')\n",
    "    print(f'Average Abs Image Error: {np.array(mean_abs_lb_error.cpu()/len(dataset))}, Maximum Abs Image Error: {np.array(max_abs_lb_error.cpu())}')\n",
    "    #if show_confusion_matrix:\n",
    "    #    visualize_confusion_matrix(preds.to(torch.device('cpu')), targets.to(torch.device('cpu')))\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train_epoch(epoch, label_criterion, reconstruction_criterion, model, optimizer, sampler, dataset, \n",
    "                print_loss_every_batches=20, optimizer_step_every_batches=3, label_cost_weight=1):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    t7=time()\n",
    "    for batch_id, sample_batch_idxs in enumerate(sampler):\n",
    "        data,  target_labels, target_images = dataset[sample_batch_idxs]  # each data, target is a whole batch of samples\n",
    "        # data is [batch_size, 2X+1, 2X+1], target_labels is [batch_size, 4], target_images is [batch_size, 2X+1, 2X+1]\n",
    "        t1=time()\n",
    "        # data, target = data.to(device), target.to(device) DATA AND TARGET ARE ALREADY IN GPU!\n",
    "        print(f\"Data retrieval time {t1-t7}\")\n",
    "        predicted_labels, predicted_images = model(data)\n",
    "        loss = label_cost_weight*label_criterion(predicted_labels, target_labels)+reconstruction_criterion(predicted_images, target_images.float())\n",
    "        loss.backward()\n",
    "        if batch_id % optimizer_step_every_batches==optimizer_step_every_batches-1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # print loss every N batches\n",
    "        if batch_id % print_loss_every_batches == print_loss_every_batches-1:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_id * len(data), len(dataset),\n",
    "                print_loss_every_batches * batch_id / len(dataset), loss.item()))\n",
    "\n",
    "        t7=time()\n",
    "        total_loss += loss.item()  #.item() is very important here\n",
    "        # Why?-> In order to avoid having total_loss as a tensor in the gpu\n",
    "\n",
    "    return total_loss / len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_training_loop(model, label_criterion, reconstruction_criterion, optimizer, sampler, dataset, epochs=10,\n",
    "                       print_loss_every_batches=20, validate_every_epochs=20, optimizer_step_every_batches=3,\n",
    "                      label_cost_weight=1):\n",
    "    losses = {\"train\": [], \"val\": []}\n",
    "    %matplotlib inline\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss = train_epoch(epoch, label_criterion, reconstruction_criterion, model, optimizer,\n",
    "                                 sampler, dataset, print_loss_every_batches=print_loss_every_batches,\n",
    "                                 optimizer_step_every_batches=optimizer_step_every_batches, label_cost_weight=label_cost_weight)\n",
    "        display_IPython.clear_output(wait=True)\n",
    "        #if epoch%validate_every_epochs==0:\n",
    "        #    val_loss = validate_epoch(label_criterion, reconstruction_criterion, model, sampler, dataset)\n",
    "        losses[\"train\"].append(train_loss)\n",
    "        #losses[\"val\"].append(val_loss)\n",
    "        \n",
    "\n",
    "        plt.plot(losses[\"train\"], label=\"log training loss\")\n",
    "        #plt.plot(losses[\"val\"], label=\"log validation loss\")\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        plt.pause(0.001)\n",
    "        plt.show()   \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CR_loss(estimation, target, extra_power_to_angles, loss_calculator):\n",
    "    # we wish to have maximum accuracy with the angles\n",
    "    return loss_calculator(estimation, target) + extra_power_to_angles * loss_calculator(estimation[:,-1], target[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset class and Data Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
    "\n",
    "# use gpu if available\n",
    "#device=\"cpu\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "'''\n",
    "La idea es que cada 5 epochs, se cambie el dataset efectivo, que sera un subset de los R0,w0,Z posibles\n",
    "multiplicado por el batch size (phiCR posibles). En cada batch, las imagenes enviadas seran todas\n",
    "de un mismo D matrix (R0,w0,Z) con diferentes angulos elegidos aleatoriamente con una uniforme\n",
    "'''\n",
    "class R0_w0_Z_Sampler(Sampler):\n",
    "    def __init__(self, R0_weights, w0_weights, Z_weights, num_batches_per_epoch):\n",
    "        self.num_batches = num_batches_per_epoch\n",
    "        self.R0_weights = R0_weights\n",
    "        self.w0_weights = w0_weights\n",
    "        self.Z_weights = Z_weights\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(torch.stack((\n",
    "            torch.multinomial(self.R0_weights, self.num_batches, replacement=True),\n",
    "            torch.multinomial(self.w0_weights, self.num_batches, replacement=True),\n",
    "            torch.multinomial(self.Z_weights, self.num_batches, replacement=True)),\n",
    "            dim=1).tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "class CR_Dataset(Dataset):\n",
    "    def __init__(self, D_matrix_file_path, ID_file_path, device, X=605, generate_images_w_depth=8, random_seed=666, \n",
    "                batch_size=10, num_batches_per_epoch=100, apply_noise=True,\n",
    "                all_stregths_random_per_epoch=False,\n",
    "                max_poisson_strength=0.5, max_blob_strength=0.5, max_angular_modulation_strength=0.5,\n",
    "                poisson_strength=0.3, blob_strength=0.1, angular_modulation_strength=0.25,\n",
    "                min_modulation_frec=2*np.pi/6, max_modulation_frec=2*np.pi/2,\n",
    "                max_blobs=1, min_blob_sigma=100, max_blob_sigma=130\n",
    "                ):\n",
    "        # If all_strengths_random_per_ecpoh, then arguments about the maximum will be valid while not the strength arguments\n",
    "        # If false, then the arguments about the particular stregths will be the global stregths\n",
    "        np.random.seed(random_seed) \n",
    "        torch.manual_seed(random_seed)\n",
    "        self.D_matrix_file_path=D_matrix_file_path\n",
    "        self.df_GTs = pd.DataFrame.from_dict(json.load(open(ID_file_path)))       \n",
    "        self.R0s = list(self.df_GTs['R0s'].drop_duplicates()) # Note they are lists of strings!\n",
    "        self.w0s = list(self.df_GTs['w0s'].drop_duplicates())\n",
    "        self.Zs = list(self.df_GTs['Zs'].drop_duplicates())\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches_per_epoch = num_batches_per_epoch\n",
    "        self.epoch_size = batch_size*num_batches_per_epoch\n",
    "        self.device = device\n",
    "        self.im_type = torch.uint16 if generate_images_w_depth==16 else torch.uint8\n",
    "        self.max_intensity = 65535 if generate_images_w_depth==16 else 254\n",
    "        self.X=X\n",
    "        self.apply_noise=apply_noise\n",
    "        self.poisson_strength=poisson_strength\n",
    "        self.blob_strength=blob_strength\n",
    "        self.angular_modulation_strength=angular_modulation_strength\n",
    "        self.min_modulation_frec=min_modulation_frec\n",
    "        self.max_modulation_frec=max_modulation_frec\n",
    "        self.max_blobs=max_blobs\n",
    "        self.min_blob_sigma=min_blob_sigma\n",
    "        self.max_blob_sigma=max_blob_sigma\n",
    "        self.all_stregths_random_per_epoch=all_stregths_random_per_epoch\n",
    "        self.max_poisson_strength=max_poisson_strength\n",
    "        self.max_blob_strength=max_blob_strength\n",
    "        self.max_angular_modulation_strength=max_angular_modulation_strength\n",
    "        \n",
    "    #def update_dataset o set_epoch_number y que aqui se genere directamente el dataset entero para las epochs que vienen\n",
    "    # lo que permitiria es que cada X epochs, se ahorrase el tener que re-generar todas las imagenes\n",
    "    # Pero claro, la pregunta es, la RAM aguantaria?\n",
    "    # Si haces con update_dataset, entonces no haria falta hacer un sampler custom, con el normal ya bastaria\n",
    "    \n",
    "    # Bueno, por ahora, vamos a hacer que en cada minibatch, se haga todo el puroceso. La cosa es que asi se \n",
    "    # puede aprovechar el multiprocessing innato, si no habria que hacer el multiprocessing dentroe del update_dataset\n",
    "    # o simplemente prescindir de hacerlo supongo.\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'h5f_D_matrices'):\n",
    "            self.h5f_D_matrices.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.epoch_size\n",
    "    \n",
    "    def open_hdf5(self):\n",
    "        self.h5f_D_matrices = h5py.File( self.D_matrix_file_path, 'r')\n",
    "        #self.dataset = self.img_hdf5['dataset'] # if you want dataset.\n",
    "        \n",
    "\n",
    "    def compute_intensity_gravity_centers(self, images):\n",
    "        \"\"\"\n",
    "            Expects input image to be an array of dimensions [N_imgs, h, w].\n",
    "            It will return an array of gravity centers [N_imgs, 2(h,w)] in pixel coordinates\n",
    "            Remember that pixel coordinates are set equal to array indices\n",
    "\n",
    "        \"\"\"\n",
    "        # image wise total intensity and marginalized inensities for weighted sum\n",
    "        intensity_in_w = torch.sum(images, dim=1) # weights for x [N_images, raw_width]\n",
    "        intensity_in_h = torch.sum(images, dim=2) # weights for y [N_images, raw_height]\n",
    "        total_intensity = intensity_in_h.sum(dim=1) # [N_images]\n",
    "\n",
    "        # Compute mass center for intensity\n",
    "        # [N_images, 2] (h_center,w_center)\n",
    "        return torch.nan_to_num( torch.stack(\n",
    "            (torch.matmul(intensity_in_h.float(), torch.arange(images.shape[1], \n",
    "                                        dtype=torch.float32, device=self.device))/total_intensity,\n",
    "             torch.matmul(intensity_in_w.float(), torch.arange(images.shape[2], \n",
    "                                        dtype=torch.float32, device=self.device))/total_intensity),\n",
    "            dim=1\n",
    "            ), nan=0.0, posinf=None, neginf=None)\n",
    "\n",
    "    def compute_raw_to_centered_iX(self, images):\n",
    "\n",
    "        g_raw = self.compute_intensity_gravity_centers(images) # [ N_images, 2]\n",
    "\n",
    "        # crop the iamges with size (X+1+X)^2 leaving the gravity center in\n",
    "        # the central pixel of the image. In case the image is not big enough for the cropping,\n",
    "        # a 0 padding will be made.\n",
    "        centered_images = torch.zeros( ( images.shape[0], 2*self.X+1, 2*self.X+1),  dtype = images.dtype, \n",
    "                                      device=self.device)\n",
    "\n",
    "        # we round the gravity centers to the nearest pixel indices\n",
    "        g_index_raw = torch.round(g_raw).int() #[ N_images, 2]\n",
    "\n",
    "        # obtain the slicing indices around the center of gravity\n",
    "        # TODO -> make all this with a single array operation by stacking the lower and upper in\n",
    "        # a new axis!!\n",
    "        # [ N_images, 2 (h,w)]\n",
    "        unclipped_lower = g_index_raw-self.X\n",
    "        unclipped_upper = g_index_raw+self.X+1\n",
    "\n",
    "        # unclipped could get out of bounds for the indices, so we clip them\n",
    "        lower_bound = torch.clip( unclipped_lower.float(), min=torch.Tensor([[0,0]]).to(self.device),\n",
    "                                 max=torch.Tensor(list(images.shape[1:])).unsqueeze(0).to(self.device)).int()\n",
    "        upper_bound = torch.clip( unclipped_upper.float(), min=torch.Tensor([[0,0]]).to(self.device),\n",
    "                                 max=torch.Tensor(list(images.shape[1:])).unsqueeze(0).to(self.device)).int()\n",
    "        # we use the difference between the clipped and unclipped to get the necessary padding\n",
    "        # such that the center of gravity is left still in the center of the image\n",
    "        padding_lower = lower_bound-unclipped_lower\n",
    "        padding_upper = upper_bound-unclipped_upper\n",
    "\n",
    "        # crop the image\n",
    "        for im in range(g_raw.shape[0]):\n",
    "            centered_images[im, padding_lower[ im, 0]:padding_upper[ im, 0] or None,\n",
    "                                        padding_lower[ im, 1]:padding_upper[ im, 1] or None] = \\\n",
    "                      images[im, lower_bound[ im, 0]:upper_bound[ im, 0],\n",
    "                                          lower_bound[ im, 1]:upper_bound[ im, 1]]\n",
    "\n",
    "        return centered_images\n",
    "    \n",
    "    def apply_random_camera_noises(self, images):\n",
    "        # Poisson noise\n",
    "        # the images are expected to already be normalized and in the integer range of the camera\n",
    "        return torch.clamp((1-self.poisson_strength)*images+self.poisson_strength*torch.poisson(images), max=self.max_intensity) \n",
    "                                    # rates are the expected intensities of the imaging time\n",
    "\n",
    "    def _gaussian_2D_pdfs(self, x_ys, mus, sigmas, strengths):\n",
    "        '''\n",
    "        x_ys : [batch_size, blob_num, 2 (h,w), 2X+1, 2X+1]\n",
    "        mus : [batch_size, blob_num, 2 (h,w), 1, 1]\n",
    "        sigmas : [batch_size, blob_num, 2(h,w), 1, 1]\n",
    "        strengths : [batch_size, blob_num, 1, 1]\n",
    "        ------\n",
    "        out : [batch_size, 2X+1, 2X+1]\n",
    "        '''\n",
    "        gaussians = torch.sum((strengths/(2*np.pi)/sigmas[:,:,0]/sigmas[:,:,1])*torch.exp(\n",
    "                -(x_ys[:,:,0,:,:]-mus[:,:,0])**2/(2*sigmas[:,:,0]**2))*torch.exp(\n",
    "                -(x_ys[:,:,1,:,:]-mus[:,:,1])**2/(2*sigmas[:,:,1]**2)), dim=1) # since strength is normalized, the whole mixture is normalized as well\n",
    "        return gaussians/gaussians.amax(dim=(1,2)).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def apply_random_pre_camera_noises(self, images):\n",
    "        # note that the input images are expected to still be centered in the gravicenter and have no normalization\n",
    "\n",
    "        # Gaussian Blobs - subtract gaussian blobs of different depths to the intensity pattern\n",
    "        # First randomly sample the centers of the blobs and their standard deviations for each image\n",
    "        # we will sample the means with probabilities proportional to the CR ring intensity pattern\n",
    "        blob_num = np.random.randint(0, self.max_blobs+1, size=1)[0]\n",
    "        if blob_num!=0:\n",
    "            mu_s = torch.stack(\n",
    "                (torch.multinomial(images.sum(dim=2),\n",
    "                        num_samples=blob_num, \n",
    "                        replacement=False), \n",
    "                 torch.multinomial(images.sum(dim=1),\n",
    "                        num_samples=blob_num, \n",
    "                        replacement=False) ),\n",
    "                 dim=2\n",
    "                ).to(self.device) #[batch_size, blob_num, 2(h,w)] mu-s are in pixel units and coordinates\n",
    "\n",
    "            sigma_s = torch.from_numpy(np.random.randint(self.min_blob_sigma, self.max_blob_sigma, \n",
    "                        size=(images.shape[0], blob_num, 2))).to(self.device) #[batch_size, blob_num, 2(h,w)]\n",
    "            strengths = torch.rand(size=(images.shape[0], blob_num)).to(self.device) #[batch_size, blob_num]\n",
    "            strengths = strengths/strengths.sum(dim=1).unsqueeze(1) # normalized strengths between blobs\n",
    "\n",
    "            w = torch.arange(images.shape[1]).repeat((images.shape[1],1)).to(self.device)\n",
    "            h = w.transpose(0,1).to(self.device)\n",
    "            h_w = torch.stack((h,w), dim=0).to(self.device)\n",
    "            images = images*(1-self.blob_strength*self._gaussian_2D_pdfs( h_w.view((1,1)+h_w.shape), \n",
    "                mu_s.view(mu_s.shape+(1,1)), sigma_s.view(sigma_s.shape+(1,1)), strengths.view(strengths.shape+(1,1)) )\n",
    "                     )           #[batch_size, 2X+1, 2X+1]        \n",
    "        # Poisson noise - makes the intesity be a poissonian generated value instead of the expected values\n",
    "        #images = (1-poisson_strength)*images+poisson_strength*torch.poisson(images) # rates are the expected intensities of the imaging time\n",
    "        # but must be an integer matrix!\n",
    "\n",
    "        # Angular Modulation - apply a pseudo-random continous wave modulation to the ring angularly\n",
    "        random_frecs = (self.min_modulation_frec + (self.max_modulation_frec-self.min_modulation_frec)*torch.rand(\n",
    "                                size=(3,images.shape[0], 1,1))).to(self.device)\n",
    "        strengths = torch.rand(size=(3, images.shape[0], 1,1)).to(self.device) #[3, batch_size, 1,1]\n",
    "        strengths = strengths/strengths.sum(dim=0) # normalized strengths between sin and coss\n",
    "        images = images*(\n",
    "            1-self.angular_modulation_strength*(\n",
    "                strengths[0]*torch.cos(random_frecs[0]*self.phis)+\n",
    "                strengths[1]*torch.sin(random_frecs[1]*self.phis)+\n",
    "                strengths[2]*torch.cos(random_frecs[2]*self.phis)\n",
    "            )**2) #[batch_size, 2X+1, 2X+1]\n",
    "\n",
    "        # Angular-Radial Modulation # ser√≠a coger phis y coger radios y con eso hacer uan funcion de ambas, de forma\n",
    "        # que por ejemplo afecte de manera diferente al mismo angulo en cada ring el pre-pogendorf y el otro\n",
    "\n",
    "        # Modos superiores\n",
    "        # esto ya es un jaleo xD\n",
    "        return images\n",
    "\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, R0_w0_Z_idxs):\n",
    "        # In order to allow multiprocessing data loading, each worker needs to initialize \n",
    "        # the h5f loader, which must be done in the first iteration of getitem and not in the init\n",
    "        # of the parent process\n",
    "        if not hasattr(self, 'h5f_D_matrices'):\n",
    "            self.open_hdf5()\n",
    "            self.phis = torch.from_numpy(self.h5f_D_matrices['phis'][:]).unsqueeze(0).to(self.device) #[1,Nx,Ny]\n",
    "\n",
    "        D_mats = torch.from_numpy(self.h5f_D_matrices[\n",
    "                f\"R0_{self.R0s[R0_w0_Z_idxs[0]]}_w0_{self.w0s[R0_w0_Z_idxs[1]]}_Z_{self.Zs[R0_w0_Z_idxs[2]]}\"][:]\n",
    "                                 ).unsqueeze(1).to(self.device) #[2, 1, Nx, Ny]            \n",
    "        \n",
    "        phiCRs = torch.FloatTensor(self.batch_size, 1, 1).uniform_(0, 2*np.pi).to(self.device) #[batch_size, 1, 1]\n",
    "        neat_images = D_mats[0]+D_mats[1]*torch.cos(phiCRs-self.phis) #[batch_size, Nx,Ny]\n",
    "        \n",
    "        if self.apply_noise:\n",
    "            if self.all_stregths_random_per_epoch:\n",
    "                self.poisson_strength = self.max_poisson_strength*np.random.rand()\n",
    "                self.angular_modulation_strength = self.max_angular_modulation_strength*np.random.rand()\n",
    "                self.blob_strength = self.max_blob_strength*np.random.rand()\n",
    "            \n",
    "            # Apply precamera noise to images (while still floats)\n",
    "            spoiled_images = self.apply_random_pre_camera_noises(neat_images)\n",
    "        \n",
    "        # convert images to selected uint format\n",
    "        spoiled_images = (self.max_intensity*(spoiled_images/spoiled_images.amax(dim=(1,2), keepdim=True)[0].unsqueeze(1)))\n",
    "        \n",
    "        if self.apply_noise:\n",
    "            # Apply camera noises (now that normalized and integers)\n",
    "            spoiled_images = self.apply_random_camera_noises(spoiled_images)\n",
    "        \n",
    "        spoiled_images = spoiled_images.type(self.im_type)\n",
    "    \n",
    "        neat_images = (self.max_intensity*(neat_images/neat_images.amax(dim=(1,2), keepdim=True)[0].unsqueeze(1))).type(self.im_type)\n",
    "        \n",
    "        \n",
    "        # get iX images\n",
    "        neat_images = self.compute_raw_to_centered_iX(neat_images) #[batch_size, 2X+1, 2X+1]\n",
    "        spoiled_images = self.compute_raw_to_centered_iX(spoiled_images) #[batch_size, 2X+1, 2X+1]\n",
    "        labels = torch.Tensor([[float(self.R0s[R0_w0_Z_idxs[0]]), float(self.w0s[R0_w0_Z_idxs[1]]), \n",
    "                               float(self.Zs[R0_w0_Z_idxs[2]])]]).to(self.device) #[1,4]\n",
    "        labels = torch.hstack( ( labels.expand(self.batch_size, 3), phiCRs.squeeze(2) ) ) #[4, batch_size]\n",
    "        del D_mats, phiCRs\n",
    "        torch.cuda.empty_cache()\n",
    "        return spoiled_images, labels, neat_images #[ batch_size, 2X+1, 2X+1] and [batch_size, 4]\n",
    "        # The whole batch is already in the GPU, since to process it we wanted it to be there\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Initialize the dataset and sampler (choose the number of batches per epoch, and their length) and fix the artificial noise hyperparameters\n",
    "\n",
    "Note that since in each epoch the dataset shown to the model will be random, we can use the same dataset as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_file_path= \"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/STRUCTURE_Grid_R0_70_w0_70_Z_4.json\"\n",
    "D_matrix_file_path= \"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/AutoEncoder/Dataset_R0_70_w0_70_Z_4.h5\"\n",
    "\n",
    "total_epochs = 20\n",
    "number_of_batches_per_epoch = 21\n",
    "batch_size = 10\n",
    "validate_every_epochs=20\n",
    "extra_power_to_angles=10\n",
    "optimizer_step_every_batches = 3\n",
    "label_cost_weight=100\n",
    "\n",
    "assert(number_of_batches_per_epoch%optimizer_step_every_batches==0)\n",
    "\n",
    "\n",
    "X=302\n",
    "generate_images_w_depth=8\n",
    "random_seed=666\n",
    "\n",
    "apply_noise=True\n",
    "all_stregths_random_per_epoch=True\n",
    "max_poisson_strength=0.25\n",
    "max_blob_strength=0.25\n",
    "max_angular_modulation_strength=0.25\n",
    "poisson_strength=0.4\n",
    "blob_strength=0.2\n",
    "angular_modulation_strength=0.2\n",
    "min_modulation_frec=2*np.pi/6\n",
    "max_modulation_frec=2*np.pi/2\n",
    "max_blobs=2\n",
    "min_blob_sigma=100\n",
    "max_blob_sigma=130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pdf(x, mu, sigma, normalized_output=True):\n",
    "    p_s = (1/np.sqrt(2*np.pi)/sigma)*torch.exp(-(x-mu)**2/(2*sigma**2))\n",
    "    return p_s/p_s.sum() if normalized_output else p_s\n",
    "\n",
    "phase_vigilant = pd.DataFrame.from_dict(json.load(open(ID_file_path)))\n",
    "R0_weights = gaussian_pdf(torch.from_numpy(np.array( phase_vigilant['R0s'].drop_duplicates(), dtype=np.float64)),\n",
    "                          mu=158, sigma=35)\n",
    "w0_weights = gaussian_pdf(torch.from_numpy(np.array( phase_vigilant['w0s'].drop_duplicates(), dtype=np.float64)),\n",
    "                          mu=25, sigma=10)\n",
    "Z_weights = gaussian_pdf(torch.from_numpy(np.array( phase_vigilant['Zs'].drop_duplicates(), dtype=np.float64)),\n",
    "                          mu=0, sigma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = R0_w0_Z_Sampler( R0_weights, w0_weights, Z_weights,  num_batches_per_epoch=number_of_batches_per_epoch)\n",
    "dataset = CR_Dataset(D_matrix_file_path=D_matrix_file_path,\n",
    "            ID_file_path =ID_file_path, \n",
    "            device = device,\n",
    "            X=X, generate_images_w_depth=generate_images_w_depth, random_seed=random_seed, \n",
    "            batch_size=batch_size, num_batches_per_epoch=number_of_batches_per_epoch,\n",
    "            apply_noise=apply_noise, all_stregths_random_per_epoch=all_stregths_random_per_epoch,\n",
    "            max_poisson_strength=max_poisson_strength, max_blob_strength=max_blob_strength,\n",
    "            max_angular_modulation_strength=max_angular_modulation_strength,\n",
    "            poisson_strength=poisson_strength, blob_strength=blob_strength, \n",
    "            angular_modulation_strength=angular_modulation_strength,\n",
    "            min_modulation_frec=min_modulation_frec, max_modulation_frec=max_modulation_frec,\n",
    "            max_blobs=max_blobs, min_blob_sigma=min_blob_sigma, max_blob_sigma=max_blob_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the Hyperparameters and Initialize the Model and the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_1=15\n",
    "feats_2=20\n",
    "feats_3=20\n",
    "feats_4=20\n",
    "prop1=3\n",
    "prop2=2\n",
    "prop3=1\n",
    "av_pool1_div=4\n",
    "conv4_feat_size=15\n",
    "av_pool2_div=10\n",
    "out_fc_1=10\n",
    "dropout_p1=0.2\n",
    "dropout_p2=0.1\n",
    "latent_representation_dims=10\n",
    "out_fc_3 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters 63868250\n"
     ]
    }
   ],
   "source": [
    "model = CR2Autoencoder( X=X, feats_1=feats_1, feats_2=feats_2, feats_3=feats_3, feats_4=feats_4,\n",
    "                 prop1=prop1, prop2=prop2, prop3=prop3, av_pool1_div=av_pool1_div, conv4_feat_size=conv4_feat_size, av_pool2_div=av_pool2_div, \n",
    "                 out_fc_1=out_fc_1,\n",
    "                 dropout_p1=dropout_p1, dropout_p2=dropout_p2, \n",
    "                 latent_representation_dims=latent_representation_dims,\n",
    "                 out_fc_3=out_fc_3 ) \n",
    "\n",
    "print(f\"Number of parameters {get_n_params(model)}\")\n",
    "\n",
    "# In case we wish to transfer the learned parameters of another run\n",
    "#checkpoint = torch.load(f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/AutoEncoder/Model_opt_Adam_it_1_5sperbatchx21batchperepx6epochs_samples_feats_1=15_feats_2=20_feats_3=20_feats_4=20_prop1=3_prop2=2_prop3=1_av_pool1_div=4_conv4_feat_size=15_av_pool2_div=10_out_fc_1=10_dropout_p1=0.2_dropout_p2=0.1.pt\")\n",
    "#checkpoint = torch.load(f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/AutoEncoder/Model_opt_Adam_8sperbatchx10batchperepx20epochs_samples_feats_1={feats_1}_feats_2={feats_2}_feats_3={feats_3}_feats_4={feats_4}_prop1={prop1}_prop2={prop2}_prop3={prop3}_av_pool1_div={av_pool1_div}_conv4_feat_size={conv4_feat_size}_av_pool2_div={av_pool2_div}_out_fc_1={out_fc_1}_dropout_p1={dropout_p1}_dropout_p2={dropout_p2}.pt\")\n",
    "\n",
    "# move model to gpu if available\n",
    "model.to(device)\n",
    "\n",
    "#model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "# Initialize the weights of the model! Default initialization might already be fine!\n",
    "\n",
    "# we can use a MSE loss for the regression task we have in hands\n",
    "mse = nn.MSELoss()\n",
    "param_criterion = lambda estimation, target: CR_loss(estimation, target, \n",
    "                                    extra_power_to_angles=extra_power_to_angles, loss_calculator=mse)\n",
    "# we will choose as optimizer the \n",
    "#optimizer = torch.optim.Adagrad(model.parameters(), lr=0.1, lr_decay=0.01, weight_decay=0.3,\n",
    "#                                initial_accumulator_value=0, eps=1e-10)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, betas=(0.9, 0.99), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "#optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data retrieval time -0.02292656898498535\n",
      "Data retrieval time -18.018421411514282\n",
      "Data retrieval time -57.780521869659424\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_21605/3173955789.py\u001b[0m in \u001b[0;36mfull_training_loop\u001b[0;34m(model, label_criterion, reconstruction_criterion, optimizer, sampler, dataset, epochs, print_loss_every_batches, validate_every_epochs, optimizer_step_every_batches, label_cost_weight)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         train_loss = train_epoch(epoch, label_criterion, reconstruction_criterion, model, optimizer,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                  \u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_loss_every_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprint_loss_every_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                  optimizer_step_every_batches=optimizer_step_every_batches, label_cost_weight=label_cost_weight)\n",
      "\u001b[0;32m/tmp/ipykernel_21605/2194228108.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, label_criterion, reconstruction_criterion, model, optimizer, sampler, dataset, print_loss_every_batches, optimizer_step_every_batches, label_cost_weight)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mt7\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batch_idxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtarget_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_batch_idxs\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# each data, target is a whole batch of samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;31m# data is [batch_size, 2X+1, 2X+1], target_labels is [batch_size, 4], target_images is [batch_size, 2X+1, 2X+1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mt1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_21605/3369026716.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, R0_w0_Z_idxs)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh5f_D_matrices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[1,Nx,Ny]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         D_mats = torch.from_numpy(self.h5f_D_matrices[\n\u001b[0m\u001b[1;32m    243\u001b[0m                 f\"R0_{self.R0s[R0_w0_Z_idxs[0]]}_w0_{self.w0s[R0_w0_Z_idxs[1]]}_Z_{self.Zs[R0_w0_Z_idxs[2]]}\"][:]\n\u001b[1;32m    244\u001b[0m                                  ).unsqueeze(1).to(self.device) #[2, 1, Nx, Ny]            \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "losses = full_training_loop(model, param_criterion, mse, optimizer, sampler, dataset, \n",
    "                    epochs=total_epochs, print_loss_every_batches=10, validate_every_epochs=validate_every_epochs,\n",
    "                           optimizer_step_every_batches=optimizer_step_every_batches,\n",
    "                           label_cost_weight=label_cost_weight)\n",
    "# Execute the training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "FINAL VALIDATION! ####################################################\n",
      "\n",
      "\n",
      "Train Set\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\\nFINAL VALIDATION! ####################################################\\n\\n\")\n",
    "print(\"Train Set\")\n",
    "validate_epoch(param_criterion, mse, model, sampler, dataset)\n",
    "print(\"Test Set\")\n",
    "validate_epoch(param_criterion, mse, model, sampler, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the resulting model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'optimizer':optimizer.state_dict(),\n",
    "            'model': model.state_dict(),\n",
    "            }, f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/AutoEncoder/Model_opt_Adam_it_2_{batch_size}sperbatchx{number_of_batches_per_epoch}batchperepx{total_epochs}epochs_samples_feats_1={feats_1}_feats_2={feats_2}_feats_3={feats_3}_feats_4={feats_4}_prop1={prop1}_prop2={prop2}_prop3={prop3}_av_pool1_div={av_pool1_div}_conv4_feat_size={conv4_feat_size}_av_pool2_div={av_pool2_div}_out_fc_1={out_fc_1}_dropout_p1={dropout_p1}_dropout_p2={dropout_p2}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charge models and do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f\"/home/melanie/Desktop/Conical_Refraction_Polarimeter/OUTPUT/ML_Models.pt\")\n",
    "\n",
    "model = CR2Autoencoder( X=X, feats_1=feats_1, feats_2=feats_2, feats_3=feats_3, feats_4=feats_4,\n",
    "                 prop1=prop1, prop2=prop2, prop3=prop3, av_pool1_div=av_pool1_div, conv4_feat_size=conv4_feat_size, av_pool2_div=av_pool2_div, \n",
    "                 out_fc_1=out_fc_1,\n",
    "                 dropout_p1=dropout_p1, dropout_p2=dropout_p2, \n",
    "                 latent_representation_dims=latent_representation_dims,\n",
    "                 out_fc_3=out_fc_3 ) \n",
    "\n",
    "model.load_state_dict(checkpoint['model']).to(device)\n",
    "\n",
    "# And we will use the same dataset to convert the experimental images to iX (not apply noise ofcourse)\n",
    "dataset = CR_Dataset(D_matrix_file_path=D_matrix_file_path,\n",
    "            ID_file_path =ID_file_path, \n",
    "            device = device,\n",
    "            X=x, generate_images_w_depth=generate_images_w_depth, random_seed=random_seed \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display a FileChooser widget\n",
    "from ipyfilechooser import FileChooser\n",
    "path=\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter\"\n",
    "fc = FileChooser(path+'/LAB/EXPERIMENTAL/Fotos_Turpin/Day2/laser_gaussian_thesis/')\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a single experimental image to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image_full_path=fc.selected\n",
    "#image_full_path=\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/Experimental_Stuff/Fotos_Turpin/Day2/laser_gaussian_thesis/All_Taken_Photos/sin_el_positivo.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "if im is None:\n",
    "    print(f\" Unable to import image {image_full_path}\")\n",
    "    raise ValueError\n",
    "# Center in gravicenter, generating iX\n",
    "im = np.asarray((dataset.compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot its Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d_resolution=0.7\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "prof_x=np.sum(im, axis=0)\n",
    "prof_y=np.sum(im, axis=1)\n",
    "fig = plt.figure(figsize=(2*4.5, 2*4.5))\n",
    "axes=fig.subplots(2,2)\n",
    "cm=axes[0, 0].imshow(im, cmap='viridis')\n",
    "axes[0,0].grid(True)\n",
    "axes[0,1].scatter(prof_y, np.arange(len(prof_y)), s=1, label=f'Intensity profile in y')\n",
    "axes[0,1].set_ylim((0,len(prof_y)))\n",
    "axes[0,1].invert_yaxis()\n",
    "axes[1,0].scatter(np.arange(len(prof_x)), prof_x, s=1, label=f'Intensity profile in y')\n",
    "axes[1,0].set_xlim((0,len(prof_x)))\n",
    "axes[1,0].invert_yaxis()\n",
    "axes[0,0].set_xlabel(\"x (pixels)\")\n",
    "#axes[0,0].set_ylabel(\"y (pixels)\")\n",
    "axes[0,1].set_xlabel(\"Cummulative Intensity\")\n",
    "axes[0,1].set_ylabel(\"y (pixels)\")\n",
    "axes[1,0].set_ylabel(\"Cummulative Intensity\")\n",
    "axes[1,0].set_xlabel(\"x (pixels)\")\n",
    "axes[1,0].grid(True)\n",
    "axes[0,1].grid(True)\n",
    "axes[1,1].set_visible(False)\n",
    "ax = fig.add_subplot(224, projection='3d')\n",
    "X_g,Y = np.meshgrid(np.arange(len(prof_y)),np.arange(len(prof_x)))\n",
    "fig.suptitle(f\"Intesity Profiles for Image\\n{image_full_path.split('/')[-1]}\")\n",
    "files_for_gif=[]\n",
    "cbax=fig.add_axes([0.54,0.05,0.4,0.01])\n",
    "fig.colorbar(cm, ax=axes[0,0], cax=cbax, orientation='horizontal')\n",
    "theta=25\n",
    "phi=30\n",
    "ax.plot_surface(X_g, Y, im.T, rcount=int(len(prof_y)*plot3d_resolution), ccount=int(len(prof_x)*plot3d_resolution), cmap='viridis') # rstride=1, cstride=1, linewidth=0\n",
    "#cset = ax.contourf(X, Y, im, 2, zdir='z', offset=-20, cmap='viridis', alpha=0.5)\n",
    "#cset = ax.contourf(X, Y, im, 1, zdir='x', offset=-8, cmap='viridis')\n",
    "#cset = ax.contourf(X, Y, im, 1, zdir='y', offset=0, cmap='viridis')\n",
    "ax.set_xlabel('Y')\n",
    "#ax.set_xlim(-8, 8)\n",
    "ax.set_ylabel('X')\n",
    "#ax.set_ylim(-10, 8)\n",
    "ax.set_zlabel('Intensity')\n",
    "ax.set_zlim(-0.078*np.max(im), np.max(im))\n",
    "ax.set_title(\"Image intensity 3D plot\")\n",
    "ax.view_init(10, theta)\n",
    "#ax.get_proj = lambda: np.dot(Axes3D.get_proj(ax), np.diag([1.3, 1.3, 1.3, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NN predictions for $R_0, w_0, \\phi_{CR}, Z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sampler:\n",
    "    ims, labs, predicted_images = dataset[i]\n",
    "    break\n",
    "nims = np.asarray(ims.to('cpu'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "predictied_labels, predicted_images = model(torch.zeros(im.shape).to(device).unsqueeze(0))\n",
    "im = np.asarray(torch.zeros(im.shape))\n",
    "predictied_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictied_labels, predicted_images = model(ims)\n",
    "# normalize the predicted images and turn them to uint8\n",
    "predicted_images = (dataset.max_intensity*(predicted_images/predicted_images.amax(dim=(1,2), keepdim=True)[0].unsqueeze(1))).type(dataset.im_type)\n",
    "predicted_images = np.asarray(predicted_images.to('cpu'))\n",
    "\n",
    "print(f\"Predicted R_0 {predictied_labels[0][0]}, w_0 {predictied_labels[0][1]}, Z {predictied_labels[0][2]}, phiCR {predictied_labels[0][3]}\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {predictied_labels[0][3]/2}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "\n",
    "ims_to_show=3\n",
    "fig, axes = plt.subplots(ims_to_show,3,figsize=(15,15))\n",
    "for i in range(ims_to_show):\n",
    "    if ims_to_show==1:\n",
    "        axes=np.expand_dims(axes, 0)\n",
    "    axes[i, 1].imshow(predicted_images[i].T)\n",
    "    axes[i,1].set_title(f\"Predicted R0={predictied_labels[i][0].item():.5} w0={predictied_labels[i][1].item():.4} Z={predictied_labels[i][2].item():.2} phiCR={predictied_labels[i][3].item():.6}\")\n",
    "    axes[i, 0].imshow(nims[i])\n",
    "    axes[i,0].set_title(f\"Noisy Original\")\n",
    "    axes[i, 2].set_visible(False)\n",
    "    ax = fig.add_subplot(int(f\"{ims_to_show}3{i*3+3}\"), projection='3d')\n",
    "    X_g,Y = np.meshgrid(np.arange(predicted_images.shape[1]),np.arange(predicted_images.shape[2]))\n",
    "    theta=25\n",
    "    phi=30\n",
    "    plot3d_resolution = 0.4\n",
    "    ax.plot_surface(X_g, Y, predicted_images[i].T, rcount=int(predicted_images.shape[1]*plot3d_resolution), ccount=int(predicted_images.shape[2]*plot3d_resolution), cmap='viridis') # rstride=1, cstride=1, linewidth=0\n",
    "    #cset = ax.contourf(X, Y, im, 2, zdir='z', offset=-20, cmap='viridis', alpha=0.5)\n",
    "    #cset = ax.contourf(X, Y, im, 1, zdir='x', offset=-8, cmap='viridis')\n",
    "    #cset = ax.contourf(X, Y, im, 1, zdir='y', offset=0, cmap='viridis')\n",
    "    ax.set_xlabel('Y')\n",
    "    #ax.set_xlim(-8, 8)\n",
    "    ax.set_ylabel('X')\n",
    "    #ax.set_ylim(-10, 8)\n",
    "    ax.set_zlabel('Intensity')\n",
    "    ax.set_zlim(-0.078*np.max(predicted_images[i]), np.max(predicted_images[i]))\n",
    "    ax.set_title(\"Predicted Reconstruction intensity\")\n",
    "    ax.view_init(10, theta)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the non-black-box algorithm estimate for $\\phi_{CR}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(f\"../../..\")\n",
    "import sys\n",
    "from SOURCE.CLASS_CODE_GPU_Classes import *\n",
    "from SOURCE.CLASS_CODE_Image_Manager import *\n",
    "from SOURCE.CLASS_CODE_Polarization_Obtention_Algorithms import Rotation_Algorithm, Mirror_Flip_Algorithm, Gradient_Algorithm\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image=im.copy()\n",
    "saturation=0.9\n",
    "pol_or_CR=\"pol\" \n",
    "deg_or_rad=\"deg\" # for the final output\n",
    "image_depth=8 # or 16 bit per pixel\n",
    "image_shortest_side=540\n",
    "randomization_seed=666\n",
    "recenter_average_image=False\n",
    "\n",
    "\n",
    "# 5. POLARIZATION RELATIVE ANGLES ###################################\n",
    "# Mirror with affine interpolation & Rotation Algorithms will be employed\n",
    "# Each using both Fibonacci and Quadratic Fit Search\n",
    "# Also a gradient algorithm\n",
    "theta_min_Rot=-np.pi\n",
    "theta_max_Rot=np.pi\n",
    "rad_min_Grav=3\n",
    "rad_max_Grav=image_shortest_side\n",
    "theta_min_Mir=0\n",
    "theta_max_Mir=np.pi\n",
    "initial_guess_delta_rad=0.1\n",
    "initial_guess_delta_pix=10\n",
    "use_exact_gravicenter=True\n",
    "precision_quadratic=1e-10\n",
    "max_it_quadratic=100\n",
    "cost_tolerance_quadratic=1e-14\n",
    "precision_fibonacci=1e-10\n",
    "max_points_fibonacci=100\n",
    "cost_tolerance_fibonacci=1e-14\n",
    "\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "im_type=np.uint16 if image_depth==16 else np.uint8\n",
    "max_intensity=65535 if image_depth==16 else 255\n",
    "np.random.seed(randomization_seed)\n",
    "polCR=1 if pol_or_CR=='CR' else 0.5\n",
    "\n",
    "# 6. POLARIZATION RELATIVE ANGLES ###################################\n",
    "# Mirror with affine interpolation & Rotation Algorithms will be employed\n",
    "# Each using both Fibonacci and Quadratic Fit Search\n",
    "# Results will be gathered in a table and outputed as an excel csv\n",
    "# Mock Image Loader\n",
    "# Computar el angulo de cada uno en un dataframe donde una de las entradas sea results y haya un result per fibo qfs y per rotation y mirror affine. Y luego procesar en un 7¬∫ paso estos angulos para obtener los angulos relativos etc y perhaps hacer tablucha con ground truth menos el resulting delta angle medido por el algoritmo\n",
    "image_loader = Image_Manager(mode=X, interpolation_flag=None)\n",
    "# Define the ROTATION ALGORITHM\n",
    "rotation_algorithm = Rotation_Algorithm(image_loader,\n",
    "    theta_min_Rot, theta_max_Rot, None,\n",
    "    initial_guess_delta_rad, use_exact_gravicenter, initialize_it=False)\n",
    "\n",
    "# Define the Affine Mirror algorithm\n",
    "mirror_algorithm = Mirror_Flip_Algorithm(image_loader,\n",
    "    theta_min_Mir, theta_max_Mir, None,\n",
    "    initial_guess_delta_rad, method=\"aff\", left_vs_right=True, use_exact_gravicenter=use_exact_gravicenter, initialize_it=False)\n",
    "\n",
    "# Define the Gradient algorithm\n",
    "gradient_algorithm = Gradient_Algorithm(image_loader,\n",
    "        rad_min_Grav, rad_max_Grav,\n",
    "        initial_guess_delta_pix,\n",
    "        use_exact_gravicenter)\n",
    "\n",
    "# A dictionary to gather all the resulting angles for each image\n",
    "\n",
    "individual_image_results = { 'polarization_method':[], 'optimization_1d':[], 'found_phiCR':[], 'predicted_opt_precision':[] }\n",
    "\n",
    "def to_result_dict(result_dict, alg, alg_name, opt_name, im_names):\n",
    "    for key, name in zip(alg.times.keys(), im_names):\n",
    "        result_dict['polarization_method'].append(alg_name)\n",
    "        result_dict['optimization_1d'].append(opt_name)\n",
    "        result_dict['found_phiCR'].append(alg.angles[key])\n",
    "        result_dict['predicted_opt_precision'].append(alg.precisions[key])\n",
    "image_container=np.zeros( (1, 2*X+1, 2*X+1), dtype=np.float64)\n",
    "image_names=[]\n",
    "# charge the image\n",
    "image_container[0]=image.astype(np.float64)\n",
    "image_names.append(f\"{fc.selected_filename}\")\n",
    "\n",
    "# charge the image loader:\n",
    "image_loader.import_converted_images_as_array(image_container, image_names)\n",
    "# Execute the Rotation and Mirror Algorithms:\n",
    "# ROTATION ######\n",
    "interpolation_flag=None\n",
    "# the interpolation algorithm used in case we disbale its usage for the iX image obtention will be the Lanczos one\n",
    "rotation_algorithm.interpolation_flag=interpolation_flag if interpolation_flag is not None else cv2.INTER_CUBIC\n",
    "rotation_algorithm.reInitialize(image_loader)\n",
    "rotation_algorithm.quadratic_fit_search(precision_quadratic, max_it_quadratic, cost_tolerance_quadratic)\n",
    "to_result_dict( individual_image_results, rotation_algorithm, \"Rotation\", \"Quadratic\", image_names)\n",
    "rotation_algorithm.reInitialize(image_loader)\n",
    "rotation_algorithm.fibonacci_ratio_search(precision_fibonacci, max_points_fibonacci, cost_tolerance_fibonacci)\n",
    "to_result_dict( individual_image_results, rotation_algorithm, \"Rotation\", \"Fibonacci\", image_names)\n",
    "\n",
    "# MIRROR #######\n",
    "mirror_algorithm.interpolation_flag=interpolation_flag if interpolation_flag is not None else cv2.INTER_CUBIC\n",
    "mirror_algorithm.reInitialize(image_loader)\n",
    "mirror_algorithm.quadratic_fit_search(precision_quadratic, max_it_quadratic, cost_tolerance_quadratic)\n",
    "to_result_dict( individual_image_results, rotation_algorithm, \"Mirror\", \"Quadratic\", image_names)\n",
    "mirror_algorithm.reInitialize(image_loader)\n",
    "mirror_algorithm.fibonacci_ratio_search(precision_fibonacci, max_points_fibonacci, cost_tolerance_fibonacci)\n",
    "to_result_dict( individual_image_results, rotation_algorithm, \"Mirror\", \"Fibonacci\", image_names)\n",
    "\n",
    "# GRADIENT #######\n",
    "optimal_masked_gravs={}\n",
    "optimal_radii={}\n",
    "grav=compute_intensity_gravity_center(image)\n",
    "\n",
    "gradient_algorithm.interpolation_flag=interpolation_flag if interpolation_flag is not None else cv2.INTER_CUBIC\n",
    "gradient_algorithm.reInitialize(image_loader)\n",
    "gradient_algorithm.quadratic_fit_search(precision_quadratic, max_it_quadratic, cost_tolerance_quadratic)\n",
    "to_result_dict( individual_image_results, gradient_algorithm, \"Gradient\", \"Quadratic\", image_names)\n",
    "#optimal_masked_gravs['quad'] = gradient_algorithm.masked_gravs[f\"Quadratic_Search_{fc.selected_filename}\"]\n",
    "#optimal_radii['quad'] = gradient_algorithm.optimals[f\"Quadratic_Search_{fc.selected_filename}\"]\n",
    "\n",
    "gradient_algorithm.reInitialize(image_loader)\n",
    "gradient_algorithm.fibonacci_ratio_search(precision_fibonacci, max_points_fibonacci, cost_tolerance_fibonacci)\n",
    "to_result_dict( individual_image_results, gradient_algorithm, \"Gradient\", \"Fibonacci\", image_names)\n",
    "\n",
    "#optimal_masked_gravs['fibo'] = gradient_algorithm.masked_gravs[f\"Fibonacci_Search_{fc.selected_filename}\"]\n",
    "#optimal_radii['fibo'] = gradient_algorithm.optimals[f\"Fibonacci_Search_{fc.selected_filename}\"]\n",
    "\n",
    "#masked_grav=(optimal_masked_gravs['quad']+optimal_masked_gravs['fibo'])/2.0\n",
    "#optimal_radi = (optimal_radii['quad']+optimal_radii['fibo'])/2\n",
    "#print(f\"\\n\\nOptimal masked gravs: {optimal_masked_gravs}\\nOptimal radii: {optimal_radii}\\n\\n\\n\")\n",
    "print(pd.DataFrame.from_dict(individual_image_results))\n",
    "\n",
    "# 7. PROCESS FINAL RESULTS ##########################################\n",
    "def angle_to_pi_pi( angle): # convert any angle to range ()-pi,pi]\n",
    "    angle= angle%(2*np.pi) # take it to [-2pi, 2pi]\n",
    "    return angle-np.sign(angle)*2*np.pi if abs(angle)>np.pi else angle    \n",
    "\n",
    "average_found_phiCR=np.mean([angle_to_pi_pi(phi) for i,phi in enumerate(individual_image_results['found_phiCR']) if individual_image_results['polarization_method'][i]!='Gradient'])\n",
    "print(\"Average found phiCR:\", average_found_phiCR)\n",
    "#print(f\"\\n\\nPredicted slope for main axis: by Gradient {(masked_grav[0]-grav[0])/(masked_grav[1]-grav[1])} and by the others averaged {np.tan(-average_found_phiCR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def forward(self, x): # [batch_size, 2X+1, 2X+1] or [batch_size, 1, 2X+1, 2X+1]\n",
    "    x = x.view(x.shape[0], 1, x.shape[-2], x.shape[-1]) # [batch_size, 1, 2X+1, 2X+1]\n",
    "    X=302\n",
    "    feats_1=15\n",
    "    feats_2=20\n",
    "    feats_3=20\n",
    "    feats_4=20\n",
    "    prop1=3\n",
    "    prop2=2\n",
    "    prop3=1\n",
    "    av_pool1_div=4\n",
    "    conv4_feat_size=15\n",
    "    av_pool2_div=10\n",
    "    out_fc_1=10 \n",
    "    print(x.shape, 2*X+1)\n",
    "\n",
    "    x = self.relu( self.conv1(x) ) # [batch_size, feats_1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "    print(\"conv1\",x.shape, prop1*(2*X+1)/5)\n",
    "\n",
    "\n",
    "    x = self.batchNorm2( self.relu( self.conv2(self.dropout1(x)) )) # [batch_size, feats_2, prop2*(2X+1)/5, prop2*(2X+1)/5]\n",
    "    print(\"conv2\",x.shape,  prop2*(2*X+1)/5)\n",
    "\n",
    "\n",
    "    x = self.relu( self.conv3(self.dropout2(x)) ) # [batch_size, feats_3, prop3*(2X+1)/5, prop3*(2X+1)/5]\n",
    "    print(\"conv3\",x.shape,  prop3*(2*X+1)/5)\n",
    "\n",
    "\n",
    "    x = self.avPool1(x) # [batch_size, feats_3, prop3*(2X+1)/5)/av_pool1_div, prop3*(2X+1)/5)/av_pool1_div]\n",
    "    print(\"av_pool1\",x.shape, int((prop3*(2*X+1)/5)/av_pool1_div))\n",
    "\n",
    "\n",
    "    x = self.batchNorm4(self.conv4(self.dropout2(x))) # [batch_size, feats_4, conv4_feat_size, conv4_feat_size]\n",
    "    print(\"conv4+batchn\",x.shape, conv4_feat_size)\n",
    "\n",
    "\n",
    "    x = self.relu( self.avPool2(x) ) # [batch_size, feats_4, conv4_feat_size/av_pool2_div, conv4_feat_size/av_pool2_div]\n",
    "    print(\"av_pool2\",x.shape, int(conv4_feat_size/av_pool2_div)+1)\n",
    "\n",
    "\n",
    "    x = x.view(x.shape[0], self.in_fc) #[batch_size, feats_4*int(conv4_feat_size/av_pool2_div)**2]\n",
    "    print(\"view_change\",x.shape, feats_4*int(conv4_feat_size/av_pool2_div+1)**2)\n",
    "\n",
    "\n",
    "    x = self.fc2( self.relu( self.fc1(x) ) ) #[batch_size, 4]\n",
    "    print(x.shape, 4)\n",
    "\n",
    "        return x\n",
    "\n",
    "def image_decoder(self, latent_representation, encoded1, encoded2, encoded3, encoded4 ):\n",
    "        X=302\n",
    "        feats_1=15\n",
    "        feats_2=20\n",
    "        feats_3=20\n",
    "        feats_4=20\n",
    "        prop1=3\n",
    "        prop2=2\n",
    "        prop3=1\n",
    "        av_pool1_div=4\n",
    "        conv4_feat_size=15\n",
    "        av_pool2_div=10\n",
    "        out_fc_1=10 \n",
    "        latent_representation_dims=10\n",
    "        out_fc_3 = 10\n",
    "        \n",
    "        # latent_representation is [batch_size, latent_representation_dims]\n",
    "        # first a fc to turn it back to the shape tkhe last convolution of the encoder left\n",
    "        x = self.relu(self.fc5(latent_representation)) # [batch_size, feats_4*int(conv4_feat_size/av_pool2_div+1)**2]\n",
    "        print(\"fc5\",x.shape, feats_4*int(conv4_feat_size/av_pool2_div+1)**2, self.in_fc)\n",
    "        \n",
    "        x = x.view( encoded1.shape ) # [batch_size, feats_4, conv4_feat_size/av_pool2_div, conv4_feat_size/av_pool2_div]\n",
    "        print(\"reshape\",x.shape, feats_4, int(conv4_feat_size/av_pool2_div))\n",
    "        \n",
    "        x = torch.hstack( (x, encoded1)) # [batch_size, 2*feats_4, conv4_feat_size/av_pool2_div, conv4_feat_size/av_pool2_div]\n",
    "        print(\"stack\",x.shape, feats_4*2, int(conv4_feat_size/av_pool2_div))\n",
    "        \n",
    "        x = self.batchNorm4(self.dropout2(self.deConv5(x))) # [batch_size, feats4, conv4_feat_size]\n",
    "        print(\"conv5\",x.shape, feats_4, conv4_feat_size)\n",
    "\n",
    "        x = torch.hstack( (x, encoded2) ) # [batch_size, 2*feats4, conv4_feat_size]\n",
    "        print(\"stack\",x.shape, 2*feats_4, conv4_feat_size)\n",
    "        \n",
    "        x = self.batchNorm2(self.relu(self.dropout2(self.deConv6(x)))) # [batch_size, feats2, prop2*(2X+1)/5, prop2*(2X+1)/5]\n",
    "        print(\"conv6\",x.shape, feats_2, prop2*(2*X+1)/5)\n",
    "        \n",
    "        x = torch.hstack( (x, encoded3) ) # [batch_size, feats2*2, prop2*(2X+1)/5, prop2*(2X+1)/5]\n",
    "        print(\"stack\",x.shape, 2*feats_2, prop2*(2*X+1)/5)\n",
    "        \n",
    "        x = self.relu(self.dropout1(self.deConv7(x))) # [batch_size, feats1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "        print(\"conv7\",x.shape, feats_1, prop1*(2*X+1)/5)\n",
    "\n",
    "        x = torch.hstack( (x, encoded4) ) # [batch_size, feats1*2, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "        print(\"stack\",x.shape, 2*feats_1, prop1*(2*X+1)/5)\n",
    "        \n",
    "        x = self.deConv8(x) # [batch_size, 1, 2X+1, 2X+1]\n",
    "        print(\"output\", x.shape, 1, (2*X+1))\n",
    "        return x\n",
    "        \n",
    "        \n",
    "a = CR2Autoencoder().to(device)\n",
    "a(torch.ones(2,1, 605,605).to(device))\n",
    "del a\n",
    "torch.cuda.empty_cache()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
