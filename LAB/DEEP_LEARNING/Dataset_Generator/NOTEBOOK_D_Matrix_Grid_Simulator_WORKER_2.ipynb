{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(f\"../../..\")\n",
    "from SOURCE.CLASS_CODE_GPU_Classes import *\n",
    "from SOURCE.CLASS_CODE_Image_Manager import *\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from time import time\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PARAMETERS ############################################\n",
    "##################################################################\n",
    "experiment_name=\"Basler_like_Proof_of_Concept\"\n",
    "N_R0 = 115 #70\n",
    "N_w0 = 115 #70\n",
    "N_Z = 1 #4\n",
    "\n",
    "output_directory=f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/{experiment_name}/\"\n",
    "#output_directory=f\"./OUTPUT/LIBRARIES_OF_THEORETICAL_D/{experiment_name}/\"\n",
    "#os.chdir(f\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/\")\n",
    "\n",
    "randomization_seed=666\n",
    "image_shortest_side=191\n",
    "saturation=1\n",
    "\n",
    "# Ring parameters to test (each will be a different simulation)\n",
    "#phiCR_s=np.linspace(-180,180,360*10**significant_decimal+1)*np.pi/180\n",
    "R0_s= np.linspace(40, 52, N_R0)#np.linspace(110, 180, N_R0) #np.linspace(70,180,40) # in pxels 153\n",
    "w0_s= np.linspace(2, 9, N_w0)#np.linspace( 8, 35, N_w0) #np.linspace(8,50,40) 11\n",
    "Z_s=np.linspace(0, 0.05, N_Z)\n",
    "rho_0s=R0_s/w0_s\n",
    "\n",
    "\n",
    "resolution_side_nx=image_shortest_side # generated images will be resolution_side x resolution_side\n",
    "# Other parameters\n",
    "max_k=50\n",
    "num_k=1200\n",
    "sim_chunk_ax=image_shortest_side\n",
    "\n",
    "\n",
    "\n",
    "##################################################\n",
    "##################################################\n",
    "\n",
    "# General considerations\n",
    "image_directory=f\"{output_directory}/SIMULATIONS/\" #nx_{image_shortest_side}_depth_{image_depth}_sat_{saturation}\n",
    "os.makedirs(image_directory, exist_ok=True)\n",
    "np.random.seed(randomization_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this data generation in parallel, we will generate the data in parallel processes.\n",
    "Eventually, when all is finished, we will manually generate a single h5f from the individual parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 0 -> 0 to 12\n",
      "Worker 1 -> 12 to 23\n",
      "Worker 2 -> 23 to 34\n",
      "Worker 3 -> 34 to 45\n",
      "Worker 4 -> 45 to 56\n"
     ]
    }
   ],
   "source": [
    "K=5 # workers\n",
    "N=56 # tareas\n",
    "\n",
    "for j in range(K):\n",
    "    print(f\"Worker {j} -> {(N//K)*j + j*((N%K-j)>0) + (N%K)*(j>=N%K) } to {(N//K)*(j+1) + (j+1)*((N%K-j-1)>0)+ (N%K)*(j+1>=N%K)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_WORKERS = 4\n",
    "WORKER_ID = 2 # beginning from 0 till TOTAL_WORKERS-1\n",
    "\n",
    "def beg_index(N,j,K):\n",
    "    # N -> number of total elements in vector, j is worker id, K is total number of workers\n",
    "    return (N//K)*j + j*((N%K-j)>0) + (N%K)*(j>=N%K)\n",
    "def end_index(N,j,K):\n",
    "    return (N//K)*(j+1) + (j+1)*((N%K-j-1)>0)+ (N%K)*(j+1>=N%K)\n",
    "\n",
    "# we just need to partitionate one of the vectors! If you do it with all of them you do not get the whole!!!\n",
    "R0_s = R0_s[beg_index(N_R0, WORKER_ID, TOTAL_WORKERS):end_index(N_R0, WORKER_ID, TOTAL_WORKERS)] \n",
    "# w0_s = w0_s[beg_index(N_w0, WORKER_ID, TOTAL_WORKERS):end_index(N_w0, WORKER_ID, TOTAL_WORKERS)] \n",
    "# Z_s = Z_s[beg_index(N_Z, WORKER_ID, TOTAL_WORKERS):end_index(N_Z, WORKER_ID, TOTAL_WORKERS)]\n",
    "# rho_0s=R0_s/w0_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using h5f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################################################################################################] 100.0% \n",
      "\n",
      "Simulated: 3336/3335\n",
      "Elapsed time: 2.0 h 30.0 min -111.0 s\n",
      "\n",
      "\n",
      "WORKER 2 FINISHED!!!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the vigilant\n",
    "try:\n",
    "    phase_vigilant = json.load(open(f\"{output_directory}/STRUCTURE_Grid_PART_{WORKER_ID}.json\"))\n",
    "except:\n",
    "    phase_vigilant = {'R0s':[], 'w0s':[], 'Zs':[], 'IDs':[]}\n",
    "\n",
    "# Set the objects ready ##################\n",
    "# The simulator object\n",
    "simulator =RingSimulator_Optimizer_GPU( n=1.5, a0=1.0, max_k=max_k, num_k=num_k, nx=resolution_side_nx, \n",
    "                                      sim_chunk_x=sim_chunk_ax, sim_chunk_y=sim_chunk_ax)\n",
    "\n",
    "# Initialize the hdf5 dataset saver\n",
    "h5f = h5py.File(f\"{image_directory}/Dataset_PART_{WORKER_ID}.h5\", 'a') # append if exists, create if not\n",
    "\n",
    "# save phis\n",
    "try:\n",
    "    h5f.create_dataset('phis', data=simulator.phis[:,:,0], compression=\"lzf\", shuffle=True) #, compression_opts=9)\n",
    "except:\n",
    "    print(f\"phis was already in h5f but not in phase vigilant\")\n",
    "    h5f['phis'][:] = simulator.phis[:,:,0]\n",
    "\n",
    "# Execute the stuff #####################\n",
    "i=1\n",
    "total=Z_s.shape[0]*R0_s.shape[0]*w0_s.shape[0]\n",
    "elapsed=0\n",
    "beg=time()\n",
    "output_info_every=25\n",
    "dump_every=25\n",
    "\n",
    "for Z in Z_s:\n",
    "    for R0 in R0_s:\n",
    "        for w0 in w0_s:\n",
    "            ID=f\"R0_{str(R0)}_w0_{str(w0)}_Z_{str(Z)}\"\n",
    "            if ID not in phase_vigilant['IDs']:\n",
    "                # simulate matrix\n",
    "                D_matrix = simulator.compute_pieces_for_I_LP(R0_pixels=R0, Z=Z, w0_pixels=w0)\n",
    "                \n",
    "                if D_matrix is None:\n",
    "                    raise ValueError\n",
    "                    \n",
    "                # save the matrix\n",
    "                try:\n",
    "                    h5f.create_dataset(ID, data=D_matrix, compression=\"lzf\", shuffle=True) #, compression_opts=9)\n",
    "                except: # in case the phase_vigilant did not record it, but it was already in h5f\n",
    "                    print(f\"{ID} was already in h5f but not in phase vigilant\")\n",
    "                    h5f[ID][:] = D_matrix\n",
    "\n",
    "                #append the data\n",
    "                phase_vigilant['IDs'].append(ID)\n",
    "                phase_vigilant['R0s'].append(str(R0))\n",
    "                phase_vigilant['Zs'].append(str(Z))\n",
    "                phase_vigilant['w0s'].append(str(w0))\n",
    "                \n",
    "                \n",
    "                if i%output_info_every==0:\n",
    "                    display.clear_output(wait=True)\n",
    "                    elapsed=time()-beg\n",
    "                    print(f\"[\"+'#'*(int(100*i/total))+' '*(100-int(100*i/total))+f\"] {100*i/total:3.4}% \\n\\nSimulated: {i}/{total}\\nElapsed time: {elapsed//3600} h {elapsed//60-(elapsed//3600)*60} min {elapsed-(elapsed//60)*60-(elapsed//3600)*60:2.4} s\")\n",
    "                    if i%dump_every==0:\n",
    "                        h5f.flush()\n",
    "                        # we save the progess (in order to be able to quit and resume)\n",
    "                        json.dump(phase_vigilant, open( f\"{output_directory}/STRUCTURE_Grid_PART_{WORKER_ID}.json\", \"w\"))\n",
    "            i+=1\n",
    "            \n",
    "display.clear_output(wait=True)\n",
    "elapsed=time()-beg\n",
    "print(f\"[\"+'#'*(int(100*i/total))+' '*(100-int(100*i/total))+f\"] {100*i/total:3.4}% \\n\\nSimulated: {i}/{total}\\nElapsed time: {elapsed//3600} h {elapsed//60-(elapsed//3600)*60} min {elapsed-(elapsed//60)*60-(elapsed//3600)*60:2.4} s\")               \n",
    "h5f.flush()\n",
    "# we save the progess (in order to be able to quit and resume)\n",
    "json.dump(phase_vigilant, open( f\"{output_directory}/STRUCTURE_Grid_PART_{WORKER_ID}.json\", \"w\"))\n",
    "\n",
    "h5f.close()\n",
    "print(f\"\\n\\nWORKER {WORKER_ID} FINISHED!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Not a file or file object (not a file or file object)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m h5f\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/h5py/_hl/files.py:561\u001b[0m, in \u001b[0;36mFile.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;124;03m\"\"\" Tell the HDF5 library to flush its buffers.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m phil:\n\u001b[0;32m--> 561\u001b[0m     \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:164\u001b[0m, in \u001b[0;36mh5py.h5f.flush\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Not a file or file object (not a file or file object)"
     ]
    }
   ],
   "source": [
    "h5f.flush()\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the code to join all the parts individually generated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError\n",
    "# Open the big h5f that will contain all of the parts\n",
    "h5f = h5py.File(f\"{image_directory}/Dataset_R0_{N_R0}_w0_{N_w0}_Z_{N_Z}.h5\", 'w') # append if exists, create if not\n",
    "total_phase = {'R0s':[], 'w0s':[], 'Zs':[], 'IDs':[]}\n",
    "for j in range(TOTAL_WORKERS):\n",
    "    h5f_worker = h5py.File(f\"{image_directory}/Dataset_PART_{j}.h5\", 'r')\n",
    "    phase_worker = json.load(open(f\"{output_directory}/STRUCTURE_Grid_PART_{j}.json\"))\n",
    "    if j==0:\n",
    "        h5f.create_dataset('phis', data=h5f_worker['phis'][:], compression=\"lzf\", shuffle=True)\n",
    "    for ID in phase_worker['IDs']:\n",
    "        h5f.create_dataset(ID, data=h5f_worker[ID][:], compression=\"lzf\", shuffle=True)\n",
    "    total_phase['R0s'] = total_phase['R0s'] + phase_worker['R0s']\n",
    "    total_phase['w0s'] = total_phase['w0s'] + phase_worker['w0s']\n",
    "    total_phase['Zs'] = total_phase['Zs'] + phase_worker['Zs']\n",
    "    total_phase['IDs'] = total_phase['IDs'] + phase_worker['IDs']\n",
    "    h5f_worker.close()\n",
    "    print(f\"Worker {j} done!\")\n",
    "json.dump(total_phase, open( f\"{output_directory}/STRUCTURE_Grid_R0_{N_R0}_w0_{N_w0}_Z_{N_Z}.json\", \"w\"))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a Random Image Library (with just the ground-truth angle $\\phi_{CR}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
    "\n",
    "# use gpu if available\n",
    "#device=\"cpu\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "'''\n",
    "La idea es que cada 5 epochs, se cambie el dataset efectivo, que sera un subset de los R0,w0,Z posibles\n",
    "multiplicado por el batch size (phiCR posibles). En cada batch, las imagenes enviadas seran todas\n",
    "de un mismo D matrix (R0,w0,Z) con diferentes angulos elegidos aleatoriamente con una uniforme\n",
    "'''\n",
    "class R0_w0_Z_Sampler(Sampler):\n",
    "    def __init__(self, R0_weights, w0_weights, Z_weights, num_batches_per_epoch):\n",
    "        self.num_batches = num_batches_per_epoch\n",
    "        self.R0_weights = R0_weights\n",
    "        self.w0_weights = w0_weights\n",
    "        self.Z_weights = Z_weights\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(torch.stack((\n",
    "            torch.multinomial(self.R0_weights, self.num_batches, replacement=True),\n",
    "            torch.multinomial(self.w0_weights, self.num_batches, replacement=True),\n",
    "            torch.multinomial(self.Z_weights, self.num_batches, replacement=True)),\n",
    "            dim=1).tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "class CR_Dataset(Dataset):\n",
    "    def __init__(self, D_matrix_file_path, ID_file_path, device, X=605, generate_images_w_depth=8, random_seed=666, \n",
    "                batch_size=10, num_batches_per_epoch=100, apply_noise=True,\n",
    "                all_stregths_random_per_epoch=False,\n",
    "                max_poisson_strength=0.5, max_blob_strength=0.5, max_angular_modulation_strength=0.5,\n",
    "                poisson_strength=0.3, blob_strength=0.1, angular_modulation_strength=0.25,\n",
    "                min_modulation_frec=2*np.pi/6, max_modulation_frec=2*np.pi/2,\n",
    "                max_blobs=1, min_blob_sigma=100, max_blob_sigma=130\n",
    "                ):\n",
    "        # If all_strengths_random_per_ecpoh, then arguments about the maximum will be valid while not the strength arguments\n",
    "        # If false, then the arguments about the particular stregths will be the global stregths\n",
    "        np.random.seed(random_seed) \n",
    "        torch.manual_seed(random_seed)\n",
    "        self.D_matrix_file_path=D_matrix_file_path\n",
    "        self.df_GTs = pd.DataFrame.from_dict(json.load(open(ID_file_path)))       \n",
    "        self.R0s = list(self.df_GTs['R0s'].drop_duplicates()) # Note they are lists of strings!\n",
    "        self.w0s = list(self.df_GTs['w0s'].drop_duplicates())\n",
    "        self.Zs = list(self.df_GTs['Zs'].drop_duplicates())\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches_per_epoch = num_batches_per_epoch\n",
    "        self.epoch_size = batch_size*num_batches_per_epoch\n",
    "        self.device = device\n",
    "        self.im_type = torch.uint16 if generate_images_w_depth==16 else torch.uint8\n",
    "        self.max_intensity = 65535 if generate_images_w_depth==16 else 254\n",
    "        self.X=X\n",
    "        self.apply_noise=apply_noise\n",
    "        self.poisson_strength=poisson_strength\n",
    "        self.blob_strength=blob_strength\n",
    "        self.angular_modulation_strength=angular_modulation_strength\n",
    "        self.min_modulation_frec=min_modulation_frec\n",
    "        self.max_modulation_frec=max_modulation_frec\n",
    "        self.max_blobs=max_blobs\n",
    "        self.min_blob_sigma=min_blob_sigma\n",
    "        self.max_blob_sigma=max_blob_sigma\n",
    "        self.all_stregths_random_per_epoch=all_stregths_random_per_epoch\n",
    "        self.max_poisson_strength=max_poisson_strength\n",
    "        self.max_blob_strength=max_blob_strength\n",
    "        self.max_angular_modulation_strength=max_angular_modulation_strength\n",
    "        \n",
    "    #def update_dataset o set_epoch_number y que aqui se genere directamente el dataset entero para las epochs que vienen\n",
    "    # lo que permitiria es que cada X epochs, se ahorrase el tener que re-generar todas las imagenes\n",
    "    # Pero claro, la pregunta es, la RAM aguantaria?\n",
    "    # Si haces con update_dataset, entonces no haria falta hacer un sampler custom, con el normal ya bastaria\n",
    "    \n",
    "    # Bueno, por ahora, vamos a hacer que en cada minibatch, se haga todo el puroceso. La cosa es que asi se \n",
    "    # puede aprovechar el multiprocessing innato, si no habria que hacer el multiprocessing dentroe del update_dataset\n",
    "    # o simplemente prescindir de hacerlo supongo.\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'h5f_D_matrices'):\n",
    "            self.h5f_D_matrices.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.epoch_size\n",
    "    \n",
    "    def open_hdf5(self):\n",
    "        self.h5f_D_matrices = h5py.File( self.D_matrix_file_path, 'r')\n",
    "        #self.dataset = self.img_hdf5['dataset'] # if you want dataset.\n",
    "        \n",
    "\n",
    "    def compute_intensity_gravity_centers(self, images):\n",
    "        \"\"\"\n",
    "            Expects input image to be an array of dimensions [N_imgs, h, w].\n",
    "            It will return an array of gravity centers [N_imgs, 2(h,w)] in pixel coordinates\n",
    "            Remember that pixel coordinates are set equal to array indices\n",
    "\n",
    "        \"\"\"\n",
    "        # image wise total intensity and marginalized inensities for weighted sum\n",
    "        intensity_in_w = torch.sum(images, dim=1) # weights for x [N_images, raw_width]\n",
    "        intensity_in_h = torch.sum(images, dim=2) # weights for y [N_images, raw_height]\n",
    "        total_intensity = intensity_in_h.sum(dim=1) # [N_images]\n",
    "\n",
    "        # Compute mass center for intensity\n",
    "        # [N_images, 2] (h_center,w_center)\n",
    "        return torch.nan_to_num( torch.stack(\n",
    "            (torch.matmul(intensity_in_h.float(), torch.arange(images.shape[1], \n",
    "                                        dtype=torch.float32, device=self.device))/total_intensity,\n",
    "             torch.matmul(intensity_in_w.float(), torch.arange(images.shape[2], \n",
    "                                        dtype=torch.float32, device=self.device))/total_intensity),\n",
    "            dim=1\n",
    "            ), nan=0.0, posinf=None, neginf=None)\n",
    "\n",
    "    def compute_raw_to_centered_iX(self, images):\n",
    "\n",
    "        g_raw = self.compute_intensity_gravity_centers(images) # [ N_images, 2]\n",
    "\n",
    "        # crop the iamges with size (X+1+X)^2 leaving the gravity center in\n",
    "        # the central pixel of the image. In case the image is not big enough for the cropping,\n",
    "        # a 0 padding will be made.\n",
    "        centered_images = torch.zeros( ( images.shape[0], 2*self.X+1, 2*self.X+1),  dtype = images.dtype, \n",
    "                                      device=self.device)\n",
    "\n",
    "        # we round the gravity centers to the nearest pixel indices\n",
    "        g_index_raw = torch.round(g_raw).int() #[ N_images, 2]\n",
    "\n",
    "        # obtain the slicing indices around the center of gravity\n",
    "        # TODO -> make all this with a single array operation by stacking the lower and upper in\n",
    "        # a new axis!!\n",
    "        # [ N_images, 2 (h,w)]\n",
    "        unclipped_lower = g_index_raw-self.X\n",
    "        unclipped_upper = g_index_raw+self.X+1\n",
    "\n",
    "        # unclipped could get out of bounds for the indices, so we clip them\n",
    "        lower_bound = torch.clip( unclipped_lower.float(), min=torch.Tensor([[0,0]]).to(self.device),\n",
    "                                 max=torch.Tensor(list(images.shape[1:])).unsqueeze(0).to(self.device)).int()\n",
    "        upper_bound = torch.clip( unclipped_upper.float(), min=torch.Tensor([[0,0]]).to(self.device),\n",
    "                                 max=torch.Tensor(list(images.shape[1:])).unsqueeze(0).to(self.device)).int()\n",
    "        # we use the difference between the clipped and unclipped to get the necessary padding\n",
    "        # such that the center of gravity is left still in the center of the image\n",
    "        padding_lower = lower_bound-unclipped_lower\n",
    "        padding_upper = upper_bound-unclipped_upper\n",
    "\n",
    "        # crop the image\n",
    "        for im in range(g_raw.shape[0]):\n",
    "            centered_images[im, padding_lower[ im, 0]:padding_upper[ im, 0] or None,\n",
    "                                        padding_lower[ im, 1]:padding_upper[ im, 1] or None] = \\\n",
    "                      images[im, lower_bound[ im, 0]:upper_bound[ im, 0],\n",
    "                                          lower_bound[ im, 1]:upper_bound[ im, 1]]\n",
    "\n",
    "        return centered_images\n",
    "    \n",
    "    def apply_random_camera_noises(self, images):\n",
    "        # Poisson noise\n",
    "        # the images are expected to already be normalized and in the integer range of the camera\n",
    "        return torch.clamp((1-self.poisson_strength)*images+self.poisson_strength*torch.poisson(images), max=self.max_intensity) \n",
    "                                    # rates are the expected intensities of the imaging time\n",
    "\n",
    "    def _gaussian_2D_pdfs(self, x_ys, mus, sigmas, strengths):\n",
    "        '''\n",
    "        x_ys : [batch_size, blob_num, 2 (h,w), 2X+1, 2X+1]\n",
    "        mus : [batch_size, blob_num, 2 (h,w), 1, 1]\n",
    "        sigmas : [batch_size, blob_num, 2(h,w), 1, 1]\n",
    "        strengths : [batch_size, blob_num, 1, 1]\n",
    "        ------\n",
    "        out : [batch_size, 2X+1, 2X+1]\n",
    "        '''\n",
    "        gaussians = torch.sum((strengths/(2*np.pi)/sigmas[:,:,0]/sigmas[:,:,1])*torch.exp(\n",
    "                -(x_ys[:,:,0,:,:]-mus[:,:,0])**2/(2*sigmas[:,:,0]**2))*torch.exp(\n",
    "                -(x_ys[:,:,1,:,:]-mus[:,:,1])**2/(2*sigmas[:,:,1]**2)), dim=1) # since strength is normalized, the whole mixture is normalized as well\n",
    "        return gaussians/gaussians.amax(dim=(1,2)).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def apply_random_pre_camera_noises(self, images):\n",
    "        # note that the input images are expected to still be centered in the gravicenter and have no normalization\n",
    "\n",
    "        # Gaussian Blobs - subtract gaussian blobs of different depths to the intensity pattern\n",
    "        # First randomly sample the centers of the blobs and their standard deviations for each image\n",
    "        # we will sample the means with probabilities proportional to the CR ring intensity pattern\n",
    "        blob_num = np.random.randint(0, self.max_blobs+1, size=1)[0]\n",
    "        if blob_num!=0:\n",
    "            mu_s = torch.stack(\n",
    "                (torch.multinomial(images.sum(dim=2),\n",
    "                        num_samples=blob_num, \n",
    "                        replacement=False), \n",
    "                 torch.multinomial(images.sum(dim=1),\n",
    "                        num_samples=blob_num, \n",
    "                        replacement=False) ),\n",
    "                 dim=2\n",
    "                ).to(self.device) #[batch_size, blob_num, 2(h,w)] mu-s are in pixel units and coordinates\n",
    "\n",
    "            sigma_s = torch.from_numpy(np.random.randint(self.min_blob_sigma, self.max_blob_sigma, \n",
    "                        size=(images.shape[0], blob_num, 2))).to(self.device) #[batch_size, blob_num, 2(h,w)]\n",
    "            strengths = torch.rand(size=(images.shape[0], blob_num)).to(self.device) #[batch_size, blob_num]\n",
    "            strengths = strengths/strengths.sum(dim=1).unsqueeze(1) # normalized strengths between blobs\n",
    "\n",
    "            w = torch.arange(images.shape[1]).repeat((images.shape[1],1)).to(self.device)\n",
    "            h = w.transpose(0,1).to(self.device)\n",
    "            h_w = torch.stack((h,w), dim=0).to(self.device)\n",
    "            images = images*(1-self.blob_strength*self._gaussian_2D_pdfs( h_w.view((1,1)+h_w.shape), \n",
    "                mu_s.view(mu_s.shape+(1,1)), sigma_s.view(sigma_s.shape+(1,1)), strengths.view(strengths.shape+(1,1)) )\n",
    "                     )           #[batch_size, 2X+1, 2X+1]        \n",
    "        # Poisson noise - makes the intesity be a poissonian generated value instead of the expected values\n",
    "        #images = (1-poisson_strength)*images+poisson_strength*torch.poisson(images) # rates are the expected intensities of the imaging time\n",
    "        # but must be an integer matrix!\n",
    "\n",
    "        # Angular Modulation - apply a pseudo-random continous wave modulation to the ring angularly\n",
    "        random_frecs = (self.min_modulation_frec + (self.max_modulation_frec-self.min_modulation_frec)*torch.rand(\n",
    "                                size=(3,images.shape[0], 1,1))).to(self.device)\n",
    "        strengths = torch.rand(size=(3, images.shape[0], 1,1)).to(self.device) #[3, batch_size, 1,1]\n",
    "        strengths = strengths/strengths.sum(dim=0) # normalized strengths between sin and coss\n",
    "        images = images*(\n",
    "            1-self.angular_modulation_strength*(\n",
    "                strengths[0]*torch.cos(random_frecs[0]*self.phis)+\n",
    "                strengths[1]*torch.sin(random_frecs[1]*self.phis)+\n",
    "                strengths[2]*torch.cos(random_frecs[2]*self.phis)\n",
    "            )**2) #[batch_size, 2X+1, 2X+1]\n",
    "\n",
    "        # Angular-Radial Modulation # sería coger phis y coger radios y con eso hacer uan funcion de ambas, de forma\n",
    "        # que por ejemplo afecte de manera diferente al mismo angulo en cada ring el pre-pogendorf y el otro\n",
    "\n",
    "        # Modos superiores\n",
    "        # esto ya es un jaleo xD\n",
    "        return images\n",
    "\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, R0_w0_Z_idxs):\n",
    "        # In order to allow multiprocessing data loading, each worker needs to initialize \n",
    "        # the h5f loader, which must be done in the first iteration of getitem and not in the init\n",
    "        # of the parent process\n",
    "        if not hasattr(self, 'h5f_D_matrices'):\n",
    "            self.open_hdf5()\n",
    "            self.phis = torch.from_numpy(self.h5f_D_matrices['phis'][:]).unsqueeze(0).to(self.device) #[1,Nx,Ny]\n",
    "\n",
    "        D_mats = torch.from_numpy(self.h5f_D_matrices[\n",
    "                f\"R0_{self.R0s[R0_w0_Z_idxs[0]]}_w0_{self.w0s[R0_w0_Z_idxs[1]]}_Z_{self.Zs[R0_w0_Z_idxs[2]]}\"][:]\n",
    "                                 ).unsqueeze(1).to(self.device) #[2, 1, Nx, Ny]            \n",
    "         \n",
    "        phiCRs = torch.FloatTensor(self.batch_size, 1, 1).uniform_(-np.pi, np.pi).to(self.device) #[batch_size, 1, 1]\n",
    "        images = D_mats[0]+D_mats[1]*torch.cos(phiCRs-self.phis) #[batch_size, Nx,Ny]\n",
    "        \n",
    "        if self.apply_noise:\n",
    "            if self.all_stregths_random_per_epoch:\n",
    "                self.poisson_strength = self.max_poisson_strength*np.random.rand()\n",
    "                self.angular_modulation_strength = self.max_angular_modulation_strength*np.random.rand()\n",
    "                self.blob_strength = self.max_blob_strength*np.random.rand()\n",
    "            \n",
    "            # Apply precamera noise to images (while still floats)\n",
    "            images = self.apply_random_pre_camera_noises(images)\n",
    "        \n",
    "        # convert images to selected uint format\n",
    "        images = (self.max_intensity*(images/images.amax(dim=(1,2), keepdim=True)[0].unsqueeze(1)))\n",
    "        \n",
    "        if self.apply_noise:\n",
    "            # Apply camera noises (now that normalized and integers)\n",
    "            images = self.apply_random_camera_noises(images)\n",
    "        \n",
    "        images = images.type(self.im_type)\n",
    "\n",
    "        # get iX images\n",
    "        images = self.compute_raw_to_centered_iX(images) #[batch_size, 2X+1, 2X+1]\n",
    "        labels = torch.Tensor([[float(self.R0s[R0_w0_Z_idxs[0]]), float(self.w0s[R0_w0_Z_idxs[1]]), \n",
    "                               float(self.Zs[R0_w0_Z_idxs[2]])]]).to(self.device) #[1,4]\n",
    "        labels = torch.hstack( ( labels.expand(self.batch_size, 3), phiCRs.squeeze(2) ) ) #[4, batch_size]\n",
    "        del D_mats, phiCRs\n",
    "        torch.cuda.empty_cache()\n",
    "        return images, labels #[ batch_size, 2X+1, 2X+1] and [batch_size, 4]\n",
    "        # The whole batch is already in the GPU, since to process it we wanted it to be there\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import h5py\n",
    "from IPython import display\n",
    "\n",
    "#output_directory=\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/IMAGE_LIBRARY/NOISY/TRAIN/\"\n",
    "output_directory=\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/IMAGE_LIBRARY/NON_NOISY/TRAIN/\"\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_file_path= \"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/STRUCTURE_Grid_R0_70_w0_70_Z_4.json\"\n",
    "D_matrix_file_path= \"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/Encoder_Alone/Dataset_R0_70_w0_70_Z_4.h5\"\n",
    "        \n",
    "\n",
    "total_images_to_generate = 500000 # para el test set unos 100000 bien supongo\n",
    "\n",
    "batch_size = 10\n",
    "number_of_batches_per_epoch = int(total_images_to_generate/batch_size)\n",
    "\n",
    "assert(total_images_to_generate%batch_size==0)\n",
    "\n",
    "X=302\n",
    "generate_images_w_depth=8\n",
    "random_seed=666 # 669 aldatu seede para generar los validation set!!!\n",
    "\n",
    "apply_noise=False # Genera otro dataset pair sin noise tb\n",
    "all_stregths_random_per_epoch=True\n",
    "max_poisson_strength=0.5\n",
    "max_blob_strength=0.5\n",
    "max_angular_modulation_strength=0.5\n",
    "poisson_strength=0.4\n",
    "blob_strength=0.2\n",
    "angular_modulation_strength=0.2\n",
    "min_modulation_frec=2*np.pi/6\n",
    "max_modulation_frec=2*np.pi/2\n",
    "max_blobs=2\n",
    "min_blob_sigma=100\n",
    "max_blob_sigma=130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pdf(x, mu, sigma, normalized_output=True):\n",
    "    p_s = (1/np.sqrt(2*np.pi)/sigma)*torch.exp(-(x-mu)**2/(2*sigma**2))\n",
    "    return p_s/p_s.sum() if normalized_output else p_s\n",
    "\n",
    "phase_vigilant = pd.DataFrame.from_dict(json.load(open(ID_file_path)))\n",
    "R0_weights = gaussian_pdf(torch.from_numpy(np.array( phase_vigilant['R0s'].drop_duplicates(), dtype=np.float64)),\n",
    "                          mu=158, sigma=8)\n",
    "w0_weights = gaussian_pdf(torch.from_numpy(np.array( phase_vigilant['w0s'].drop_duplicates(), dtype=np.float64)),\n",
    "                          mu=25, sigma=4)\n",
    "Z_weights = gaussian_pdf(torch.from_numpy(np.array( phase_vigilant['Zs'].drop_duplicates(), dtype=np.float64)),\n",
    "                          mu=0, sigma=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = R0_w0_Z_Sampler( R0_weights, w0_weights, Z_weights, num_batches_per_epoch=number_of_batches_per_epoch)\n",
    "dataset = CR_Dataset(D_matrix_file_path=D_matrix_file_path,\n",
    "            ID_file_path =ID_file_path, \n",
    "            device = device,\n",
    "            X=X, generate_images_w_depth=generate_images_w_depth, random_seed=random_seed, \n",
    "            batch_size=batch_size, num_batches_per_epoch=number_of_batches_per_epoch,\n",
    "            apply_noise=apply_noise, all_stregths_random_per_epoch=all_stregths_random_per_epoch,\n",
    "            max_poisson_strength=max_poisson_strength, max_blob_strength=max_blob_strength,\n",
    "            max_angular_modulation_strength=max_angular_modulation_strength,\n",
    "            poisson_strength=poisson_strength, blob_strength=blob_strength, \n",
    "            angular_modulation_strength=angular_modulation_strength,\n",
    "            min_modulation_frec=min_modulation_frec, max_modulation_frec=max_modulation_frec,\n",
    "            max_blobs=max_blobs, min_blob_sigma=min_blob_sigma, max_blob_sigma=max_blob_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500000/500000 images 100.0 %\n",
      "CPU times: user 24min 23s, sys: 1min 18s, total: 25min 42s\n",
      "Wall time: 52min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "j=0\n",
    "output_info_every=100\n",
    "ground_truths={'ID':[], 'phiCR':[]}\n",
    "for i in sampler:\n",
    "    imgs, labs = dataset[i]\n",
    "    imgs = np.asarray(imgs.to('cpu'))\n",
    "    labs = np.asarray(labs.to('cpu'))\n",
    "    for k in range(imgs.shape[0]):\n",
    "        cv2.imwrite(f\"{output_directory}/IM_{j}_phiCR_{labs[k][-1]}.png\", imgs[k])\n",
    "        ground_truths['ID'].append(j)\n",
    "        ground_truths['phiCR'].append(f\"{labs[k][-1]}\")\n",
    "        j+=1\n",
    "        if j%output_info_every==0:\n",
    "            display.clear_output(wait=True)\n",
    "            print(f\"Processed {j}/{total_images_to_generate} images {j/total_images_to_generate*100} %\")\n",
    "json.dump(ground_truths, open( f\"{output_directory}/GROUND_TRUTHS.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=time()\n",
    "h5f = h5py.File(f\"{image_directory}/Dataset.h5\",'r')\n",
    "D_mat = h5f[ID][:]\n",
    "phis = h5f['phis'][:]\n",
    "#print(list(h5f.keys()))\n",
    "h5f.close()\n",
    "print(f\"hf5:{time()-t}s size {os.path.getsize(image_directory+'/Dataset.h5')}\")   \n",
    "print(D_mat.dtype, D_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.getsize(image_directory+'/Dataset.h5')/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_directory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3367/2901206933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m# save the matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mrel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{image_directory}/{ID}.pkl.lzma\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0;31m#np.save( rel_path, D_matrix, allow_pickle=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nDUMP {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_directory' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle, gzip, lzma, bz2\n",
    "import h5py\n",
    "\n",
    "\n",
    "# Initialize the vigilant\n",
    "try:\n",
    "    phase_vigilant = json.load(open(f\"{output_directory}/STRUCTURE_Grid.json\"))\n",
    "except:\n",
    "    phase_vigilant = {'R0s':[], 'w0s':[], 'Zs':[], 'IDs':[], 'rel_path':[]}\n",
    "\n",
    "# Set the objects ready ##################\n",
    "# The simulator object\n",
    "simulator=RingSimulator_Optimizer_GPU( n=1.5, a0=1.0, max_k=max_k, num_k=num_k, nx=resolution_side_nx, sim_chunk_x=sim_chunk_ax, sim_chunk_y=sim_chunk_ax)\n",
    "\n",
    "# Execute the stuff #####################\n",
    "i=1\n",
    "total=Z_s.shape[0]*R0_s.shape[0]*w0_s.shape[0]\n",
    "elapsed=0\n",
    "beg=time()\n",
    "output_info_every=100\n",
    "dump_every=10000\n",
    "\n",
    "for Z in Z_s:\n",
    "    for R0 in R0_s:\n",
    "        for w0 in w0_s:\n",
    "            ID=f\"R0_{R0}_w0_{w0}_Z_{Z}\"\n",
    "            if ID not in phase_vigilant['IDs']:\n",
    "                # simulate matrix\n",
    "                #D_matrix = simulator.compute_D_matrix( R0_pixels=R0, Z=Z, w0_pixels=w0)\n",
    "                D_matrix = simulator.compute_pieces_for_I_LP(R0_pixels=R0, Z=Z, w0_pixels=w0)\n",
    "                \n",
    "                # save the matrix\n",
    "                rel_path=f\"{image_directory}/{ID}.pkl.lzma\"\n",
    "                #np.save( rel_path, D_matrix, allow_pickle=False)\n",
    "                print(f\"\\nDUMP {i}\")\n",
    "                t=time()\n",
    "                pickle.dump(D_matrix, lzma.open(rel_path, 'wb'))\n",
    "                print(f\"lzma:{time()-t}s size {os.path.getsize(rel_path)}\")\n",
    "                t=time()\n",
    "                pickle.dump(D_matrix, gzip.open(rel_path+\".gzip\", 'wb'))\n",
    "                print(f\"gzip:{time()-t}s size {os.path.getsize(rel_path+'.gzip')}\")\n",
    "                t=time()\n",
    "                pickle.dump(D_matrix, bz2.open(rel_path+\".bz2\", 'wb'))\n",
    "                print(f\"bz2:{time()-t}s size {os.path.getsize(rel_path+'.bz2')}\")\n",
    "                t=time()\n",
    "                fp = np.memmap(rel_path+\".p\", dtype='complex64', mode='w+', shape=(3,image_shortest_side,image_shortest_side))\n",
    "                fp=D_matrix\n",
    "                print(f\"memmap:{time()-t}s size {os.path.getsize(rel_path+'.p')}\")\n",
    "                t=time()\n",
    "                h5f = h5py.File(rel_path+'.h5', 'w')\n",
    "                h5f.create_dataset('dataset_1', data=D_matrix, compression=\"gzip\", shuffle=True) #, compression_opts=9)\n",
    "                h5f.close()\n",
    "                print(f\"hf5 lzf:{time()-t}s size {os.path.getsize(rel_path+'.h5')}\")     \n",
    "\n",
    "\n",
    "                \n",
    "                print(f\"\\nLOAD {i}\")\n",
    "                t=time()\n",
    "                D_mat=pickle.load(lzma.open(rel_path, 'rb'))\n",
    "                print(f\"lzma:{time()-t}s size {os.path.getsize(rel_path)}\")\n",
    "                t=time()\n",
    "                D_mat=pickle.load(gzip.open(rel_path+\".gzip\", 'rb'))\n",
    "                print(f\"gzip:{time()-t}s size {os.path.getsize(rel_path+'.gzip')}\")\n",
    "                t=time()\n",
    "                D_mat=pickle.load( bz2.open(rel_path+\".bz2\",'rb'))\n",
    "                #print(D_mat.dtype, D_mat.shape)\n",
    "                print(f\"bz2:{time()-t}s size {os.path.getsize(rel_path+'.bz2')}\")     \n",
    "                t=time()\n",
    "                D_mat= np.memmap(rel_path+\".p\", dtype='complex64', mode='r+', shape=(3,image_shortest_side,image_shortest_side))\n",
    "                print(f\"memmap:{time()-t}s size {os.path.getsize(rel_path+'.p')}\")     \n",
    "                t=time()\n",
    "                h5f = h5py.File(rel_path+'.h5','r')\n",
    "                D_mat = h5f['dataset_1'][:]\n",
    "                h5f.close()\n",
    "                print(f\"hf5:{time()-t}s size {os.path.getsize(rel_path+'.h5')}\")     \n",
    "                print(type(D_mat), D_mat.dtype, D_mat.shape)\n",
    "                print(np.allclose(D_mat, D_matrix))\n",
    "\n",
    "                \n",
    "\n",
    "                if D_matrix is None:\n",
    "                    raise ValueError\n",
    "\n",
    "                #append the data\n",
    "                phase_vigilant['IDs'].append(ID)\n",
    "                phase_vigilant['R0s'].append(float(R0))\n",
    "                phase_vigilant['Zs'].append(float(Z))\n",
    "                phase_vigilant['w0s'].append(float(w0))\n",
    "                phase_vigilant['rel_path'].append(rel_path)\n",
    "                \n",
    "                \n",
    "                if i%output_info_every==0:\n",
    "                    display.clear_output(wait=True)\n",
    "                    elapsed=time()-beg\n",
    "                    print(f\"[\"+'#'*(int(100*i/total))+' '*(100-int(100*i/total))+f\"] {100*i/total:3.4}% \\n\\nSimulated: {i}/{total}\\nElapsed time: {elapsed//3600} h {elapsed//60-(elapsed//3600)*60} min {elapsed-(elapsed//60)*60-(elapsed//3600)*60:2.4} s\")\n",
    "                    if i%dump_every==0:\n",
    "                        # we save the progess (in order to be able to quit and resume)\n",
    "                        json.dump(phase_vigilant, open( f\"{output_directory}/STRUCTURE_Grid.json\", \"w\"))\n",
    "            i+=1\n",
    "print(\"\\n\\nFINISHED!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(1000):\n",
    "    a=np.abs(D_mat)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(1000):\n",
    "    a=(D_mat*D_mat.conjugate()).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(1000):\n",
    "    a=D_mat.real**2+D_mat.imag**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(1000):\n",
    "    a=D_mat.real**2\n",
    "    a+=D_mat.imag**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose( (D_mat*D_mat.conjugate()).real, D_mat.real**2+D_mat.imag**2, rtol=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
