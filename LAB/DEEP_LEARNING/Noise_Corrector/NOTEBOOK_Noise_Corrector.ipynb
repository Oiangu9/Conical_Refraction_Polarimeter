{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Corrector CNN to get CR images without noise\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_13465537029981420876() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_13465537029981420876()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is just a function to allow toggleing code cells that are too long for good\n",
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "def hide_toggle(for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    toggle_text = 'Toggle show/hide'  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #should be installed by default in any colab notebook\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from IPython import display as display_IPython\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
    "\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the functions and routines for the DL\n",
    "### Define the model and its constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise_Corrector(nn.Module):\n",
    "    def __init__(self, S0=2*302+1, S1=2*290+1, S2=2*250+1, S3=2*200+1,\n",
    "                 feats_S1=10, feats_S2=10, feats_S3=20, feats_S0=10,\n",
    "                 dropout_p=0.1\n",
    "                ): \n",
    "       \n",
    "        super(Noise_Corrector, self).__init__()\n",
    "        self.Ss = [S0, S1, S2, S3]\n",
    "        self.feats = [feats_S0, feats_S1, feats_S2, feats_S3]\n",
    "        # in is [batch_size, 1, S0, S0]\n",
    "        self.conv_S01 = nn.Conv2d(in_channels=1, out_channels=feats_S1, \n",
    "                               kernel_size = S0-S1+1, bias=True) \n",
    "        # out conv_S01 [batch_size, feats_S1, S1, S1]\n",
    "        self.conv_S12 = nn.Conv2d(in_channels=feats_S1, out_channels=feats_S2, \n",
    "                               kernel_size = S1-S2+1, bias=True) \n",
    "        # out conv_S12 [batch_size, feats_S2, S2, S2]\n",
    "        self.conv_S23 = nn.Conv2d(in_channels=feats_S2, out_channels=feats_S3, \n",
    "                               kernel_size = S2-S3+1, bias=True) \n",
    "        # out conv_S23 [batch_size, feats_S3, S3, S3]\n",
    "        \n",
    "        self.deConv_S32 = torch.nn.ConvTranspose2d(in_channels=feats_S3, out_channels=feats_S2, \n",
    "                                                kernel_size = S2-S3+1)\n",
    "        # out deConv_S32+memory [batch_size, 2*feats_S2, S2, S2]\n",
    "        self.deConv_S21 = torch.nn.ConvTranspose2d(in_channels=2*feats_S2, out_channels=feats_S1, \n",
    "                                                kernel_size = S1-S2+1)\n",
    "        # out deConv_S21+memory [batch_size, 2*feats_S1, S1, S1]\n",
    "        self.deConv_S10 = torch.nn.ConvTranspose2d(in_channels=2*feats_S1, out_channels=feats_S0, \n",
    "                                                kernel_size = S0-S1+1)\n",
    "\n",
    "        # out conv_S01 [batch_size, feats_S1, S1, S1]\n",
    "        self.conv_S00 = nn.Conv2d(in_channels=feats_S0+1, out_channels=1, \n",
    "                               kernel_size = 1, bias=True) \n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_p, inplace=False)\n",
    "        self.relu = torch.nn.functional.leaky_relu\n",
    "\n",
    "        self.batchNorm = nn.BatchNorm2d(num_features=feats_S3)\n",
    "\n",
    "    def forward(self, x): # [batch_size, 2X+1, 2X+1] or [batch_size, 1, 2X+1, 2X+1] Already Normalized!\n",
    "        x = x.view(x.shape[0], 1, x.shape[-2], x.shape[-1]).float() # [batch_size, 1, 2X+1, 2X+1]\n",
    "        # Normalize to unity the float image\n",
    "        #x = x/x.amax(dim=(2,3), keepdim=True)[0] # [batch_size, 1, 2X+1, 2X+1]\n",
    "        \n",
    "        # Conv layers\n",
    "        s1 = self.relu(self.conv_S01(x)) # [batch_size, feats_S1, S1, S1]\n",
    "        s2 = self.dropout( self.relu(self.conv_S12(s1)) ) # [batch_size, feats_S2, S2, S2]\n",
    "        s3 = self.batchNorm( self.relu(self.conv_S23(s2)) ) # [batch_size, feats_S3, S3, S3]\n",
    "        # deConv layers\n",
    "        s2 = torch.cat((s2, self.relu(self.deConv_S32(s3))), 1) #[batch_size, 2*feats_S2, S2, S2]\n",
    "        s1 = torch.cat((s1, self.relu(self.deConv_S21( self.dropout(s2) )) ), 1) #[batch_size, 2*feats_S1, S1, S1]\n",
    "        x = torch.cat((x, self.relu(self.deConv_S10(s1))), 1) #[batch_size, 1+feats_S0, S0, S0]\n",
    "        # Conv layer\n",
    "        return self.conv_S00(x) # [batch_size, 1, S0, S0]\n",
    "\n",
    "    \n",
    "    def print_shapes(self, batch_size=10):\n",
    "        x = torch.ones((batch_size, 1, self.Ss[0], self.Ss[0])).to(device)\n",
    "        print(f\"Initial shape {x.shape}\")\n",
    "        s1 = self.relu(self.conv_S01(x)) # [batch_size, feats_S1, S1, S1]\n",
    "        print(f\"Conv01 shape {s1.shape} should be [{batch_size},{self.feats[1]},{self.Ss[1]}, {self.Ss[1]}]\")\n",
    "        s2 = self.dropout( self.relu(self.conv_S12(s1)) ) # [batch_size, feats_S2, S2, S2]\n",
    "        print(f\"Conv12 shape {s2.shape} should be [{batch_size},{self.feats[2]},{self.Ss[2]}, {self.Ss[2]}]\")\n",
    "        s3 = self.batchNorm( self.relu(self.conv_S23(s2)) ) # [batch_size, feats_S3, S3, S3]\n",
    "        print(f\"Conv23 shape {s3.shape} should be [{batch_size},{self.feats[3]},{self.Ss[3]}, {self.Ss[3]}]\")\n",
    "\n",
    "        s2 = torch.cat((s2, self.relu(self.deConv_S32(s3))), 1) #[batch_size, 2*feats_S2, S2, S2]\n",
    "        print(f\"DeConv32+mem shape {s2.shape} should be [{batch_size},{2*self.feats[2]},{self.Ss[2]}, {self.Ss[2]}]\")\n",
    "        s1 = torch.cat((s1, self.relu(self.deConv_S21( self.dropout(s2) )) ), 1) #[batch_size, 2*feats_S1, S1, S1]\n",
    "        print(f\"DeConv21+mem shape {s1.shape} should be [{batch_size},{2*self.feats[1]},{self.Ss[1]}, {self.Ss[1]}]\")\n",
    "        x = torch.cat((x, self.relu(self.deConv_S10(s1))), 1) #[batch_size, 1+feats_S0, S0, S0]\n",
    "        print(f\"DeConv10+mem shape {x.shape} should be [{batch_size},{1+self.feats[0]},{self.Ss[0]}, {self.Ss[0]}]\")\n",
    "        x= self.conv_S00(x) # [batch_size, 1, S0, S0]\n",
    "        print(f\"Conv00 shape {x.shape} should be [{batch_size}, 1, {self.Ss[0]}, {self.Ss[0]}]\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_2838271587736425604() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_2838271587736425604()\">Toggle show/hide next cell</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hide_toggle(for_next=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subroutine to count number of parameters in the model\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.numel()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The routines to validate and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_6325661460045212320() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_6325661460045212320()\">Toggle show/hide next cell</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hide_toggle(for_next=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # prevent this function from computing gradients \n",
    "def validate_epoch(criterion, model, dataloader, per_epoch_use_max_batches=None): #show_confusion_matrix = False):\n",
    "    if per_epoch_use_max_batches is None:\n",
    "        per_epoch_use_max_batches = len(dataloader)\n",
    "    val_loss = 0\n",
    "    max_abs_error = torch.Tensor([0]).to(device)\n",
    "    mean_abs_error = 0\n",
    "    preds = torch.Tensor().to(device)\n",
    "    targets = torch.Tensor().to(device)\n",
    "\n",
    "    model.eval() # disable the dropout, among others\n",
    "\n",
    "    for batch_id in range(len(dataloader)):  \n",
    "        data, target = dataloader[batch_id] # dataloader sends them to device already   \n",
    "        \n",
    "        # images to float\n",
    "        data = data.view(data.shape[0], 1, data.shape[-2], data.shape[-1]).float()\n",
    "        target = target.view(data.shape[0], 1, data.shape[-2], data.shape[-1]).float()\n",
    "        # normalize the data and target\n",
    "        data = data/data.amax(dim=(-2,-1), keepdim=True)[0] # [batch_size, 1, 2X+1, 2X+1]\n",
    "        target = target/target.amax(dim=(-2,-1), keepdim=True)[0]\n",
    "        prediction = model(data) # data is [batch_size, embedding_dim]\n",
    "        loss = criterion(prediction, target)\n",
    "        val_loss += loss.item()             \n",
    "        max_abs_error = torch.maximum(torch.max(torch.abs(prediction-target), 0).values, max_abs_error)\n",
    "        mean_abs_error += torch.sum(torch.mean(torch.abs(prediction-target), (-2,-1)), 0)\n",
    "        if batch_id % per_epoch_use_max_batches == per_epoch_use_max_batches-1:\n",
    "            break\n",
    "    val_loss /= min(len(dataloader), per_epoch_use_max_batches)\n",
    "    mean_abs_error /= min(len(dataloader), per_epoch_use_max_batches)\n",
    "    #accuracy = 100. * correct / len(loader.dataset)\n",
    "    print(f'\\nValidation set: Average loss: {val_loss:.4f}, Average Abs Error: {np.array(mean_abs_error.cpu())}, Maximum Abs Error: {np.array(max_abs_error.cpu())} \\n')\n",
    "\n",
    "    #if show_confusion_matrix:\n",
    "    #    visualize_confusion_matrix(preds.to(torch.device('cpu')), targets.to(torch.device('cpu')))\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train_epoch(epoch, criterion, model, optimizer, dataloader, print_loss_every_batches=20,\n",
    "                optimizer_step_every_batches=1, per_epoch_use_max_batches=None):\n",
    "    if per_epoch_use_max_batches is None:\n",
    "        per_epoch_use_max_batches = len(dataloader)\n",
    "        \n",
    "    total_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    #t = time()\n",
    "    random_indices = np.random.choice(range(len(dataloader)), per_epoch_use_max_batches, replace=False)\n",
    "    for batch_id, idx in enumerate(random_indices):  \n",
    "        data, target = dataloader[idx] # dataloader sends them to device already\n",
    "        #data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # images to float\n",
    "        data = data.view(data.shape[0], 1, data.shape[-2], data.shape[-1]).float()\n",
    "        target = target.view(data.shape[0], 1, data.shape[-2], data.shape[-1]).float()\n",
    "        # normalize the data and target\n",
    "        data = data/data.amax(dim=(-2,-1), keepdim=True)[0] # [batch_size, 1, 2X+1, 2X+1]\n",
    "        target = target/target.amax(dim=(-2,-1), keepdim=True)[0]\n",
    "\n",
    "        prediction = model(data) # data is [batch_size, embedding_dim]\n",
    "        loss = criterion(prediction, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        if batch_id % optimizer_step_every_batches==optimizer_step_every_batches-1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # print loss every N batches\n",
    "        if batch_id % print_loss_every_batches == print_loss_every_batches-1:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_id+1) * len(data), len(dataloader)*dataloader.batch_size,\n",
    "                100*(batch_id+1)*len(data) / (len(dataloader)*dataloader.batch_size), loss.item()))\n",
    "\n",
    "        #if batch_id % per_epoch_use_max_batches == per_epoch_use_max_batches-1:\n",
    "        #    break\n",
    "\n",
    "\n",
    "        total_loss += loss.item()  #.item() is very important here\n",
    "        # In order to avoid having total_loss as a tensor in the gpu\n",
    "        #t = time()\n",
    "\n",
    "    return total_loss / min(len(dataloader), per_epoch_use_max_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_6670678266162676199() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_6670678266162676199()\">Toggle show/hide next cell</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hide_toggle(for_next=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_training_loop(model, criterion, optimizer, train_loader, test_loader, epochs=10,\n",
    "                       print_loss_every_batches=20, validate_every_epochs=2, optimizer_step_every_batches=1,\n",
    "                      per_epoch_use_max_train_batches=None, per_epoch_use_max_test_batches=None,\n",
    "                      image_path=None, save_model_every_epochs=1, model_path=None, best_model_path=None):\n",
    "    losses = {\"train\": [], \"val\": []}\n",
    "    %matplotlib inline\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss = train_epoch(epoch, criterion, model, optimizer, train_loader,\n",
    "                                 print_loss_every_batches=print_loss_every_batches,\n",
    "                                optimizer_step_every_batches=optimizer_step_every_batches,\n",
    "                                per_epoch_use_max_batches=per_epoch_use_max_train_batches)\n",
    "        if epoch%validate_every_epochs==0 and epoch!=0:\n",
    "            val_loss = validate_epoch(criterion, model, test_loader, per_epoch_use_max_test_batches)\n",
    "        else:\n",
    "            try:\n",
    "                val_loss = losses[\"val\"][-1]\n",
    "            except:\n",
    "                val_loss = train_loss\n",
    "        if epoch and train_loss<=min(losses[\"train\"]) and best_model_path:\n",
    "            torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }, best_model_path)\n",
    "        \n",
    "        losses[\"train\"].append(train_loss)\n",
    "        losses[\"val\"].append(val_loss)        \n",
    "        plt.plot(losses[\"train\"], label=\"training loss\")\n",
    "        plt.plot(losses[\"val\"], label=\"validation loss\")\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        if image_path is not None:\n",
    "            plt.savefig(image_path)\n",
    "            plt.clf()\n",
    "        else:\n",
    "            display_IPython.clear_output(wait=True)\n",
    "            plt.pause(0.001)\n",
    "            plt.show()\n",
    "        if epoch % save_model_every_epochs==save_model_every_epochs-1 and model_path:\n",
    "            torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }, model_path)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset class and Data Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_9229376515605844030() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_9229376515605844030()\">Toggle show/hide next cell</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hide_toggle(for_next=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Noisy_Non_Noisy_Image_Dataloader(Dataset):\n",
    "    def __init__(self, GT_file_path, h5f_full_path):\n",
    "        self.df_GTs = pd.DataFrame.from_dict(json.load(open(GT_file_path))) \n",
    "        # Que te linkee el nombre de la entrada del h5f (un index del batch) con los ground truth phis\n",
    "\n",
    "        self.h5f = h5py.File(f\"{h5f_full_path}\", 'r')\n",
    "        self.num_batches = len(self.h5f)\n",
    "        shape = np.array(self.h5f[ str(self.df_GTs['ID'][0]) ]).shape\n",
    "\n",
    "        self.batch_size = shape[0]//2\n",
    "        self.image_size = shape[1:]\n",
    "        print(f\"There are {self.batch_size} images per batch\\nwith {self.image_size} size images\\nand {self.num_batches} batches in total\")\n",
    "        print(f\"A total of {self.batch_size*self.num_batches} images with their GTs (denoised versions).\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_and_gt = torch.tensor(np.array(self.h5f[str(idx)]), device=device, dtype=torch.float32).unsqueeze(1) \n",
    "        # [2*batch_size, 1, 2X+1, 2X1]                                          en h5f son uint8\n",
    "        return data_and_gt[:self.batch_size], data_and_gt[self.batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class denoise_criterion:\n",
    "    def __init__(self, general_mse_over_base_mse, saturation_threshold):\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.w  = general_mse_over_base_mse\n",
    "        self.saturation_threshold = torch.tensor(saturation_threshold, device=device, dtype=torch.float32)\n",
    "\n",
    "    def compute_loss(self, denoised, gt):\n",
    "        denoised_sat = torch.where(denoised<self.saturation_threshold,\n",
    "                                   denoised, self.saturation_threshold)/self.saturation_threshold\n",
    "        gt_sat = torch.where(gt<self.saturation_threshold,\n",
    "                                   gt, self.saturation_threshold)/self.saturation_threshold\n",
    "\n",
    "        return self.w*self.mse(denoised, gt)+(1-self.w)*( self.mse(denoised_sat, gt_sat) )\n",
    "    # si no funciona bien probar de ahcer una metrica aprate d ela mse ke pondere las diferencias por la media de las intensidades\n",
    "    # uno sobre la media mas bien\n",
    "    \n",
    "class simple_denoise_criterion:\n",
    "    def __init__(self, l2_vs_l1):\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.w = l2_vs_l1\n",
    "        \n",
    "    def compute_loss(denoised, gt):\n",
    "        return self.w*self.mse(denoised, gt)+(1-self.w)*self.l1(denoised, gt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Initialize the dataset and sampler (choose the number of batches per epoch, and their length) and fix the artificial noise hyperparameters\n",
    "\n",
    "Note that since in each epoch the dataset shown to the model will be random, we can use the same dataset as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/Noisy_Non_Noisy_Different_Angles\"\n",
    "GT_file_path_train = f\"{dataset_path}/TRAIN/GROUND_TRUTHS.json\"\n",
    "images_h5_path_train = f\"{dataset_path}/TRAIN/Noisy_Non_Noisy_Images_Dataset.h5\" \n",
    "GT_file_path_test = f\"{dataset_path}/TRAIN/GROUND_TRUTHS.json\"\n",
    "images_h5_path_test = f\"{dataset_path}/TRAIN/Noisy_Non_Noisy_Images_Dataset.h5\" \n",
    "\n",
    "save_stuff_path = f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/Denoiser/\"\n",
    "\n",
    "total_epochs = 10000\n",
    "validate_every_epochs = 20\n",
    "optimizer_step_every_batches = 7\n",
    "per_epoch_use_max_train_batches= 21\n",
    "per_epoch_use_max_test_batches=3\n",
    "save_model_every_epochs = 1\n",
    "torch.manual_seed(681)\n",
    "\n",
    "general_mse_over_base_mse = 0.8\n",
    "saturation_threshold = 0.1\n",
    "\n",
    "exp_name='INTENTO2_Noise_Corrector_Scratch_Network_general_MSE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 images per batch\n",
      "with (605, 605) size images\n",
      "and 20000 batches in total\n",
      "A total of 200000 images with their GTs (denoised versions).\n"
     ]
    }
   ],
   "source": [
    "training_data = Noisy_Non_Noisy_Image_Dataloader(GT_file_path_train, images_h5_path_train)\n",
    "#test_data = Noisy_Non_Noisy_Image_Dataloader(GT_file_path_test, images_h5_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the Hyperparameters and Initialize the Model and the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=302\n",
    "S0=2*X+1\n",
    "S1=2*220+1\n",
    "S2=2*180+1\n",
    "S3=2*120+1\n",
    "feats_S1=5\n",
    "feats_S2=7\n",
    "feats_S3=10\n",
    "feats_S0=10\n",
    "dropout_p=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters 5597346\n",
      "Initial shape torch.Size([2, 1, 605, 605])\n",
      "Conv01 shape torch.Size([2, 5, 441, 441]) should be [2,5,441, 441]\n",
      "Conv12 shape torch.Size([2, 7, 361, 361]) should be [2,7,361, 361]\n",
      "Conv23 shape torch.Size([2, 10, 241, 241]) should be [2,10,241, 241]\n",
      "DeConv32+mem shape torch.Size([2, 14, 361, 361]) should be [2,14,361, 361]\n",
      "DeConv21+mem shape torch.Size([2, 10, 441, 441]) should be [2,10,441, 441]\n",
      "DeConv10+mem shape torch.Size([2, 11, 605, 605]) should be [2,11,605, 605]\n",
      "Conv00 shape torch.Size([2, 1, 605, 605]) should be [2, 1, 605, 605]\n"
     ]
    }
   ],
   "source": [
    "model = Noise_Corrector(S0=S0, S1=S1, S2=S2, S3=S3,\n",
    "                 feats_S1=feats_S1, feats_S2=feats_S2, feats_S3=feats_S3, feats_S0=feats_S0,\n",
    "                 dropout_p=dropout_p ) \n",
    "\n",
    "print(f\"Number of parameters {get_n_params(model)}\")\n",
    "\n",
    "# In case we wish to transfer the learned parameters of another run\n",
    "check_file=\"Model_and_Optimizer_2022-05-10 12:43:06.965847_INTENTO2_Noise_Corrector_Scratch_Network_general_MSE.pt\"\n",
    "checkpoint = torch.load(save_stuff_path+f\"/{check_file}\")\n",
    "\n",
    "# move model to gpu if available\n",
    "model.to(device)\n",
    "model.print_shapes(batch_size=2)\n",
    "\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "\n",
    "# Initialize the weights of the model! Default initialization might already be fine!\n",
    "\n",
    "# we can use a MSE loss for the regression task we have in hands\n",
    "#criterion = simple_denoise_criterion(0.2)\n",
    "#criterion = nn.L1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = denoise_criterion(general_mse_over_base_mse=general_mse_over_base_mse, saturation_threshold=saturation_threshold)\n",
    "# CRITERION DE SIMILITUD DE LA CORRECCIÓN AL GT PURO!!!!! IS IT MSE REALLY??\n",
    "\n",
    "# we will choose as optimizer the \n",
    "#optimizer = torch.optim.Adagrad(model.parameters(), lr=0.1, lr_decay=0.01, weight_decay=0.3,\n",
    "#                                initial_accumulator_value=0, eps=1e-10)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-9, betas=(0.9, 0.99), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [100/200000 (0%)]\tLoss: 0.084948\n",
      "Train Epoch: 0 [200/200000 (0%)]\tLoss: 0.066628\n",
      "Train Epoch: 1 [100/200000 (0%)]\tLoss: 0.088089\n",
      "Train Epoch: 1 [200/200000 (0%)]\tLoss: 0.083301\n",
      "Train Epoch: 2 [100/200000 (0%)]\tLoss: 0.073343\n",
      "Train Epoch: 2 [200/200000 (0%)]\tLoss: 0.078715\n",
      "Train Epoch: 3 [100/200000 (0%)]\tLoss: 0.082828\n",
      "Train Epoch: 3 [200/200000 (0%)]\tLoss: 0.077913\n",
      "Train Epoch: 4 [100/200000 (0%)]\tLoss: 0.080604\n",
      "Train Epoch: 4 [200/200000 (0%)]\tLoss: 0.081867\n",
      "Train Epoch: 5 [100/200000 (0%)]\tLoss: 0.079958\n",
      "Train Epoch: 5 [200/200000 (0%)]\tLoss: 0.085525\n",
      "Train Epoch: 6 [100/200000 (0%)]\tLoss: 0.085772\n",
      "Train Epoch: 6 [200/200000 (0%)]\tLoss: 0.072601\n",
      "Train Epoch: 7 [100/200000 (0%)]\tLoss: 0.078986\n",
      "Train Epoch: 7 [200/200000 (0%)]\tLoss: 0.078444\n",
      "Train Epoch: 8 [100/200000 (0%)]\tLoss: 0.079332\n",
      "Train Epoch: 8 [200/200000 (0%)]\tLoss: 0.081676\n",
      "Train Epoch: 9 [100/200000 (0%)]\tLoss: 0.083834\n",
      "Train Epoch: 9 [200/200000 (0%)]\tLoss: 0.075518\n",
      "Train Epoch: 10 [100/200000 (0%)]\tLoss: 0.078685\n",
      "Train Epoch: 10 [200/200000 (0%)]\tLoss: 0.080864\n",
      "Train Epoch: 11 [100/200000 (0%)]\tLoss: 0.081307\n",
      "Train Epoch: 11 [200/200000 (0%)]\tLoss: 0.079326\n",
      "Train Epoch: 12 [100/200000 (0%)]\tLoss: 0.088624\n",
      "Train Epoch: 12 [200/200000 (0%)]\tLoss: 0.088984\n",
      "Train Epoch: 13 [100/200000 (0%)]\tLoss: 0.078929\n",
      "Train Epoch: 13 [200/200000 (0%)]\tLoss: 0.077137\n",
      "Train Epoch: 14 [100/200000 (0%)]\tLoss: 0.070185\n",
      "Train Epoch: 14 [200/200000 (0%)]\tLoss: 0.080228\n",
      "Train Epoch: 15 [100/200000 (0%)]\tLoss: 0.088742\n",
      "Train Epoch: 15 [200/200000 (0%)]\tLoss: 0.075237\n",
      "Train Epoch: 16 [100/200000 (0%)]\tLoss: 0.083215\n",
      "Train Epoch: 16 [200/200000 (0%)]\tLoss: 0.078846\n",
      "Train Epoch: 17 [100/200000 (0%)]\tLoss: 0.075978\n",
      "Train Epoch: 17 [200/200000 (0%)]\tLoss: 0.089006\n",
      "Train Epoch: 18 [100/200000 (0%)]\tLoss: 0.084211\n",
      "Train Epoch: 18 [200/200000 (0%)]\tLoss: 0.074599\n",
      "Train Epoch: 19 [100/200000 (0%)]\tLoss: 0.078553\n",
      "Train Epoch: 19 [200/200000 (0%)]\tLoss: 0.076243\n",
      "Train Epoch: 20 [100/200000 (0%)]\tLoss: 0.086633\n",
      "Train Epoch: 20 [200/200000 (0%)]\tLoss: 0.082061\n",
      "\n",
      "Validation set: Average loss: 0.0879, Average Abs Error: [2.5619795], Maximum Abs Error: [[[0.23781304 0.23781635 0.23781294 ... 0.23781532 0.23781516 0.23781493]\n",
      "  [0.23782106 0.2378311  0.23783109 ... 0.237809   0.23781584 0.23782894]\n",
      "  [0.2378179  0.23782851 0.23782694 ... 0.23780626 0.23781501 0.23784848]\n",
      "  ...\n",
      "  [0.2378053  0.23780315 0.23779738 ... 0.23781802 0.23780954 0.23782144]\n",
      "  [0.23780537 0.23780343 0.237803   ... 0.23782763 0.23782243 0.23782678]\n",
      "  [0.2378067  0.23780599 0.23780634 ... 0.23781808 0.23781429 0.23781279]]] \n",
      "\n",
      "Train Epoch: 21 [100/200000 (0%)]\tLoss: 0.076518\n",
      "Train Epoch: 21 [200/200000 (0%)]\tLoss: 0.069793\n",
      "Train Epoch: 22 [100/200000 (0%)]\tLoss: 0.074926\n",
      "Train Epoch: 22 [200/200000 (0%)]\tLoss: 0.077702\n",
      "Train Epoch: 23 [100/200000 (0%)]\tLoss: 0.078103\n",
      "Train Epoch: 23 [200/200000 (0%)]\tLoss: 0.077523\n",
      "Train Epoch: 24 [100/200000 (0%)]\tLoss: 0.083719\n",
      "Train Epoch: 24 [200/200000 (0%)]\tLoss: 0.086784\n",
      "Train Epoch: 25 [100/200000 (0%)]\tLoss: 0.080472\n",
      "Train Epoch: 25 [200/200000 (0%)]\tLoss: 0.080245\n",
      "Train Epoch: 26 [100/200000 (0%)]\tLoss: 0.077512\n",
      "Train Epoch: 26 [200/200000 (0%)]\tLoss: 0.093716\n",
      "Train Epoch: 27 [100/200000 (0%)]\tLoss: 0.083465\n",
      "Train Epoch: 27 [200/200000 (0%)]\tLoss: 0.068319\n",
      "Train Epoch: 28 [100/200000 (0%)]\tLoss: 0.095744\n",
      "Train Epoch: 28 [200/200000 (0%)]\tLoss: 0.068045\n",
      "Train Epoch: 29 [100/200000 (0%)]\tLoss: 0.079085\n",
      "Train Epoch: 29 [200/200000 (0%)]\tLoss: 0.084593\n",
      "Train Epoch: 30 [100/200000 (0%)]\tLoss: 0.084380\n",
      "Train Epoch: 30 [200/200000 (0%)]\tLoss: 0.083226\n",
      "Train Epoch: 31 [100/200000 (0%)]\tLoss: 0.079068\n",
      "Train Epoch: 31 [200/200000 (0%)]\tLoss: 0.082218\n",
      "Train Epoch: 32 [100/200000 (0%)]\tLoss: 0.069440\n",
      "Train Epoch: 32 [200/200000 (0%)]\tLoss: 0.084272\n",
      "Train Epoch: 33 [100/200000 (0%)]\tLoss: 0.085593\n",
      "Train Epoch: 33 [200/200000 (0%)]\tLoss: 0.069409\n",
      "Train Epoch: 34 [100/200000 (0%)]\tLoss: 0.088621\n",
      "Train Epoch: 34 [200/200000 (0%)]\tLoss: 0.077556\n",
      "Train Epoch: 35 [100/200000 (0%)]\tLoss: 0.070417\n",
      "Train Epoch: 35 [200/200000 (0%)]\tLoss: 0.081765\n",
      "Train Epoch: 36 [100/200000 (0%)]\tLoss: 0.075437\n",
      "Train Epoch: 36 [200/200000 (0%)]\tLoss: 0.092085\n",
      "Train Epoch: 37 [100/200000 (0%)]\tLoss: 0.070029\n",
      "Train Epoch: 37 [200/200000 (0%)]\tLoss: 0.077643\n",
      "Train Epoch: 38 [100/200000 (0%)]\tLoss: 0.080473\n",
      "Train Epoch: 38 [200/200000 (0%)]\tLoss: 0.079365\n",
      "Train Epoch: 39 [100/200000 (0%)]\tLoss: 0.079647\n",
      "Train Epoch: 39 [200/200000 (0%)]\tLoss: 0.072487\n",
      "Train Epoch: 40 [100/200000 (0%)]\tLoss: 0.079677\n",
      "Train Epoch: 40 [200/200000 (0%)]\tLoss: 0.083304\n",
      "\n",
      "Validation set: Average loss: 0.0875, Average Abs Error: [2.5558348], Maximum Abs Error: [[[0.237813   0.23781632 0.23781289 ... 0.23781528 0.23781511 0.23781489]\n",
      "  [0.23782103 0.23783109 0.23783107 ... 0.23780896 0.23781578 0.23782887]\n",
      "  [0.23781785 0.2378285  0.2378269  ... 0.2378062  0.23781496 0.23784837]\n",
      "  ...\n",
      "  [0.23780526 0.2378031  0.23779733 ... 0.23781793 0.2378095  0.2378214 ]\n",
      "  [0.23780532 0.23780338 0.23780295 ... 0.23782757 0.23782238 0.23782673]\n",
      "  [0.23780665 0.23780595 0.23780629 ... 0.23781802 0.23781425 0.23781274]]] \n",
      "\n",
      "Train Epoch: 41 [100/200000 (0%)]\tLoss: 0.081571\n",
      "Train Epoch: 41 [200/200000 (0%)]\tLoss: 0.079047\n",
      "Train Epoch: 42 [100/200000 (0%)]\tLoss: 0.080241\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "losses = full_training_loop(model, criterion, optimizer, training_data, training_data, \n",
    "                    epochs=total_epochs, print_loss_every_batches=10,\n",
    "                            validate_every_epochs=validate_every_epochs,\n",
    "                           optimizer_step_every_batches=optimizer_step_every_batches,\n",
    "                           per_epoch_use_max_train_batches=per_epoch_use_max_train_batches, \n",
    "                            per_epoch_use_max_test_batches=per_epoch_use_max_test_batches,\n",
    "                           image_path=save_stuff_path+ f\"/Training_Loss_{datetime.now()}_{exp_name}.png\",\n",
    "                           save_model_every_epochs=save_model_every_epochs, \n",
    "                            model_path=save_stuff_path+f\"/Model_and_Optimizer_{datetime.now()}_{exp_name}.pt\",\n",
    "                            best_model_path=save_stuff_path+f\"/BEST_Model_and_Optimizer_{datetime.now()}_{exp_name}.pt\"\n",
    "                           )\n",
    "# Execute the training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the resulting model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }, save_stuff_path+f\"FINAL_Model_and_Optimizer_{datetime.now()}_{exp_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\\nFINAL VALIDATION! ####################################################\\n\\n\")\n",
    "print(\"Train Set\")\n",
    "#validate_epoch(nn.MSELoss(), model, sampler, dataset, per_epoch_use_max_train_batches)\n",
    "print(\"Test Set\")\n",
    "validate_epoch(nn.MSELoss(), model, test_dataloader, per_epoch_use_max_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_toggle(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intensity_gravity_centers(images):\n",
    "    \"\"\"\n",
    "        Expects input image to be an array of dimensions [N_imgs, h, w].\n",
    "        It will return an array of gravity centers [N_imgs, 2(h,w)] in pixel coordinates\n",
    "        Remember that pixel coordinates are set equal to array indices\n",
    "\n",
    "    \"\"\"\n",
    "    # image wise total intensity and marginalized inensities for weighted sum\n",
    "    intensity_in_w = torch.sum(images, dim=1) # weights for x [N_images, raw_width]\n",
    "    intensity_in_h = torch.sum(images, dim=2) # weights for y [N_images, raw_height]\n",
    "    total_intensity = intensity_in_h.sum(dim=1) # [N_images]\n",
    "\n",
    "    # Compute mass center for intensity\n",
    "    # [N_images, 2] (h_center,w_center)\n",
    "    return torch.nan_to_num( torch.stack(\n",
    "        (torch.matmul(intensity_in_h.float(), torch.arange(images.shape[1], \n",
    "                                    dtype=torch.float32, device=device))/total_intensity,\n",
    "         torch.matmul(intensity_in_w.float(), torch.arange(images.shape[2], \n",
    "                                    dtype=torch.float32, device=device))/total_intensity),\n",
    "        dim=1\n",
    "        ), nan=0.0, posinf=None, neginf=None)\n",
    "\n",
    "def compute_raw_to_centered_iX(images, X=302):\n",
    "\n",
    "        g_raw = compute_intensity_gravity_centers(images) # [ N_images, 2]\n",
    "\n",
    "        # crop the iamges with size (X+1+X)^2 leaving the gravity center in\n",
    "        # the central pixel of the image. In case the image is not big enough for the cropping,\n",
    "        # a 0 padding will be made.\n",
    "        centered_images = torch.zeros( ( images.shape[0], 2*X+1, 2*X+1),  dtype = images.dtype, \n",
    "                                      device=device)\n",
    "\n",
    "        # we round the gravity centers to the nearest pixel indices\n",
    "        g_index_raw = torch.round(g_raw).int() #[ N_images, 2]\n",
    "\n",
    "        # obtain the slicing indices around the center of gravity\n",
    "        # TODO -> make all this with a single array operation by stacking the lower and upper in\n",
    "        # a new axis!!\n",
    "        # [ N_images, 2 (h,w)]\n",
    "        unclipped_lower = g_index_raw-X\n",
    "        unclipped_upper = g_index_raw+X+1\n",
    "\n",
    "        # unclipped could get out of bounds for the indices, so we clip them\n",
    "        lower_bound = torch.clip( unclipped_lower.float(), min=torch.Tensor([[0,0]]).to(device),\n",
    "                                 max=torch.Tensor(list(images.shape[1:])).unsqueeze(0).to(device)).int()\n",
    "        upper_bound = torch.clip( unclipped_upper.float(), min=torch.Tensor([[0,0]]).to(device),\n",
    "                                 max=torch.Tensor(list(images.shape[1:])).unsqueeze(0).to(device)).int()\n",
    "        # we use the difference between the clipped and unclipped to get the necessary padding\n",
    "        # such that the center of gravity is left still in the center of the image\n",
    "        padding_lower = lower_bound-unclipped_lower\n",
    "        padding_upper = upper_bound-unclipped_upper\n",
    "\n",
    "        # crop the image\n",
    "        for im in range(g_raw.shape[0]):\n",
    "            centered_images[im, padding_lower[ im, 0]:padding_upper[ im, 0] or None,\n",
    "                                        padding_lower[ im, 1]:padding_upper[ im, 1] or None] = \\\n",
    "                      images[im, lower_bound[ im, 0]:upper_bound[ im, 0],\n",
    "                                          lower_bound[ im, 1]:upper_bound[ im, 1]]\n",
    "\n",
    "        return centered_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "plot3d_resolution=0.3\n",
    "\n",
    "path = \"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/UMAP_Regressor/TEST_IMAGES/\"\n",
    "image_names = os.listdir(f\"{path}\")\n",
    "\n",
    "predicted_corrections={}\n",
    "problem_images = {}\n",
    "os.makedirs(f\"{save_stuff_path}/Test_Corrections/\", exist_ok=True)\n",
    "\n",
    "for im_n in image_names:\n",
    "    model.eval()\n",
    "    im = cv2.imread(path+im_n, cv2.IMREAD_ANYDEPTH)\n",
    "    im_type = im.dtype\n",
    "    max_int = 2**8-1 if im_type==np.uint8 else 2**16-1 if im_type==np.uint16 else 2**32-1 if im_type==np.uint32 else 2**53-1 if im_type==np.uint64 else None\n",
    "    im = compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))\n",
    "    corr_im = model(im)[0] #[1, 2X+1, 2X+1]\n",
    "    corr_im = max_int*(torch.abs(corr_im/corr_im.amax(dim=(1,2), keepdim=True)[0].unsqueeze(1)))\n",
    "    corr_im = np.asarray(corr_im.detach().to('cpu').squeeze(0)).astype(im_type)\n",
    "    predicted_corrections[im_n] = corr_im\n",
    "    im = np.asarray(im.detach().to('cpu').squeeze(0)).astype(im_type)\n",
    "    problem_images[im_n] = im\n",
    "    \n",
    "    # On the one hand we want to plot the resulting corrections on the test set\n",
    "    fig = plt.figure(figsize=(2*4.5, 2*4.5))\n",
    "    axes=fig.subplots(2,2)\n",
    "\n",
    "    cm=axes[0, 0].imshow(im, cmap='viridis')\n",
    "    cm2 = axes[0,1].imshow(corr_im, cmap='viridis')\n",
    "    axes[0,0].grid(True)\n",
    "    axes[0,1].grid(True)\n",
    "\n",
    "    axes[1,0].set_visible(False)\n",
    "    axes[1,1].set_visible(False)\n",
    "    ax = fig.add_subplot(223, projection='3d')\n",
    "    Xs,Ys = np.meshgrid(np.arange(im.shape[0]),np.arange(im.shape[1]))\n",
    "    fig.suptitle(f\"Intesity Profiles for Image\\n{im_n}\")\n",
    "    cbax=fig.add_axes([0.54,0.05,0.4,0.01])\n",
    "    fig.colorbar(cm, ax=axes[0,0], cax=cbax, orientation='horizontal')\n",
    "    theta=25\n",
    "    phi=30\n",
    "    ax.plot_surface(Xs, Ys, im.T, rcount=int(im.shape[1]*plot3d_resolution), \n",
    "                    ccount=int(im.shape[0]*plot3d_resolution), cmap='viridis') # rstride=1, cstride=1, linewidth=0\n",
    "    ax.set_xlabel('Y')\n",
    "    #ax.set_xlim(-8, 8)\n",
    "    ax.set_ylabel('X')\n",
    "    #ax.set_ylim(-10, 8)\n",
    "    ax.set_zlabel('Intensity')\n",
    "    ax.set_zlim(-0.078*np.max(im), np.max(im))\n",
    "    ax.set_title(\"Image intensity 3D plot\")\n",
    "    ax.view_init(10, theta)\n",
    "    #ax.get_proj = lambda: np.dot(Axes3D.get_proj(ax), np.diag([1.3, 1.3, 1.3, 1]))\n",
    "\n",
    "    ax = fig.add_subplot(224, projection='3d')\n",
    "    Xs,Ys = np.meshgrid(np.arange(corr_im.shape[0]),np.arange(corr_im.shape[1]))\n",
    "    fig.suptitle(f\"Intesity Profiles for Image\\n{im_n}\")\n",
    "    theta=25\n",
    "    phi=30\n",
    "    ax.plot_surface(Xs, Ys, corr_im.T, rcount=int(corr_im.shape[0]*plot3d_resolution), ccount=int(corr_im.shape[1]*plot3d_resolution), cmap='viridis') # rstride=1, cstride=1, linewidth=0\n",
    "    ax.set_xlabel('Y')\n",
    "    #ax.set_xlim(-8, 8)\n",
    "    ax.set_ylabel('X')\n",
    "    #ax.set_ylim(-10, 8)\n",
    "    ax.set_zlabel('Intensity')\n",
    "    ax.set_zlim(-0.078*np.max(corr_im), np.max(corr_im))\n",
    "    ax.set_title(\"Image intensity 3D plot\")\n",
    "    ax.view_init(10, theta)\n",
    "    #ax.get_proj = lambda: np.dot(Axes3D.get_proj(ax), np.diag([1.3, 1.3, 1.3, 1]))\n",
    "    fig.savefig(f\"{save_stuff_path}/Test_Corrections/{im_n}\")\n",
    "    print(f\"{im_n} processed by the denoiser and image saved\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We now save them with the correct directory structure such that we can use the Todor Algorithms on them\n",
    "# This will work as a benchmark of how well the correction was made\n",
    "for i in [\"Problem\", \"Reference\"]:\n",
    "    os.makedirs( f\"{save_stuff_path}/Todor_Benchmark/17_18/{i}\", exist_ok=True)\n",
    "    os.makedirs( f\"{save_stuff_path}/Todor_Benchmark/18_19/{i}\", exist_ok=True)\n",
    "    os.makedirs( f\"{save_stuff_path}/Todor_Benchmark/28_29/{i}\", exist_ok=True)\n",
    "    os.makedirs( f\"{save_stuff_path}/Todor_Benchmark/43_44/{i}\", exist_ok=True)\n",
    "    os.makedirs( f\"{save_stuff_path}/Todor_Benchmark/70_71/{i}\", exist_ok=True)\n",
    "    os.makedirs( f\"{save_stuff_path}/Todor_Benchmark/con_los_dos/{i}\", exist_ok=True)\n",
    "    os.makedirs( f\"{save_stuff_path}/Todor_Benchmark/non_noisy_5_6/{i}\", exist_ok=True)\n",
    "    os.makedirs( f\"{save_stuff_path}/Todor_Benchmark/non_noisy_72_73/{i}\", exist_ok=True)\n",
    "    os.makedirs( f\"{save_stuff_path}/Todor_Benchmark/ortog/{i}\", exist_ok=True)\n",
    "    os.makedirs( f\"{save_stuff_path}/Todor_Benchmark/ref_vs_ref/{i}\", exist_ok=True)\n",
    "    os.makedirs( f\"{save_stuff_path}/Todor_Benchmark/sin_el_negativo/{i}\", exist_ok=True)\n",
    "    os.makedirs( f\"{save_stuff_path}/Todor_Benchmark/sin_el_positivo/{i}\", exist_ok=True)\n",
    "    \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/17_18/Problem/Corr_17.png\", predicted_corrections['IM_53017_phiCR_0.659442126750946.png'])    \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/17_18/Reference/Corr_18.png\", predicted_corrections['IM_53018_phiCR_-2.2813968658447266.png'])    \n",
    "\n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/18_19/Problem/Corr_18.png\", predicted_corrections['IM_53018_phiCR_-2.2813968658447266.png'])    \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/18_19/Reference/Corr_19.png\", predicted_corrections['IM_53019_phiCR_-2.679948091506958.png'])    \n",
    "\n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/28_29/Problem/Corr_28.png\", predicted_corrections['IM_52928_phiCR_0.6789670586585999.png'])    \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/28_29/Reference/Corr_29.png\", predicted_corrections['IM_52929_phiCR_0.9714600443840027.png'])    \n",
    "\n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/43_44/Problem/Corr_43.png\", predicted_corrections['IM_43_phiCR_-1.57120680809021.png'])    \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/43_44/Reference/Corr_44.png\", predicted_corrections['IM_44_phiCR_2.6544740200042725.png'])  \n",
    "\n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/70_71/Problem/Corr_70.png\", predicted_corrections['IM_40870_phiCR_-0.6731816530227661.png'])    \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/70_71/Reference/Corr_71.png\", predicted_corrections['IM_40871_phiCR_-2.4470927715301514.png'])  \n",
    "\n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/con_los_dos/Problem/Corr_con_los_dos.png\", predicted_corrections['con_los_dos.png'])    \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/con_los_dos/Reference/Corr_ref1.png\", predicted_corrections['sin_los_dos_solo_tubo.png'])  \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/con_los_dos/Reference/Corr_ref2.png\", predicted_corrections['antes_de_la_estandar.png'])  \n",
    "\n",
    "\n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/sin_el_negativo/Problem/Corr_sin_el_negativo.png\", predicted_corrections['sin_el_negativo.png'])    \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/sin_el_negativo/Reference/Corr_ref1.png\", predicted_corrections['sin_los_dos_solo_tubo.png'])  \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/sin_el_negativo/Reference/Corr_ref2.png\", predicted_corrections['antes_de_la_estandar.png'])  \n",
    "\n",
    "\n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/sin_el_positivo/Problem/Corr_sin_el_positivo.png\", predicted_corrections['sin_el_positivo.png'])    \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/sin_el_positivo/Reference/Corr_ref1.png\", predicted_corrections['sin_los_dos_solo_tubo.png'])  \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/sin_el_positivo/Reference/Corr_ref2.png\", predicted_corrections['antes_de_la_estandar.png'])  \n",
    "\n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/ref_vs_ref/Problem/Corr_ref1.png\", predicted_corrections['sin_los_dos_solo_tubo.png'])  \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/ref_vs_ref/Reference/Corr_ref2.png\", predicted_corrections['antes_de_la_estandar.png'])  \n",
    "\n",
    "\n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/ortog/Problem/Corr_90.png\", predicted_corrections['90__100.png'])  \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/ortog/Reference/Corr_Ref_90.png\", predicted_corrections['Reference__100.png'])  \n",
    "\n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/non_noisy_5_6/Problem/Corr_5.png\", predicted_corrections['IM_5_phiCR_-2.6049387454986572.png'])  \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/non_noisy_5_6/Reference/Corr_6.png\", predicted_corrections['IM_6_phiCR_-1.7562638521194458.png'])  \n",
    "\n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/non_noisy_72_73/Problem/Corr_72.png\", predicted_corrections['IM_72_phiCR_-2.946422576904297.png'])  \n",
    "cv2.imwrite(f\"{save_stuff_path}/Todor_Benchmark/non_noisy_72_73/Reference/Corr_73.png\", predicted_corrections['IM_73_phiCR_1.33404541015625.png'])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\nProceeding to apply Todor Algorithms...\\n\")\n",
    "os.system(f\"python ../../ANALYSIS_SCRIPTS/CODE_Get_Angle_Live.py {save_stuff_path+'/Todor_Benchmark/'} {0.05} False\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charge models and do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/Simple_Encoder/Noisy_Model_and_Optimizer_2022-01-24 19:57:31.886991.pt\")\n",
    "\n",
    "model = Simple_Encoder( X=X, feats_1=feats_1, feats_2=feats_2, feats_3=feats_3, feats_4=feats_4,\n",
    "                 prop1=prop1, prop2=prop2, prop3=prop3, av_pool1_div=av_pool1_div, conv4_feat_size=conv4_feat_size, av_pool2_div=av_pool2_div, \n",
    "                 out_fc_1=out_fc_1,\n",
    "                 dropout_p1=dropout_p1, dropout_p2=dropout_p2 ) \n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intensity_gravity_centers(images):\n",
    "    \"\"\"\n",
    "        Expects input image to be an array of dimensions [N_imgs, h, w].\n",
    "        It will return an array of gravity centers [N_imgs, 2(h,w)] in pixel coordinates\n",
    "        Remember that pixel coordinates are set equal to array indices\n",
    "\n",
    "    \"\"\"\n",
    "    # image wise total intensity and marginalized inensities for weighted sum\n",
    "    intensity_in_w = torch.sum(images, dim=1) # weights for x [N_images, raw_width]\n",
    "    intensity_in_h = torch.sum(images, dim=2) # weights for y [N_images, raw_height]\n",
    "    total_intensity = intensity_in_h.sum(dim=1) # [N_images]\n",
    "\n",
    "    # Compute mass center for intensity\n",
    "    # [N_images, 2] (h_center,w_center)\n",
    "    return torch.nan_to_num( torch.stack(\n",
    "        (torch.matmul(intensity_in_h.float(), torch.arange(images.shape[1], \n",
    "                                    dtype=torch.float32, device=device))/total_intensity,\n",
    "         torch.matmul(intensity_in_w.float(), torch.arange(images.shape[2], \n",
    "                                    dtype=torch.float32, device=device))/total_intensity),\n",
    "        dim=1\n",
    "        ), nan=0.0, posinf=None, neginf=None)\n",
    "\n",
    "def compute_raw_to_centered_iX(images, X=302):\n",
    "\n",
    "        g_raw = compute_intensity_gravity_centers(images) # [ N_images, 2]\n",
    "\n",
    "        # crop the iamges with size (X+1+X)^2 leaving the gravity center in\n",
    "        # the central pixel of the image. In case the image is not big enough for the cropping,\n",
    "        # a 0 padding will be made.\n",
    "        centered_images = torch.zeros( ( images.shape[0], 2*X+1, 2*X+1),  dtype = images.dtype, \n",
    "                                      device=device)\n",
    "\n",
    "        # we round the gravity centers to the nearest pixel indices\n",
    "        g_index_raw = torch.round(g_raw).int() #[ N_images, 2]\n",
    "\n",
    "        # obtain the slicing indices around the center of gravity\n",
    "        # TODO -> make all this with a single array operation by stacking the lower and upper in\n",
    "        # a new axis!!\n",
    "        # [ N_images, 2 (h,w)]\n",
    "        unclipped_lower = g_index_raw-X\n",
    "        unclipped_upper = g_index_raw+X+1\n",
    "\n",
    "        # unclipped could get out of bounds for the indices, so we clip them\n",
    "        lower_bound = torch.clip( unclipped_lower.float(), min=torch.Tensor([[0,0]]).to(device),\n",
    "                                 max=torch.Tensor(list(images.shape[1:])).unsqueeze(0).to(device)).int()\n",
    "        upper_bound = torch.clip( unclipped_upper.float(), min=torch.Tensor([[0,0]]).to(device),\n",
    "                                 max=torch.Tensor(list(images.shape[1:])).unsqueeze(0).to(device)).int()\n",
    "        # we use the difference between the clipped and unclipped to get the necessary padding\n",
    "        # such that the center of gravity is left still in the center of the image\n",
    "        padding_lower = lower_bound-unclipped_lower\n",
    "        padding_upper = upper_bound-unclipped_upper\n",
    "\n",
    "        # crop the image\n",
    "        for im in range(g_raw.shape[0]):\n",
    "            centered_images[im, padding_lower[ im, 0]:padding_upper[ im, 0] or None,\n",
    "                                        padding_lower[ im, 1]:padding_upper[ im, 1] or None] = \\\n",
    "                      images[im, lower_bound[ im, 0]:upper_bound[ im, 0],\n",
    "                                          lower_bound[ im, 1]:upper_bound[ im, 1]]\n",
    "\n",
    "        return centered_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display a FileChooser widget\n",
    "from ipyfilechooser import FileChooser\n",
    "path=\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter\"\n",
    "fc = FileChooser(path+'/LAB/EXPERIMENTAL/Fotos_Turpin/Day2/laser_gaussian_thesis/')\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a single experimental image to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image_full_path=fc.selected\n",
    "#image_full_path=\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/Experimental_Stuff/Fotos_Turpin/Day2/laser_gaussian_thesis/All_Taken_Photos/sin_el_positivo.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "if im is None:\n",
    "    print(f\" Unable to import image {image_full_path}\")\n",
    "    raise ValueError\n",
    "# Center in gravicenter, generating iX\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot its Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d_resolution=0.7\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "prof_x=np.sum(im, axis=0)\n",
    "prof_y=np.sum(im, axis=1)\n",
    "fig = plt.figure(figsize=(2*4.5, 2*4.5))\n",
    "axes=fig.subplots(2,2)\n",
    "\n",
    "cm=axes[0, 0].imshow(im, cmap='viridis')\n",
    "axes[0,0].grid(True)\n",
    "axes[0,1].scatter(prof_y, np.arange(len(prof_y)), s=1, label=f'Intensity profile in y')\n",
    "axes[0,1].set_ylim((0,len(prof_y)))\n",
    "axes[0,1].invert_yaxis()\n",
    "axes[1,0].scatter(np.arange(len(prof_x)), prof_x, s=1, label=f'Intensity profile in y')\n",
    "axes[1,0].set_xlim((0,len(prof_x)))\n",
    "axes[1,0].invert_yaxis()\n",
    "axes[0,0].set_xlabel(\"x (pixels)\")\n",
    "#axes[0,0].set_ylabel(\"y (pixels)\")\n",
    "axes[0,1].set_xlabel(\"Cummulative Intensity\")\n",
    "axes[0,1].set_ylabel(\"y (pixels)\")\n",
    "axes[1,0].set_ylabel(\"Cummulative Intensity\")\n",
    "axes[1,0].set_xlabel(\"x (pixels)\")\n",
    "axes[1,0].grid(True)\n",
    "axes[0,1].grid(True)\n",
    "axes[1,1].set_visible(False)\n",
    "ax = fig.add_subplot(224, projection='3d')\n",
    "Xs,Ys = np.meshgrid(np.arange(len(prof_y)),np.arange(len(prof_x)))\n",
    "fig.suptitle(f\"Intesity Profiles for Image\\n{image_full_path.split('/')[-1]}\")\n",
    "files_for_gif=[]\n",
    "cbax=fig.add_axes([0.54,0.05,0.4,0.01])\n",
    "fig.colorbar(cm, ax=axes[0,0], cax=cbax, orientation='horizontal')\n",
    "theta=25\n",
    "phi=30\n",
    "ax.plot_surface(Xs, Ys, im.T, rcount=int(len(prof_y)*plot3d_resolution), ccount=int(len(prof_x)*plot3d_resolution), cmap='viridis') # rstride=1, cstride=1, linewidth=0\n",
    "#cset = ax.contourf(X, Y, im, 2, zdir='z', offset=-20, cmap='viridis', alpha=0.5)\n",
    "#cset = ax.contourf(X, Y, im, 1, zdir='x', offset=-8, cmap='viridis')\n",
    "#cset = ax.contourf(X, Y, im, 1, zdir='y', offset=0, cmap='viridis')\n",
    "ax.set_xlabel('Y')\n",
    "#ax.set_xlim(-8, 8)\n",
    "ax.set_ylabel('X')\n",
    "#ax.set_ylim(-10, 8)\n",
    "ax.set_zlabel('Intensity')\n",
    "ax.set_zlim(-0.078*np.max(im), np.max(im))\n",
    "ax.set_title(\"Image intensity 3D plot\")\n",
    "ax.view_init(10, theta)\n",
    "#ax.get_proj = lambda: np.dot(Axes3D.get_proj(ax), np.diag([1.3, 1.3, 1.3, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NN predictions for $R_0, w_0, \\phi_{CR}, Z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Custom\")\n",
    "model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "print(f\"Predicted phi_CR {predictions[0]} rad {predictions[0]*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {predictions[0]/2} rad {predictions[0]*180/np.pi/2} deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "print(\"Referencia sin nada\\n\")\n",
    "%matplotlib inline\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day2/laser_gaussian_thesis/All_Taken_Photos/sin_los_dos_solo_tubo.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "ref=predictions[0].item()\n",
    "print(f\"Predicted phi_CR {ref} rad {ref*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {ref/2} rad {ref*180/np.pi/2} deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Referencia sin nada\\n\")\n",
    "%matplotlib inline\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day2/laser_gaussian_thesis/All_Taken_Photos/antes_de_la_estandar.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "ref2=predictions[0].item()\n",
    "print(f\"Predicted phi_CR {ref2} rad {ref2*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {ref2/2} rad {ref2*180/np.pi/2} deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sin el negativo\\n\")\n",
    "%matplotlib inline\n",
    "\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day2/laser_gaussian_thesis/All_Taken_Photos/sin_el_negativo.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "pos=predictions[0].item()\n",
    "print(f\"Predicted phi_CR {pos} rad {pos*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {pos/2} rad {pos*180/np.pi/2} deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sin el positivo\\n\")\n",
    "%matplotlib inline\n",
    "\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day2/laser_gaussian_thesis/All_Taken_Photos/sin_el_positivo.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "neg = predictions[0].item()\n",
    "print(f\"Predicted phi_CR {neg} rad {neg*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {neg/2} rad {neg*180/np.pi/2} deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Con ambos\\n\")\n",
    "%matplotlib inline\n",
    "\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day2/laser_gaussian_thesis/All_Taken_Photos/con_los_dos.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "both=predictions[0].item()\n",
    "print(f\"Predicted phi_CR {both} rad {both*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {both/2} rad {both*180/np.pi/2} deg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ref Ort\\n\")\n",
    "%matplotlib inline\n",
    "\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day3/Reference/Reference__100.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "ref_ort=predictions[0].item()\n",
    "print(f\"Predicted phi_CR {ref_ort} rad {ref_ort*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {ref_ort/2} rad {ref_ort*180/np.pi/2} deg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ref Ort\\n\")\n",
    "%matplotlib inline\n",
    "\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day3/Problem/90__100.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "ort=predictions[0].item()\n",
    "print(f\"Predicted phi_CR {ref_ort} rad {ort*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {ort/2} rad {ort*180/np.pi/2} deg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Positivo-Ref deberian ser {13.85} deg son {(pos-ref)*180/np.pi/2} deg\")\n",
    "print(f\"Negativo-Ref deberian ser {9.45} deg son {(neg-ref)*180/np.pi/2} deg\")\n",
    "print(f\"Ambos-Ref deberian ser {4.4} deg son {(both-ref)*180/np.pi/2} deg\\n\")\n",
    "\n",
    "print(f\"Positivo-Ref2 deberian ser {13.85} deg son {(pos-ref2)*180/np.pi/2} deg\")\n",
    "print(f\"Negativo-Ref2 deberian ser {9.45} deg son {(neg-ref2)*180/np.pi/2} deg\")\n",
    "print(f\"Ambos-Ref2 deberian ser {4.4} deg son {(both-ref2)*180/np.pi/2} deg\\n\")\n",
    "\n",
    "print(f\"Ref2-Ref deberian ser {0} deg son {(ref2-ref)*180/np.pi/2} deg\\n\")\n",
    "\n",
    "print(f\"El de noventa deberian ser {90} deg son {(ref_ort-ort)*180/np.pi/2} deg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the non-black-box algorithm estimate for $\\phi_{CR}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(f\"../../..\")\n",
    "import sys\n",
    "from SOURCE.CLASS_CODE_GPU_Classes import *\n",
    "from SOURCE.CLASS_CODE_Image_Manager import *\n",
    "from SOURCE.CLASS_CODE_Polarization_Obtention_Algorithms import Rotation_Algorithm, Mirror_Flip_Algorithm, Gradient_Algorithm\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image=im.copy()\n",
    "saturation=0.9\n",
    "pol_or_CR=\"pol\" \n",
    "deg_or_rad=\"deg\" # for the final output\n",
    "image_depth=8 # or 16 bit per pixel\n",
    "image_shortest_side=540\n",
    "randomization_seed=666\n",
    "recenter_average_image=False\n",
    "\n",
    "\n",
    "# 5. POLARIZATION RELATIVE ANGLES ###################################\n",
    "# Mirror with affine interpolation & Rotation Algorithms will be employed\n",
    "# Each using both Fibonacci and Quadratic Fit Search\n",
    "# Also a gradient algorithm\n",
    "theta_min_Rot=-np.pi\n",
    "theta_max_Rot=np.pi\n",
    "rad_min_Grav=3\n",
    "rad_max_Grav=image_shortest_side\n",
    "theta_min_Mir=0\n",
    "theta_max_Mir=np.pi\n",
    "initial_guess_delta_rad=0.1\n",
    "initial_guess_delta_pix=10\n",
    "use_exact_gravicenter=True\n",
    "precision_quadratic=1e-10\n",
    "max_it_quadratic=100\n",
    "cost_tolerance_quadratic=1e-14\n",
    "precision_fibonacci=1e-10\n",
    "max_points_fibonacci=100\n",
    "cost_tolerance_fibonacci=1e-14\n",
    "\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "im_type=np.uint16 if image_depth==16 else np.uint8\n",
    "max_intensity=65535 if image_depth==16 else 255\n",
    "np.random.seed(randomization_seed)\n",
    "polCR=1 if pol_or_CR=='CR' else 0.5\n",
    "\n",
    "# 6. POLARIZATION RELATIVE ANGLES ###################################\n",
    "# Mirror with affine interpolation & Rotation Algorithms will be employed\n",
    "# Each using both Fibonacci and Quadratic Fit Search\n",
    "# Results will be gathered in a table and outputed as an excel csv\n",
    "# Mock Image Loader\n",
    "# Computar el angulo de cada uno en un dataframe donde una de las entradas sea results y haya un result per fibo qfs y per rotation y mirror affine. Y luego procesar en un 7º paso estos angulos para obtener los angulos relativos etc y perhaps hacer tablucha con ground truth menos el resulting delta angle medido por el algoritmo\n",
    "image_loader = Image_Manager(mode=X, interpolation_flag=None)\n",
    "# Define the ROTATION ALGORITHM\n",
    "rotation_algorithm = Rotation_Algorithm(image_loader,\n",
    "    theta_min_Rot, theta_max_Rot, None,\n",
    "    initial_guess_delta_rad, use_exact_gravicenter, initialize_it=False)\n",
    "\n",
    "# Define the Affine Mirror algorithm\n",
    "mirror_algorithm = Mirror_Flip_Algorithm(image_loader,\n",
    "    theta_min_Mir, theta_max_Mir, None,\n",
    "    initial_guess_delta_rad, method=\"aff\", left_vs_right=True, use_exact_gravicenter=use_exact_gravicenter, initialize_it=False)\n",
    "\n",
    "# Define the Gradient algorithm\n",
    "gradient_algorithm = Gradient_Algorithm(image_loader,\n",
    "        rad_min_Grav, rad_max_Grav,\n",
    "        initial_guess_delta_pix,\n",
    "        use_exact_gravicenter)\n",
    "\n",
    "# A dictionary to gather all the resulting angles for each image\n",
    "\n",
    "individual_image_results = { 'polarization_method':[], 'optimization_1d':[], 'found_phiCR':[], 'predicted_opt_precision':[] }\n",
    "\n",
    "def to_result_dict(result_dict, alg, alg_name, opt_name, im_names):\n",
    "    for key, name in zip(alg.times.keys(), im_names):\n",
    "        result_dict['polarization_method'].append(alg_name)\n",
    "        result_dict['optimization_1d'].append(opt_name)\n",
    "        result_dict['found_phiCR'].append(alg.angles[key])\n",
    "        result_dict['predicted_opt_precision'].append(alg.precisions[key])\n",
    "image_container=np.zeros( (1, 2*X+1, 2*X+1), dtype=np.float64)\n",
    "image_names=[]\n",
    "# charge the image\n",
    "image_container[0]=image.astype(np.float64)\n",
    "image_names.append(f\"{fc.selected_filename}\")\n",
    "\n",
    "# charge the image loader:\n",
    "image_loader.import_converted_images_as_array(image_container, image_names)\n",
    "# Execute the Rotation and Mirror Algorithms:\n",
    "# ROTATION ######\n",
    "interpolation_flag=None\n",
    "# the interpolation algorithm used in case we disbale its usage for the iX image obtention will be the Lanczos one\n",
    "rotation_algorithm.interpolation_flag=interpolation_flag if interpolation_flag is not None else cv2.INTER_CUBIC\n",
    "rotation_algorithm.reInitialize(image_loader)\n",
    "rotation_algorithm.quadratic_fit_search(precision_quadratic, max_it_quadratic, cost_tolerance_quadratic)\n",
    "to_result_dict( individual_image_results, rotation_algorithm, \"Rotation\", \"Quadratic\", image_names)\n",
    "rotation_algorithm.reInitialize(image_loader)\n",
    "rotation_algorithm.fibonacci_ratio_search(precision_fibonacci, max_points_fibonacci, cost_tolerance_fibonacci)\n",
    "to_result_dict( individual_image_results, rotation_algorithm, \"Rotation\", \"Fibonacci\", image_names)\n",
    "\n",
    "# MIRROR #######\n",
    "mirror_algorithm.interpolation_flag=interpolation_flag if interpolation_flag is not None else cv2.INTER_CUBIC\n",
    "mirror_algorithm.reInitialize(image_loader)\n",
    "mirror_algorithm.quadratic_fit_search(precision_quadratic, max_it_quadratic, cost_tolerance_quadratic)\n",
    "to_result_dict( individual_image_results, rotation_algorithm, \"Mirror\", \"Quadratic\", image_names)\n",
    "mirror_algorithm.reInitialize(image_loader)\n",
    "mirror_algorithm.fibonacci_ratio_search(precision_fibonacci, max_points_fibonacci, cost_tolerance_fibonacci)\n",
    "to_result_dict( individual_image_results, rotation_algorithm, \"Mirror\", \"Fibonacci\", image_names)\n",
    "\n",
    "# GRADIENT #######\n",
    "def compute_intensity_gravity_center(image):\n",
    "    \"\"\"\n",
    "        Expects input image to be an array of dimensions [h, w].\n",
    "        It will return an array of gravity centers [2(h,w)] in pixel coordinates\n",
    "        Remember that pixel coordinates are set equal to numpy indices\n",
    "\n",
    "    \"\"\"\n",
    "    # image wise total intensity and marginalized inensities for weighted sum\n",
    "    intensity_in_w = np.sum(image, axis=0) # weights for x [raw_width]\n",
    "    intensity_in_h = np.sum(image, axis=1) # weights for y [raw_height]\n",
    "    total_intensity = intensity_in_h.sum()\n",
    "\n",
    "    # Compute mass center for intensity\n",
    "    # [2] (h_center,w_center)\n",
    "    return np.nan_to_num( np.stack(\n",
    "        (np.dot(intensity_in_h, np.arange(image.shape[0]))/total_intensity,\n",
    "         np.dot(intensity_in_w, np.arange(image.shape[1]))/total_intensity)\n",
    "        ) )\n",
    "\n",
    "optimal_masked_gravs={}\n",
    "optimal_radii={}\n",
    "grav=compute_intensity_gravity_center(image)\n",
    "\n",
    "gradient_algorithm.interpolation_flag=interpolation_flag if interpolation_flag is not None else cv2.INTER_CUBIC\n",
    "gradient_algorithm.reInitialize(image_loader)\n",
    "gradient_algorithm.quadratic_fit_search(precision_quadratic, max_it_quadratic, cost_tolerance_quadratic)\n",
    "to_result_dict( individual_image_results, gradient_algorithm, \"Gradient\", \"Quadratic\", image_names)\n",
    "#optimal_masked_gravs['quad'] = gradient_algorithm.masked_gravs[f\"Quadratic_Search_{fc.selected_filename}\"]\n",
    "#optimal_radii['quad'] = gradient_algorithm.optimals[f\"Quadratic_Search_{fc.selected_filename}\"]\n",
    "\n",
    "gradient_algorithm.reInitialize(image_loader)\n",
    "gradient_algorithm.fibonacci_ratio_search(precision_fibonacci, max_points_fibonacci, cost_tolerance_fibonacci)\n",
    "to_result_dict( individual_image_results, gradient_algorithm, \"Gradient\", \"Fibonacci\", image_names)\n",
    "\n",
    "#optimal_masked_gravs['fibo'] = gradient_algorithm.masked_gravs[f\"Fibonacci_Search_{fc.selected_filename}\"]\n",
    "#optimal_radii['fibo'] = gradient_algorithm.optimals[f\"Fibonacci_Search_{fc.selected_filename}\"]\n",
    "\n",
    "#masked_grav=(optimal_masked_gravs['quad']+optimal_masked_gravs['fibo'])/2.0\n",
    "#optimal_radi = (optimal_radii['quad']+optimal_radii['fibo'])/2\n",
    "#print(f\"\\n\\nOptimal masked gravs: {optimal_masked_gravs}\\nOptimal radii: {optimal_radii}\\n\\n\\n\")\n",
    "print(pd.DataFrame.from_dict(individual_image_results))\n",
    "\n",
    "# 7. PROCESS FINAL RESULTS ##########################################\n",
    "def angle_to_pi_pi( angle): # convert any angle to range ()-pi,pi]\n",
    "    angle= angle%(2*np.pi) # take it to [-2pi, 2pi]\n",
    "    return angle-np.sign(angle)*2*np.pi if abs(angle)>np.pi else angle    \n",
    "\n",
    "average_found_phiCR=np.mean([angle_to_pi_pi(phi) for i,phi in enumerate(individual_image_results['found_phiCR']) if individual_image_results['polarization_method'][i]!='Gradient'])\n",
    "print(\"Average found phiCR:\", average_found_phiCR)\n",
    "#print(f\"\\n\\nPredicted slope for main axis: by Gradient {(masked_grav[0]-grav[0])/(masked_grav[1]-grav[1])} and by the others averaged {np.tan(-average_found_phiCR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def forward(self, x): # [batch_size, 2X+1, 2X+1] or [batch_size, 1, 2X+1, 2X+1]\n",
    "    x = x.view(x.shape[0], 1, x.shape[-2], x.shape[-1]) # [batch_size, 1, 2X+1, 2X+1]\n",
    "    X=302\n",
    "    feats_1=15\n",
    "    feats_2=20\n",
    "    feats_3=20\n",
    "    feats_4=20\n",
    "    prop1=3\n",
    "    prop2=2\n",
    "    prop3=1\n",
    "    av_pool1_div=4\n",
    "    conv4_feat_size=15\n",
    "    av_pool2_div=10\n",
    "    out_fc_1=10 \n",
    "    print(x.shape, 2*X+1)\n",
    "\n",
    "    x = self.relu( self.conv1(x) ) # [batch_size, feats_1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "    print(\"conv1\",x.shape, prop1*(2*X+1)/5)\n",
    "\n",
    "\n",
    "    x = self.batchNorm2( self.relu( self.conv2(self.dropout1(x)) )) # [batch_size, feats_2, prop2*(2X+1)/5, prop2*(2X+1)/5]\n",
    "    print(\"conv2\",x.shape,  prop2*(2*X+1)/5)\n",
    "\n",
    "\n",
    "    x = self.relu( self.conv3(self.dropout2(x)) ) # [batch_size, feats_3, prop3*(2X+1)/5, prop3*(2X+1)/5]\n",
    "    print(\"conv3\",x.shape,  prop3*(2*X+1)/5)\n",
    "\n",
    "\n",
    "    x = self.avPool1(x) # [batch_size, feats_3, prop3*(2X+1)/5)/av_pool1_div, prop3*(2X+1)/5)/av_pool1_div]\n",
    "    print(\"av_pool1\",x.shape, int((prop3*(2*X+1)/5)/av_pool1_div))\n",
    "\n",
    "\n",
    "    x = self.batchNorm4(self.conv4(self.dropout2(x))) # [batch_size, feats_4, conv4_feat_size, conv4_feat_size]\n",
    "    print(\"conv4+batchn\",x.shape, conv4_feat_size)\n",
    "\n",
    "\n",
    "    x = self.relu( self.avPool2(x) ) # [batch_size, feats_4, conv4_feat_size/av_pool2_div, conv4_feat_size/av_pool2_div]\n",
    "    print(\"av_pool2\",x.shape, int(conv4_feat_size/av_pool2_div)+1)\n",
    "\n",
    "\n",
    "    x = x.view(x.shape[0], self.in_fc) #[batch_size, feats_4*int(conv4_feat_size/av_pool2_div)**2]\n",
    "    print(\"view_change\",x.shape, feats_4*int(conv4_feat_size/av_pool2_div+1)**2)\n",
    "\n",
    "\n",
    "    x = self.fc2( self.relu( self.fc1(x) ) ) #[batch_size, 4]\n",
    "    print(x.shape, 4)\n",
    "\n",
    "        return x\n",
    "a = Simple_Encoder().to(device)\n",
    "a(torch.ones(2,1, 605,605).to(device))\n",
    "del a\n",
    "torch.cuda.empty_cache()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crazy_epoch(epoch, criterion, model, optimizer, datas, targets, batch_number, batch_size,\n",
    "                      print_loss_every_batches=20,\n",
    "                    optimizer_step_every_batches=1):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    t2 = time()\n",
    "    for k in range(batch_number):        \n",
    "        \n",
    "        prediction = model(datas[k*batch_size:(k+1)*batch_size]) # data is [batch_size, 1, 2X+1, 2X+1]\n",
    "        loss = criterion(prediction, targets[k*batch_size:(k+1)*batch_size])\n",
    "        loss.backward()\n",
    "        \n",
    "        if k % optimizer_step_every_batches==optimizer_step_every_batches-1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # print loss every N batches\n",
    "        if k % print_loss_every_batches == print_loss_every_batches-1:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (k+1) * batch_size, len(datas),\n",
    "                100*(k+1)*batch_size / len(datas), loss.item()))\n",
    "\n",
    "        #total_loss += loss.item()  #.item() is very important here\n",
    "        # Why?-> In order to avoid having total_loss as a tensor in the gpu\n",
    "        t1= time()\n",
    "        print(f\"Iteration time{t1-t2}\")\n",
    "        t2 = time()\n",
    "\n",
    "    return total_loss / len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_crazy_training_loop(model, criterion, optimizer_generator, train_loader,\n",
    "                             batch_number, batch_size, epochs=10,\n",
    "                       print_loss_every_batches=20, optimizer_step_every_batches=1,\n",
    "                            meta_epoch_number=1):\n",
    "    %matplotlib inline\n",
    "    for meta_epoch in range(meta_epoch_number):\n",
    "        for meta_batch_id, (datas, targets) in enumerate(train_loader):        \n",
    "            datas, targets = datas.to(device), targets.to(device) # pero muuh gordos\n",
    "            losses = {\"train\": []}\n",
    "            optimizer = optimizer_generator(model)\n",
    "            for epoch in range(epochs): # que overfitee el muuh gordo este\n",
    "                train_loss = train_crazy_epoch(epoch, criterion, model, optimizer, datas,\n",
    "                                         targets, batch_number, batch_size,\n",
    "                                          print_loss_every_batches=20,\n",
    "                                            optimizer_step_every_batches=1)\n",
    "\n",
    "                display_IPython.clear_output(wait=True)\n",
    "                losses[\"train\"].append(train_loss)\n",
    "                plt.plot(losses[\"train\"], label=f\"log training loss- MetaBatch {meta_batch_id/len(train_loader)*100}%\")\n",
    "                plt.yscale('log')\n",
    "                plt.legend()\n",
    "                plt.pause(0.001)\n",
    "                plt.show()   \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_epoch_number = 1\n",
    "meta_batch_size = 100\n",
    "batch_size = 10\n",
    "batch_number = int(meta_batch_size/batch_size)\n",
    "assert(meta_batch_size%batch_size==0)\n",
    "\n",
    "crazy_loader = DataLoader(training_data, batch_size=meta_batch_size, shuffle=True, num_workers=worker_num,\n",
    "                              pin_memory=True, drop_last=False, persistent_workers=False)\n",
    "\n",
    "def adam_generator(model):\n",
    "    return torch.optim.Adam(model.parameters(), lr=0.05, betas=(0.99, 0.9999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "full_crazy_training_loop(model, criterion, \n",
    "                         adam_generator, \n",
    "                         crazy_loader,\n",
    "                             batch_number, batch_size, epochs=10,\n",
    "                       print_loss_every_batches=10, optimizer_step_every_batches=2, \n",
    "                         meta_epoch_number=meta_epoch_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time()\n",
    "for datas, targets in train_dataloader:\n",
    "    datas, targets = datas.to(device), targets.to(device)\n",
    "    pred = model(datas)\n",
    "    t2=time()\n",
    "    print(f\"inf time {t2-t1}\")\n",
    "    loss = criterion(pred, targets)\n",
    "    loss.backward()\n",
    "    t3=time()\n",
    "    print(f\"with backward {t3-t1}\")\n",
    "    t1=time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
