{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximity Metric for Images Belonging to a same phi_{CR} \n",
    "# in their Noisy and Non-Noisy Version\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "Perhaps in the future it could be interesting to force this in a strictly $phi_{CR}$ basis (with mixed $R_0$ and $w_0$-s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_13924468237385187853() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_13924468237385187853()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is just a function to allow toggleing code cells that are too long for good\n",
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "def hide_toggle(for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    toggle_text = 'Toggle show/hide'  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch #should be installed by default in any colab notebook\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from IPython import display as display_IPython\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
    "\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the functions and routines for the DL\n",
    "### Define the model and its constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proximity_Metric_Based_On_Simple_Encoder(nn.Module):\n",
    "    def __init__(self, X=302, feats_1=15, feats_2=20, feats_3=20, feats_4=20,\n",
    "                 prop1=3, prop2=2, prop3=1, av_pool1_div=4, conv4_feat_size=15, av_pool2_div=10, \n",
    "                 out_fc_1=10, out_fc_2=10,\n",
    "                 dropout_p1=0.2, dropout_p2=0.1\n",
    "                ): \n",
    "        # propj is such that the_ image getting out from stage j is propj/prop_{j-1}-ths of the previous (with j=0 being 5)\n",
    "        # clearly, prop_{j-1}>prop_{j}>...\n",
    "        # 2X+1 will be assumed to be divisible by 5\n",
    "        assert((2*X+1)%5==0)\n",
    "        assert(prop1>prop2)\n",
    "        assert(prop2>prop3)\n",
    "        assert((int((prop3*(2*X+1)/5)/av_pool1_div)-conv4_feat_size)>0)\n",
    "        \n",
    "        \n",
    "        super(Proximity_Metric_Based_On_Simple_Encoder, self).__init__()\n",
    "        # in is [epoch_size, 1, 2X+1, 2X+1]\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=feats_1, \n",
    "                               kernel_size = int((2*X+1)/5*(5-prop1)+1), bias=True) \n",
    "        # out conv1 [epoch_size, feats_1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "        self.conv2 = nn.Conv2d(in_channels=feats_1, out_channels=feats_2, \n",
    "                               kernel_size = int((2*X+1)/5*(prop1-prop2)+1), bias=True) \n",
    "        # out conv1 [epoch_size, feats_2, prop2*(prop1*(2X+1)/5)/prop1, prop2*(prop1*(2X+1)/5)/prop1]\n",
    "        # that is [epoch_size, feats_2, prop2*(2X+1)/5), prop2*(2X+1)/5)]\n",
    "        self.conv3 = nn.Conv2d(in_channels=feats_2, out_channels=feats_3, \n",
    "                               kernel_size = int((2*X+1)/5*(prop2-prop3)+1), bias=True)\n",
    "        # out conv3 is [epoch_size, feats_3, prop3*(2X+1)/5), prop3*(2X+1)/5)]\n",
    "\n",
    "        self.avPool1 = nn.AvgPool2d(kernel_size= int((prop3*(2*X+1)/5)*(1-1/av_pool1_div)) +1, stride=1)\n",
    "        # out avpool1 is [epoch_size, feats_3, prop3*(2X+1)/5)/av_pool1_div, prop3*(2X+1)/5)/av_pool1_div]\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=feats_3, out_channels=feats_4, \n",
    "                              kernel_size= int((prop3*(2*X+1)/5)/av_pool1_div+1)-conv4_feat_size+1, bias=True)\n",
    "        # [epoch_size, feats_4, conv4_feat_size, conv4_feat_size]\n",
    "        \n",
    "        self.avPool2 = nn.AvgPool2d(kernel_size= int(conv4_feat_size*(1-1/av_pool2_div)) +1, stride=1)\n",
    "        # out avpool1 is [epoch_size, feats_4, conv4_feat_size/av_pool2_div+1, conv4_feat_size/av_pool2_div+1]\n",
    "        \n",
    "        #self.in_fc = int(feats_4*(conv4_feat_size/av_pool2_div+1)**2)\n",
    "        self.in_fc = feats_4*((((((2*X+1-int((2*X+1)/5*(5-prop1)+1)+1)\n",
    "                                  -int((2*X+1)/5*(prop1-prop2)+1)+1)\n",
    "                                 -int((2*X+1)/5*(prop2-prop3)+1)+1)\n",
    "                                -int((prop3*(2*X+1)/5)*(1-1/av_pool1_div)) -1+1)\n",
    "                               -int((prop3*(2*X+1)/5)/av_pool1_div+1)+conv4_feat_size-1+1)\n",
    "                              -int(conv4_feat_size*(1-1/av_pool2_div)) -1+1)**2\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=self.in_fc, out_features=out_fc_1, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=out_fc_1, out_features=out_fc_2, bias=True)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=dropout_p1, inplace=False)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p2, inplace=False)\n",
    "        self.relu = torch.nn.functional.leaky_relu\n",
    "\n",
    "        self.batchNorm2 = nn.BatchNorm2d(num_features=feats_2)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(num_features=feats_4)\n",
    "\n",
    "    def forward(self, x): # [batch_size, 2X+1, 2X+1] or [batch_size, 1, 2X+1, 2X+1]\n",
    "        x = x.view(x.shape[0], 1, x.shape[-2], x.shape[-1]).float() # [batch_size, 1, 2X+1, 2X+1]\n",
    "        # Normalize to unity the float image\n",
    "        x = x/x.amax(dim=(2,3), keepdim=True)[0] # [batch_size, 1, 2X+1, 2X+1]\n",
    "        \n",
    "        x = self.relu( self.conv1(x) ) # [batch_size, feats_1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "        \n",
    "        x = self.batchNorm2( self.relu( self.conv2(self.dropout1(x)) )) # [batch_size, feats_2, prop2*(2X+1)/5, prop2*(2X+1)/5]\n",
    "\n",
    "        \n",
    "        x = self.relu( self.conv3(self.dropout2(x)) ) # [batch_size, feats_3, prop3*(2X+1)/5, prop3*(2X+1)/5]\n",
    "\n",
    "        \n",
    "        x = self.avPool1(x) # [batch_size, feats_3, prop3*(2X+1)/5)/av_pool1_div, prop3*(2X+1)/5)/av_pool1_div]\n",
    "\n",
    "        \n",
    "        x = self.batchNorm4(self.conv4(self.dropout2(x))) # [batch_size, feats_4, conv4_feat_size, conv4_feat_size]\n",
    "\n",
    "        \n",
    "        x = self.relu( self.avPool2(x) ) # [batch_size, feats_4, conv4_feat_size/av_pool2_div, conv4_feat_size/av_pool2_div]\n",
    "\n",
    "        \n",
    "        x = x.view(x.shape[0], self.in_fc) #[batch_size, feats_4*int(conv4_feat_size/av_pool2_div)**2]\n",
    "\n",
    "        \n",
    "        x = self.fc2( self.relu( self.fc1(x) ) ) #[batch_size, out_fc_2]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def print_shapes(self, batch_size=10, X=302):\n",
    "        x = torch.ones((batch_size, 1, 2*X+1, 2*X+1)).to(device)\n",
    "        print(f\"Initial shape {x.shape}\")\n",
    "        x = self.relu( self.conv1(x) ) # [batch_size, feats_1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "        print(f\"Post Conv1+relu shape {x.shape}\")\n",
    "        x = self.batchNorm2( self.relu( self.conv2(self.dropout1(x)) )) # [batch_size, feats_2, prop2*(2X+1)/5, prop2*(2X+1)/5]\n",
    "        print(f\"Post drop1+Conv2+relu+batchnorm shape {x.shape}\")\n",
    "        \n",
    "        x = self.relu( self.conv3(self.dropout2(x)) ) # [batch_size, feats_3, prop3*(2X+1)/5, prop3*(2X+1)/5]\n",
    "        print(f\"Post drop2+Conv3+relu shape {x.shape}\")\n",
    "        \n",
    "        x = self.avPool1(x) # [batch_size, feats_3, prop3*(2X+1)/5)/av_pool1_div, prop3*(2X+1)/5)/av_pool1_div]\n",
    "        print(f\"Post Av Pool1 shape {x.shape}\")\n",
    "        \n",
    "        x = self.batchNorm4(self.conv4(self.dropout2(x))) # [batch_size, feats_4, conv4_feat_size, conv4_feat_size]\n",
    "        print(f\"Post drop2+Conv4+batchnorm shape {x.shape}\")\n",
    "        \n",
    "        x = self.relu( self.avPool2(x) ) # [batch_size, feats_4, conv4_feat_size/av_pool2_div, conv4_feat_size/av_pool2_div]\n",
    "        print(f\"Post Av. Pool2 shape {x.shape}\")\n",
    "        \n",
    "        x = x.view(x.shape[0], self.in_fc) #[batch_size, feats_4*int(conv4_feat_size/av_pool2_div)**2]\n",
    "        print(f\"Post Pre-fc shape {x.shape}\")\n",
    "        \n",
    "        x = self.fc2( self.relu( self.fc1(x) ) ) #[batch_size, out_fc2]\n",
    "        print(f\"Post fc1+relu+fc2 shape {x.shape}\")\n",
    "\n",
    "    def load_my_state_dict(self, state_dict):\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if name not in own_state:\n",
    "                print(f\"Params NOT in own state: {name}\")\n",
    "                continue\n",
    "            if isinstance(param, nn.Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                param = param.data\n",
    "            print(f\"Params YES in own state: {name} shape on external {param.shape} shape on own {own_state[name].shape}\")\n",
    "            if param.shape==own_state[name].shape:\n",
    "                own_state[name].copy_(param)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Proximity_Metric_Based_On_Corrector(nn.Module):\n",
    "    def __init__(self, S0=2*302+1, S1=2*290+1, S2=2*250+1, S3=2*200+1, S4 = 2*10+1,\n",
    "                 S5 = 2*1+1, S6 =2,\n",
    "                 feats_S1=10, feats_S2=10, feats_S3=20, feats_S4=20, feats_S5 = 20,\n",
    "                 out_fc1=100, out_fc2=10,\n",
    "                 feats_S6 = 25,\n",
    "                 dropout_p=0.1\n",
    "                ): \n",
    "       \n",
    "        super(Proximity_Metric_Based_On_Corrector, self).__init__()\n",
    "        self.Ss = [S0, S1, S2, S3, S4, S5, S6]\n",
    "        self.feats = [1, feats_S1, feats_S2, feats_S3, feats_S4, feats_S5, feats_S6]\n",
    "        self.out_fc1 = out_fc1\n",
    "        self.out_fc2 = out_fc2\n",
    "        # in is [batch_size, 1, S0, S0]\n",
    "        self.conv_S01 = nn.Conv2d(in_channels=1, out_channels=feats_S1, \n",
    "                               kernel_size = S0-S1+1, bias=True) \n",
    "        # out conv_S01 [batch_size, feats_S1, S1, S1]\n",
    "        self.conv_S12 = nn.Conv2d(in_channels=feats_S1, out_channels=feats_S2, \n",
    "                               kernel_size = S1-S2+1, bias=True) \n",
    "        # out conv_S12 [batch_size, feats_S2, S2, S2]\n",
    "        self.conv_S23 = nn.Conv2d(in_channels=feats_S2, out_channels=feats_S3, \n",
    "                               kernel_size = S2-S3+1, bias=True) \n",
    "        # out conv_S23 [batch_size, feats_S3, S3, S3]\n",
    "        \n",
    "        self.conv_S33 = nn.Conv2d(in_channels=feats_S3, out_channels=feats_S3, \n",
    "                               kernel_size = 1, bias=True) \n",
    "        # out conv_S33 [batch_size, feats_S3, S3, S3]\n",
    "        \n",
    "        self.conv_S34 = nn.Conv2d(in_channels=feats_S3, out_channels=feats_S4, \n",
    "                               kernel_size = S3-S4+1, bias=True) \n",
    "        # out conv_S34 [batch_size, feats_S4, S4, S4]\n",
    "        \n",
    "        self.conv_S45 = nn.Conv2d(in_channels=feats_S4, out_channels=feats_S5, \n",
    "                               kernel_size = S4-S5+1, bias=True) \n",
    "        # out conv_S45 [batch_size, feats_S5, S5, S5]\n",
    "        self.conv_S56 = nn.Conv2d(in_channels=feats_S5, out_channels=feats_S6, \n",
    "                               kernel_size = S5-S6+1, bias=True) \n",
    "        # out conv_S56 [batch_size, feats_S6, S6, S6]\n",
    "        \n",
    "        self.in_fc1 = S6*S6*feats_S6\n",
    "        self.fc1 = nn.Linear(in_features=self.in_fc1, out_features=out_fc1, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=out_fc1, out_features=out_fc2, bias=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_p, inplace=False)\n",
    "        self.relu = torch.nn.functional.leaky_relu\n",
    "\n",
    "        self.batchNorm1 = nn.BatchNorm2d(num_features=feats_S3)\n",
    "        self.batchNorm2 = nn.BatchNorm1d(num_features=out_fc1)\n",
    "        \n",
    "\n",
    "    def forward(self, x): # [batch_size, 2X+1, 2X+1] or [batch_size, 1, 2X+1, 2X+1]\n",
    "        x = x.view(x.shape[0], 1, x.shape[-2], x.shape[-1]).float() # [batch_size, 1, 2X+1, 2X+1]\n",
    "        # Normalize to unity the float image\n",
    "        x = x/x.amax(dim=(2,3), keepdim=True)[0] # [batch_size, 1, 2X+1, 2X+1]\n",
    "        \n",
    "        # Conv layers\n",
    "        x = self.relu(self.conv_S01(x)) # [batch_size, feats_S1, S1, S1]\n",
    "        x = self.dropout( self.relu(self.conv_S12(x)) ) # [batch_size, feats_S2, S2, S2]\n",
    "        x = self.relu(self.conv_S23(x)) # [batch_size, feats_S3, S3, S3]\n",
    "        x = self.batchNorm1(self.relu(self.conv_S33(x)))\n",
    "        x = self.relu(self.conv_S34(x))\n",
    "        x = self.relu(self.conv_S45(x))\n",
    "        x = self.relu(self.conv_S56(x))\n",
    "        \n",
    "        x = x.view(x.shape[0], self.in_fc1)\n",
    "        x = self.dropout( self.relu(self.batchNorm2(self.fc1(self.dropout(x)))) )\n",
    "        return self.fc2(x)\n",
    "\n",
    "        \n",
    "    def print_shapes(self, batch_size=10):\n",
    "        x = torch.ones((batch_size, 1, self.Ss[0], self.Ss[0])).to(device)\n",
    "        print(f\"Initial shape {x.shape}\")\n",
    "        x = self.relu(self.conv_S01(x)) # [batch_size, feats_S1, S1, S1]\n",
    "        print(f\"Conv01 shape {x.shape} should be [{batch_size},{self.feats[1]},{self.Ss[1]}, {self.Ss[1]}]\")\n",
    "        x = self.dropout( self.relu(self.conv_S12(x)) ) # [batch_size, feats_S2, S2, S2]\n",
    "        print(f\"Conv12 shape {x.shape} should be [{batch_size},{self.feats[2]},{self.Ss[2]}, {self.Ss[2]}]\")\n",
    "        x = self.batchNorm1( self.relu(self.conv_S23(x)) ) # [batch_size, feats_S3, S3, S3]\n",
    "        print(f\"Conv23 shape {x.shape} should be [{batch_size},{self.feats[3]},{self.Ss[3]}, {self.Ss[3]}]\")\n",
    "        x = self.relu(self.conv_S34(x))  # [batch_size, feats_S3, S3, S3]\n",
    "        print(f\"Conv34 shape {x.shape} should be [{batch_size},{self.feats[4]},{self.Ss[4]}, {self.Ss[4]}]\")\n",
    "        x = self.relu(self.conv_S45(x))  # [batch_size, feats_S3, S3, S3]\n",
    "        print(f\"Conv45 shape {x.shape} should be [{batch_size},{self.feats[5]},{self.Ss[5]}, {self.Ss[5]}]\")\n",
    "        x = self.relu(self.conv_S56(x))  # [batch_size, feats_S3, S3, S3]\n",
    "        print(f\"Conv56 shape {x.shape} should be [{batch_size},{self.feats[6]},{self.Ss[6]}, {self.Ss[6]}]\")\n",
    "        x = x.view(x.shape[0], self.in_fc1)\n",
    "        print(f\"Input to fc1 is {x.shape} should be [{batch_size}, {self.in_fc1}]\")\n",
    "        x = self.dropout( self.relu(self.batchNorm2(self.fc1(self.dropout(x)))) )\n",
    "        print(f\"Output of fc1 is {x.shape} should be [{batch_size}, {self.out_fc1}]\")\n",
    "        x= self.fc2(x)\n",
    "        print(f\"Output of fc2 is {x.shape} should be [{batch_size}, {self.out_fc2}]\")\n",
    "    \n",
    "    def load_my_state_dict(self, state_dict):\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if name not in own_state:\n",
    "                print(f\"Params NOT in own state: {name}\")\n",
    "                continue\n",
    "            if isinstance(param, nn.Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                param = param.data\n",
    "            print(f\"Params YES in own state: {name} shape on external {param.shape} shape on own {own_state[name].shape}\")\n",
    "            if param.shape==own_state[name].shape:\n",
    "                own_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_9832012844871717990() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_9832012844871717990()\">Toggle show/hide next cell</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hide_toggle(for_next=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subroutine to count number of parameters in the model\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.numel()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The routines to validate and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_12512258476977854368() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_12512258476977854368()\">Toggle show/hide next cell</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hide_toggle(for_next=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, triplet_losses, model, optimizer, dataloader, print_loss_every_batches=20,\n",
    "                optimizer_step_every_batches=1, per_epoch_use_max_batches=None):\n",
    "    if per_epoch_use_max_batches is None:\n",
    "        per_epoch_use_max_batches = len(dataloader)\n",
    "        \n",
    "    av_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch_id in range(per_epoch_use_max_batches):  \n",
    "        data = dataloader[batch_id] # [PK, 2X+1, 2X+1] dataloader sends them to device already\n",
    "        embeddings = model(data) # embeddings is [PK, embedding_dim]\n",
    "        loss = torch.sum( triplet_losses.forward(embeddings) )\n",
    "        loss.backward()\n",
    "        \n",
    "        if batch_id % optimizer_step_every_batches==optimizer_step_every_batches-1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # print loss every N batches\n",
    "        if batch_id % print_loss_every_batches == print_loss_every_batches-1:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_id+1) * len(data), len(data)*per_epoch_use_max_batches,\n",
    "                100*(batch_id+1)*len(data) / (len(data)*per_epoch_use_max_batches), loss.item()))\n",
    "\n",
    "        #if batch_id % per_epoch_use_max_batches == per_epoch_use_max_batches-1:\n",
    "        #    break\n",
    "\n",
    "        av_loss += loss.item()/per_epoch_use_max_batches  #.item() is very important here\n",
    "\n",
    "    return av_loss\n",
    "\n",
    "\n",
    "def train_epoch_monitor_everything(epoch, triplet_losses, model, optimizer, dataloader,\n",
    "                print_loss_every_batches, optimizer_step_every_batches,\n",
    "                epoch_av_loss, epoch_av_non_zero_loss,\n",
    "                epoch_q_loss, epoch_q_dist_embs, epoch_q_norm_embs,\n",
    "                percentiles,\n",
    "                tolerance_zero_loss = 0, per_epoch_use_max_batches=None):\n",
    "    if per_epoch_use_max_batches is None:\n",
    "        per_epoch_use_max_batches = len(dataloader)\n",
    "    \n",
    "    # initialize batch metrics\n",
    "    epoch_av_loss.append(  0.0  )\n",
    "    epoch_av_non_zero_loss.append( 0.0 )\n",
    "    epoch_q_loss.append(  np.zeros((len(percentiles)))  )\n",
    "    epoch_q_dist_embs.append(  np.zeros((len(percentiles)))  )\n",
    "    epoch_q_norm_embs.append(  np.zeros((len(percentiles)))  )\n",
    "\n",
    "        \n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch_id in range(per_epoch_use_max_batches):  \n",
    "        data = dataloader[batch_id] # [PK, 2X+1, 2X+1] dataloader sends them to device already\n",
    "        embeddings = model(data) # embeddings is [PK, embedding_dim]\n",
    "        losses = triplet_losses.forward(embeddings) \n",
    "        loss = torch.sum(losses)\n",
    "        loss.backward()\n",
    "        \n",
    "        epoch_av_loss[-1] += loss.item()/per_epoch_use_max_batches # we divide it here to avoid overflowing it\n",
    "        epoch_q_loss[-1] += torch.quantile(losses, percentiles, dim=0).detach().to('cpu').numpy()\n",
    "        epoch_q_dist_embs[-1] += torch.quantile( triplet_losses.D[triplet_losses.triu_indices[0], \n",
    "                                                               triplet_losses.triu_indices[1]],\n",
    "                                            percentiles, dim=0).detach().to('cpu').numpy()\n",
    "        epoch_q_norm_embs[-1] += torch.quantile(torch.norm(embeddings, dim=1), percentiles, dim=0).detach().to('cpu').numpy()\n",
    "        epoch_av_non_zero_loss[-1] += torch.sum(losses>tolerance_zero_loss).item()/losses.shape[0]\n",
    "        \n",
    "        if batch_id % optimizer_step_every_batches==optimizer_step_every_batches-1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # print loss every N batches\n",
    "        if batch_id % print_loss_every_batches == print_loss_every_batches-1:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_id+1) * len(data), len(data)*per_epoch_use_max_batches,\n",
    "                100*(batch_id+1)*len(data) / (len(data)*per_epoch_use_max_batches), loss.item()))\n",
    "            \n",
    "        # Detach the matrix we are reusing\n",
    "        #print(triplet_losses.D.grad, triplet_losses.D.requires_grad)\n",
    "\n",
    "\n",
    "    # make them average values among the batches\n",
    "    epoch_q_loss[-1] /= per_epoch_use_max_batches\n",
    "    epoch_q_dist_embs[-1] /= per_epoch_use_max_batches\n",
    "    epoch_q_norm_embs[-1] /= per_epoch_use_max_batches\n",
    "    epoch_av_non_zero_loss[-1] /= per_epoch_use_max_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_8513316162150131410() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_8513316162150131410()\">Toggle show/hide next cell</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hide_toggle(for_next=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_training_loop(model, criterion, optimizer, train_loader, epochs=10,\n",
    "                       print_loss_every_batches=20, optimizer_step_every_batches=1,\n",
    "                      per_epoch_use_max_train_batches=None,\n",
    "                      image_path=None, save_model_every_epochs=1, model_path=None, best_model_path=None):\n",
    "    losses = {\"train\": []}\n",
    "    %matplotlib inline\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss = train_epoch(epoch, criterion, model, optimizer, train_loader,\n",
    "                                 print_loss_every_batches=print_loss_every_batches,\n",
    "                                optimizer_step_every_batches=optimizer_step_every_batches,\n",
    "                                per_epoch_use_max_batches=per_epoch_use_max_train_batches)\n",
    "        \n",
    "        if epoch and train_loss<=min(losses[\"train\"]) and best_model_path:\n",
    "            torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }, best_model_path)\n",
    "        \n",
    "        losses[\"train\"].append(train_loss)\n",
    "        plt.plot(losses[\"train\"], label=\"training loss\")\n",
    "        #plt.yscale('log')\n",
    "        plt.legend()\n",
    "        if image_path is not None:\n",
    "            plt.savefig(image_path)\n",
    "            plt.clf()\n",
    "        else:\n",
    "            display_IPython.clear_output(wait=True)\n",
    "            plt.pause(0.001)\n",
    "            plt.show()\n",
    "        if epoch % save_model_every_epochs==save_model_every_epochs-1 and model_path:\n",
    "            torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }, model_path)\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "from cycler import cycler\n",
    "def full_training_loop_monitor_everything(model, criterion, optimizer, train_loader, epochs=10,\n",
    "                       print_loss_every_batches=20, plot_metrics_every_epochs=1, optimizer_step_every_batches=1,\n",
    "                      per_epoch_use_max_train_batches=None,\n",
    "                      image_path=None, save_model_every_epochs=1, model_path=None, best_model_path=None,\n",
    "                                         percentiles=[0.1, 0.5, 0.9]):\n",
    "\n",
    "    # prepare the metric monitorers\n",
    "    epoch_av_loss=[]\n",
    "    epoch_av_non_zero_loss=[]\n",
    "    epoch_q_loss=[]\n",
    "    epoch_q_dist_embs=[]\n",
    "    epoch_q_norm_embs=[]\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    # prepare the metric plot figure\n",
    "    %matplotlib inline\n",
    "    fig, ax_array = plt.subplots(2, 2, figsize=(10,10))\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax_array[i,j].set_xlabel(\"Epoch\")\n",
    "    ax_array[0,0].set_title(\"Triplet Loss Percentiles averaged\\n over batches in epoch\")\n",
    "    ax_array[0,0].set_ylabel(\"Loss\")\n",
    "    ax_array[0,1].set_title(\"Average non-zero loss in Batch (over epoch)\")\n",
    "    ax_array[0,1].set_ylabel(\"Ratio of non-zero losses\")\n",
    "    ax_array[1,0].set_title(\"Distance Between Embedding Percentiles\\n averaged over batches in epoch\")\n",
    "    ax_array[1,0].set_ylabel(\"Distance\")\n",
    "    ax_array[1,1].set_title(\"Magnitude of Embeddings Percentiles\\n averaged over batches in epoch\")\n",
    "    ax_array[1,1].set_title(\"Norm\")\n",
    "    artists=[]\n",
    "    ps = np.array(percentiles.to('cpu'))\n",
    "    met_epochs=np.arange( 0, epoch, 1)\n",
    "    fig.subplots_adjust(hspace=0.6)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_epoch_monitor_everything(epoch, criterion, model, optimizer, train_loader,\n",
    "                                print_loss_every_batches=print_loss_every_batches,\n",
    "                                optimizer_step_every_batches=optimizer_step_every_batches,\n",
    "                                epoch_av_loss=epoch_av_loss, \n",
    "                                epoch_av_non_zero_loss=epoch_av_non_zero_loss,\n",
    "                                epoch_q_loss=epoch_q_loss,\n",
    "                                epoch_q_dist_embs=epoch_q_dist_embs,\n",
    "                                epoch_q_norm_embs=epoch_q_norm_embs,\n",
    "                                percentiles=percentiles,\n",
    "                                per_epoch_use_max_batches=per_epoch_use_max_train_batches)\n",
    "        \n",
    "        \n",
    "        if epoch and epoch_av_loss[-1] <= min_loss and best_model_path:\n",
    "            min_loss = epoch_av_loss[-1]\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                }, best_model_path)\n",
    "        \n",
    "        if epoch % plot_metrics_every_epochs==plot_metrics_every_epochs-1:\n",
    "\n",
    "            artists += ax_array[0,0].plot(met_epochs[:epoch], np.array(epoch_q_loss), label=ps)\n",
    "            artists += ax_array[0,0].plot(met_epochs[:epoch], epoch_av_loss, label=\"Average\", color=\"r\")\n",
    "            artists += ax_array[0,1].plot(met_epochs[:epoch], epoch_av_non_zero_loss, label=\"Average\"  )\n",
    "            artists += ax_array[1,0].plot(met_epochs[:epoch], np.array(epoch_q_dist_embs), label=ps)\n",
    "            artists += ax_array[1,1].plot(met_epochs[:epoch], np.array(epoch_q_norm_embs), label=ps)\n",
    "\n",
    "            for ax in ax_array.flatten():\n",
    "                ax.set_prop_cycle(cycler('color', [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]))\n",
    "                ax.legend()\n",
    "\n",
    "            if image_path is not None:\n",
    "                plt.savefig(image_path)\n",
    "                for artist in artists:\n",
    "                    artist.remove()\n",
    "                artists=[]\n",
    "            else:\n",
    "                display_IPython.clear_output(wait=True)\n",
    "                plt.pause(0.001)\n",
    "                plt.show()\n",
    "                artists=[]\n",
    "                \n",
    "        if epoch % save_model_every_epochs==save_model_every_epochs-1 and model_path:\n",
    "            torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }, model_path)\n",
    "\n",
    "    return losses\n",
    "\n",
    "def full_training_loop_monitor_partially(model, criterion, optimizer, train_loader, epochs=10,\n",
    "                       print_loss_every_batches=20, compute_metrics_every_epochs=2, plot_metrics_every_epochs=5, \n",
    "                    optimizer_step_every_batches=1,\n",
    "                      per_epoch_use_max_train_batches=None,\n",
    "                      image_path=None, save_model_every_epochs=1, model_path=None, best_model_path=None,\n",
    "                                         percentiles=[0.1, 0.5, 0.9]):\n",
    "    assert plot_metrics_every_epochs>=compute_metrics_every_epochs, \"Do not need to plot so frequently!\"\n",
    "    # prepare the metric monitorers\n",
    "    epoch_av_loss=[]\n",
    "    epoch_av_non_zero_loss=[]\n",
    "    epoch_q_loss=[]\n",
    "    epoch_q_dist_embs=[]\n",
    "    epoch_q_norm_embs=[]\n",
    "    min_loss = float('inf')\n",
    "    ps = np.array(percentiles.to('cpu'))\n",
    "    \n",
    "    # prepare the metric plot figure\n",
    "    %matplotlib inline\n",
    "    fig, ax_array = plt.subplots(2, 2, figsize=(10,10))\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax_array[i,j].set_xlabel(\"Epoch\")\n",
    "    ax_array[0,0].set_title(\"Triplet Loss Percentiles averaged\\n over batches in epoch\")\n",
    "    ax_array[0,0].set_ylabel(\"Loss\")\n",
    "    ax_array[0,1].set_title(\"Average non-zero loss in Batch (over epoch)\")\n",
    "    ax_array[0,1].set_ylabel(\"Ratio of non-zero losses\")\n",
    "    ax_array[1,0].set_title(\"Distance Between Embedding Percentiles\\n averaged over batches in epoch\")\n",
    "    ax_array[1,0].set_ylabel(\"Distance\")\n",
    "    ax_array[1,1].set_title(\"Magnitude of Embeddings Percentiles\\n averaged over batches in epoch\")\n",
    "    ax_array[1,1].set_title(\"Norm\")\n",
    "    artists=[]\n",
    "    fig.subplots_adjust(hspace=0.6)\n",
    "    met_epochs=np.arange(compute_metrics_every_epochs-1, epochs, compute_metrics_every_epochs)\n",
    "    all_epochs=np.arange(epochs)\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if epoch % compute_metrics_every_epochs==compute_metrics_every_epochs-1:\n",
    "            \n",
    "            train_epoch_monitor_everything(epoch, criterion, model, optimizer, train_loader,\n",
    "                                print_loss_every_batches=print_loss_every_batches,\n",
    "                                optimizer_step_every_batches=optimizer_step_every_batches,\n",
    "                                epoch_av_loss=epoch_av_loss, \n",
    "                                epoch_av_non_zero_loss=epoch_av_non_zero_loss,\n",
    "                                epoch_q_loss=epoch_q_loss,\n",
    "                                epoch_q_dist_embs=epoch_q_dist_embs,\n",
    "                                epoch_q_norm_embs=epoch_q_norm_embs,\n",
    "                                percentiles=percentiles,\n",
    "                                per_epoch_use_max_batches=per_epoch_use_max_train_batches)\n",
    "           \n",
    "            if epoch % plot_metrics_every_epochs==plot_metrics_every_epochs-1:\n",
    "                till = epoch // compute_metrics_every_epochs +1\n",
    "                artists += ax_array[0,0].plot(met_epochs[:till], np.array(epoch_q_loss), label=ps)\n",
    "                artists += ax_array[0,0].plot(all_epochs[:epoch+1], epoch_av_loss, label=\"Average\", color=\"r\")\n",
    "                artists += ax_array[0,1].plot(met_epochs[:till], epoch_av_non_zero_loss, label=\"Average\" )\n",
    "                artists += ax_array[1,0].plot(met_epochs[:till], np.array(epoch_q_dist_embs), label=ps)\n",
    "                artists += ax_array[1,1].plot(met_epochs[:till], np.array(epoch_q_norm_embs), label=ps)\n",
    "                ax_array[0,0].set_yscale('log')\n",
    "                for ax in ax_array.flatten():\n",
    "                    ax.set_prop_cycle(cycler('color', [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]))\n",
    "                    ax.legend()\n",
    "\n",
    "                if image_path is not None:\n",
    "                    plt.savefig(image_path)\n",
    "                    for artist in artists:\n",
    "                        artist.remove()\n",
    "                    artists=[]\n",
    "                else:\n",
    "                    display_IPython.clear_output(wait=True)\n",
    "                    plt.pause(0.001)\n",
    "                    plt.show()\n",
    "                    artists=[]\n",
    "        else:\n",
    "            av_loss = train_epoch(epoch, criterion, model, optimizer, train_loader, print_loss_every_batches,\n",
    "                optimizer_step_every_batches, per_epoch_use_max_train_batches)\n",
    "            epoch_av_loss.append(av_loss) # it is convenient to keep its track at every epoch to allow saving min loss models\n",
    "        \n",
    "        \n",
    "        if epoch and epoch_av_loss[-1] <= min_loss and best_model_path:\n",
    "            min_loss = epoch_av_loss[-1]\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                }, best_model_path)\n",
    "        \n",
    "                \n",
    "        if epoch % save_model_every_epochs==save_model_every_epochs-1 and model_path:\n",
    "            torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }, model_path)\n",
    "\n",
    "    return epoch_av_loss, epoch_av_non_zero_loss, epoch_q_loss, epoch_q_dist_embs, epoch_q_norm_embs, min_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset class and Data Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_5095394256657675342() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_5095394256657675342()\">Toggle show/hide next cell</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hide_toggle(for_next=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ImageDataloader(Dataset):\n",
    "    def __init__(self, GT_file_path, h5f_full_path, P, K, produce_batches_per_epoch):\n",
    "        self.df_GTs = pd.DataFrame.from_dict(json.load(open(GT_file_path))) \n",
    "        # Que te linkee el nombre de la entrada del h5f (un index del batch) con los ground truth phis\n",
    "\n",
    "        self.h5f = h5py.File(f\"{h5f_full_path}\", 'r')\n",
    "        self.num_diff_perfs = len(self.h5f)\n",
    "        shape = self.h5f[ str(self.df_GTs.iloc[0]['ID']) ].shape\n",
    "        self.max_K = shape[0]\n",
    "        self.image_size = shape[1:]\n",
    "        print(f\"There are {self.num_diff_perfs} total different classes (w0,R0,phiCR) triplets.\")\n",
    "        print(f\"For each of them, there are the denoised and {self.max_K-1} noisy versions\")\n",
    "        print(f\"Each image is of shape {self.image_size}\")\n",
    "        print(f\"A total of {self.num_diff_perfs*self.max_K} different images in the dataset, with their GT phiCRs\")\n",
    "        self.K = min(K, self.max_K) # number of versions of each class to employ per batch\n",
    "        self.P = P # number of different \"classes\" to employ per batch\n",
    "        self.PK = self.K*self.P\n",
    "        self.produce_batches_per_epoch = produce_batches_per_epoch\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.produce_batches_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # sudar olimplicamente del idx y simplemente devolver una combinación que tenga que ser aleatoria\n",
    "        random_indices = np.random.choice(range(self.num_diff_perfs), self.P, replace=False)\n",
    "        data = torch.zeros((self.P, self.K, self.image_size[0], self.image_size[1]), \n",
    "                          device=device, dtype=torch.float32) # [P,K,2X+1,2X+1]\n",
    "        #gt_phiCR = torch.zeros((self.P, 1), device=device, dtype=torch.float32) #[P,1]\n",
    "        for i,ind in enumerate(random_indices):\n",
    "             data[i,:,:,:] = torch.tensor( np.array(self.h5f[str(ind)]), device=device, dtype=torch.float32 ) \n",
    "            #gt_phiCR[i] = self.df_GTs['ID'==str(i)]['phiCR']\n",
    "        return data.reshape((self.PK, self.image_size[0], self.image_size[1])) #, gt_phiCR\n",
    "    # we return the data as [PK, 2X+1, 2X+1] where the first K in axis 0 belong to a same anchor P, next\n",
    "    # K to a different same anchor etc.\n",
    "    # solo devuelve las imagenes ya subidas a la gpu, luego fuera hay que hacer el cómputo de la métrica\n",
    "    # y hacer el pairwise todo!\n",
    "    \n",
    "    def get_im_and_label(self):\n",
    "        random_indices = np.random.choice(range(self.num_diff_perfs), self.P, replace=False)\n",
    "        data = torch.zeros((self.P, self.K, self.image_size[0], self.image_size[1]), \n",
    "                          device=device, dtype=torch.float32) # [P,K,2X+1,2X+1]\n",
    "        gt_phiCR = torch.zeros((self.P, 1), device=device, dtype=torch.float32) #[P,1]\n",
    "        for i,ind in enumerate(random_indices):\n",
    "            data[i,:,:,:] = torch.tensor( np.array(self.h5f[str(ind)]), device=device, dtype=torch.float32 ) \n",
    "            gt_phiCR[i] = float(self.df_GTs[self.df_GTs['ID']==ind]['phiCR'].item())\n",
    "        return data.reshape((self.PK, self.image_size[0], self.image_size[1])), gt_phiCR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Triplet Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batch_hard_losses_hard_margin():#torch.autograd.Function):\n",
    "    def __init__(self, m, P, K):\n",
    "        #super(batch_hard_losses_hard_margin, self).__init__()\n",
    "        self.margin = m\n",
    "        self.P=P\n",
    "        self.K=K\n",
    "        self.PK=P*K\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # we will NOT reuse the table of distances matrix, for the gradient to be computed correctly\n",
    "        #self.D = torch.zeros((self.PK, self.PK), device=device) #[PK, PK]\n",
    "        \n",
    "        # also the indices for upper triangular access\n",
    "        self.triu_indices = torch.triu_indices(row=self.PK, col=self.PK, offset=1) # indices for upper triangular\n",
    "\n",
    "        # Let us now build the indices for negative and positive selection\n",
    "        ones_block=torch.ones((K,K), dtype=torch.bool, device=device)\n",
    "        self.diagonal_KxK_blocks_are_one = torch.kron(torch.eye(P, dtype=torch.bool, device=device), ones_block)\n",
    "        #diagonal_KxK_blocks_are_zero = torch.logical_not(diagonal_KxK_blocks_are_one)\n",
    "\n",
    "    #@staticmethod\n",
    "    def forward(self, embeddings):\n",
    "        # Embeddings are expected to be in the format [PK,embedding_dim]\n",
    "        # where the K first belong to a same P, the next K to a different P etc.\n",
    "        # we can achieve this by reshaping [P,K,embedding_dim] to [PK, embedding_dim]\n",
    "        # We first need to get the [0.5*PK*(PK-1)] relative distances Only strictly necessary distances computed\n",
    "        distances = torch.pdist(embeddings, p=2)\n",
    "        \n",
    "        self.D = torch.zeros((self.PK, self.PK), device=device) #[PK, PK]\n",
    "        # We will build here the relative distances table\n",
    "        self.D[self.triu_indices[0], self.triu_indices[1]] = distances\n",
    "        self.D.T[self.triu_indices[0], self.triu_indices[1]] = distances\n",
    "        # Now D is the relative distances matrix (with zeros in diagonal)\n",
    "\n",
    "        \n",
    "        positives = torch.where(self.diagonal_KxK_blocks_are_one, \n",
    "                                self.D, torch.tensor([0.0], device=device))\n",
    "        negatives = torch.where(self.diagonal_KxK_blocks_are_one, \n",
    "                                torch.tensor([torch.inf], device=device), self.D)\n",
    "\n",
    "        hard_positives = torch.max(positives, axis=0)[0]\n",
    "        hard_negatives = torch.min(negatives, axis=0)[0]\n",
    "\n",
    "        return self.relu(self.margin+hard_positives-hard_negatives) #[PK] losses returned. Overall loss will be the sum\n",
    "\n",
    "    \n",
    "class batch_hard_losses_soft_margin():#torch.autograd.Function):\n",
    "    def __init__(self, P, K):\n",
    "        #super(batch_hard_losses_soft_margin, self).__init__()\n",
    "        self.P=P\n",
    "        self.K=K\n",
    "        self.PK=P*K\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        # we will NOT reuse the table of distances matrix for the gradients to be computed correctly\n",
    "        #self.D = torch.zeros((self.PK, self.PK), device=device) #[PK, PK]\n",
    "        \n",
    "        # also the indices for upper triangular access\n",
    "        self.triu_indices = torch.triu_indices(row=self.PK, col=self.PK, offset=1) # indices for upper triangular\n",
    "\n",
    "        # Let us now build the indices for negative and positive selection\n",
    "        ones_block=torch.ones((K,K), dtype=torch.bool, device=device)\n",
    "        self.diagonal_KxK_blocks_are_one = torch.kron(torch.eye(P, dtype=torch.bool, device=device), ones_block)\n",
    "        #diagonal_KxK_blocks_are_zero = torch.logical_not(diagonal_KxK_blocks_are_one)\n",
    "\n",
    "    #@staticmethod\n",
    "    def forward(self, embeddings):\n",
    "        # Embeddings are expected to be in the format [PK,embedding_dim]\n",
    "        # where the K first belong to a same P, the next K to a different P etc.\n",
    "        # we can achieve this by reshaping [P,K,embedding_dim] to [PK, embedding_dim]\n",
    "        # We first need to get the [0.5*PK*(PK-1)] relative distances Only strictly necessary distances computed\n",
    "        distances = torch.pdist(embeddings, p=2)\n",
    "        \n",
    "        # We will build here the relative distances table\n",
    "        self.D = torch.zeros((self.PK, self.PK), device=device) #[PK, PK]\n",
    "\n",
    "        self.D[self.triu_indices[0], self.triu_indices[1]] = distances\n",
    "        self.D.T[self.triu_indices[0], self.triu_indices[1]] = distances\n",
    "        # Now D is the relative distances matrix (with zeros in diagonal)\n",
    "\n",
    "        \n",
    "        positives = torch.where(self.diagonal_KxK_blocks_are_one, \n",
    "                                self.D, torch.tensor([0.0], device=device))\n",
    "        negatives = torch.where(self.diagonal_KxK_blocks_are_one, \n",
    "                                torch.tensor([torch.inf], device=device), self.D)\n",
    "\n",
    "        hard_positives = torch.max(positives, axis=0)[0]\n",
    "        hard_negatives = torch.min(negatives, axis=0)[0]\n",
    "\n",
    "        return self.softplus(hard_positives-hard_negatives) #[PK] losses returned (the overall loss will be the sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Initialize the dataset and sampler (choose the number of batches per epoch, and their length) and fix the artificial noise hyperparameters\n",
    "\n",
    "Note that since in each epoch the dataset shown to the model will be random, we can use the same dataset as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/Noisy_Non_Noisy_same_angle/\"\n",
    "GT_file_path_train = f\"{dataset_path}/TRAIN/GROUND_TRUTHS_K=4_Noisy_Non_Noisy_same_angle.json\"\n",
    "images_h5_path_train = f\"{dataset_path}/TRAIN/Dataset_K=4_Noisy_Non_Noisy_same_angle.h5\" \n",
    "GT_file_path_test = f\"{dataset_path}/TRAIN/GROUND_TRUTHS_K=4_Noisy_Non_Noisy_same_angle.json\"\n",
    "images_h5_path_test = f\"{dataset_path}/TRAIN/Dataset_K=4_Noisy_Non_Noisy_same_angle.h5\" \n",
    "\n",
    "save_stuff_path = f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/Proximity_Metric/\"\n",
    "\n",
    "total_epochs = 10000\n",
    "optimizer_step_every_batches = 2 #2\n",
    "per_epoch_use_max_train_batches= 6\n",
    "save_model_every_epochs = 1\n",
    "torch.manual_seed(625)\n",
    "compute_metrics_every_epochs=3\n",
    "plot_metrics_every_epochs=3\n",
    "\n",
    "percentiles=torch.tensor([0.1,0.5,0.9], device=device)\n",
    "\n",
    "K=4 #4\n",
    "P=20 #6\n",
    "margin=10\n",
    "\n",
    "#exp_name='Proximity_Metric_from_Image_Corrector'+'_Batch_Hard_Margin'\n",
    "#exp_name='Proximity_Metric_from_Simple_Encoder'+'_Batch_Hard_Margin'\n",
    "#exp_name='Proximity_Metric_from_Image_Corrector'+'_Batch_Hard_Soft_Margin'\n",
    "exp_name='Proximity_Metric_from_Simple_Encoder'+'_Batch_Hard_Soft_Margin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50000 total different classes (w0,R0,phiCR) triplets.\n",
      "For each of them, there are the denoised and 3 noisy versions\n",
      "Each image is of shape (605, 605)\n",
      "A total of 200000 different images in the dataset, with their GT phiCRs\n"
     ]
    }
   ],
   "source": [
    "training_data = ImageDataloader(GT_file_path_train, images_h5_path_train, P, K, per_epoch_use_max_train_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the Hyperparameters and Initialize the Model and the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENOISER BASED METRIC ############\n",
    "X=302\n",
    "S0=2*X+1\n",
    "S1=2*250+1\n",
    "S2=2*200+1\n",
    "S3=2*150+1\n",
    "S4=2*10+1\n",
    "S5=2*1+1\n",
    "S6=2\n",
    "feats_S1=5\n",
    "feats_S2=5\n",
    "feats_S3=10\n",
    "feats_S4=20\n",
    "feats_S5=20\n",
    "feats_S6=25\n",
    "out_fc1=100\n",
    "dropout_p=0.1\n",
    "\n",
    "# SIMPLE ENCODER TO phiCR BASED METRIC ###########\n",
    "X=302\n",
    "feats_1=20\n",
    "feats_2=20\n",
    "feats_3=20\n",
    "feats_4=5\n",
    "prop1=2.5\n",
    "prop2=1.5\n",
    "prop3=0.6\n",
    "av_pool1_div=2\n",
    "conv4_feat_size=8\n",
    "av_pool2_div=10\n",
    "out_fc_1=5\n",
    "dropout_p1=0.2\n",
    "dropout_p2=0.1\n",
    "\n",
    "\n",
    "# FOR BOTH, THE EMBEDDING SPACE DIMENSION WILL BE CHOSEN BY\n",
    "out_fc2=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters 12632460\n",
      "Initial shape torch.Size([3, 1, 605, 605])\n",
      "Post Conv1+relu shape torch.Size([3, 20, 303, 303])\n",
      "Post drop1+Conv2+relu+batchnorm shape torch.Size([3, 20, 182, 182])\n",
      "Post drop2+Conv3+relu shape torch.Size([3, 20, 74, 74])\n",
      "Post Av Pool1 shape torch.Size([3, 20, 38, 38])\n",
      "Post drop2+Conv4+batchnorm shape torch.Size([3, 5, 9, 9])\n",
      "Post Av. Pool2 shape torch.Size([3, 5, 2, 2])\n",
      "Post Pre-fc shape torch.Size([3, 20])\n",
      "Post fc1+relu+fc2 shape torch.Size([3, 10])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "model = Proximity_Metric_Based_On_Corrector(S0=S0, S1=S1, S2=S2, S3=S3, S4=S4, S5=S5, S6=S6,\n",
    "                 feats_S1=feats_S1, feats_S2=feats_S2, feats_S3=feats_S3, feats_S4=feats_S4,\n",
    "                 feats_S5=feats_S5, feats_S6=feats_S6,\n",
    "                 out_fc1=out_fc1, out_fc2=out_fc2,\n",
    "                 dropout_p=dropout_p ) \n",
    "\n",
    "'''\n",
    "model = Proximity_Metric_Based_On_Simple_Encoder( X=X, feats_1=feats_1, feats_2=feats_2, feats_3=feats_3, feats_4=feats_4,\n",
    "                 prop1=prop1, prop2=prop2, prop3=prop3, av_pool1_div=av_pool1_div, conv4_feat_size=conv4_feat_size, \n",
    "                av_pool2_div=av_pool2_div, \n",
    "                 out_fc_1=out_fc_1, out_fc_2=out_fc2,\n",
    "                 dropout_p1=dropout_p1, dropout_p2=dropout_p2 ) \n",
    "\n",
    "\n",
    "print(f\"Number of parameters {get_n_params(model)}\")\n",
    "\n",
    "# In case we wish to transfer the learned parameters of another run\n",
    "check_file=\"BEST_Model_and_Optimizer_2022-03-28 17:29:04.947290_Proximity_Metric_from_Simple_Encoder_Batch_Hard_Soft_Margin.pt\"\n",
    "checkpoint = torch.load(save_stuff_path+f\"/NNs/{check_file}\")\n",
    "#checkpoint = torch.load('/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/Simple_Encoder/'+f\"/NNs/{check_file}\")\n",
    "\n",
    "# move model to gpu if available\n",
    "model.to(device)\n",
    "model.print_shapes(batch_size=3)\n",
    "\n",
    "print(\"\\n\")\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "\n",
    "# Initialize the weights of the model! Default initialization might already be fine!\n",
    "\n",
    "# Triplet loss initialization\n",
    "#loss = batch_hard_losses_hard_margin(margin, P, K)\n",
    "loss = batch_hard_losses_soft_margin( P, K)\n",
    "\n",
    "# we will choose as optimizer the \n",
    "#optimizer = torch.optim.Adagrad(model.parameters(), lr=0.1, lr_decay=0.01, weight_decay=0.3,\n",
    "#                                initial_accumulator_value=0, eps=1e-10)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-9, betas=(0.3, 0.33), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "#optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [240/480 (50%)]\tLoss: 30.900654\n",
      "Train Epoch: 0 [480/480 (100%)]\tLoss: 22.163021\n",
      "Train Epoch: 1 [240/480 (50%)]\tLoss: 19.001680\n",
      "Train Epoch: 1 [480/480 (100%)]\tLoss: 6.659529\n",
      "Train Epoch: 2 [240/480 (50%)]\tLoss: 23.844250\n",
      "Train Epoch: 2 [480/480 (100%)]\tLoss: 29.745026\n",
      "Train Epoch: 3 [240/480 (50%)]\tLoss: 5.016692\n",
      "Train Epoch: 3 [480/480 (100%)]\tLoss: 23.297840\n",
      "Train Epoch: 4 [240/480 (50%)]\tLoss: 3.269733\n",
      "Train Epoch: 4 [480/480 (100%)]\tLoss: 17.443121\n",
      "Train Epoch: 5 [240/480 (50%)]\tLoss: 22.918030\n",
      "Train Epoch: 5 [480/480 (100%)]\tLoss: 7.273867\n",
      "Train Epoch: 6 [240/480 (50%)]\tLoss: 6.334849\n",
      "Train Epoch: 6 [480/480 (100%)]\tLoss: 10.497663\n",
      "Train Epoch: 7 [240/480 (50%)]\tLoss: 7.428959\n",
      "Train Epoch: 7 [480/480 (100%)]\tLoss: 9.890162\n",
      "Train Epoch: 8 [240/480 (50%)]\tLoss: 6.010443\n",
      "Train Epoch: 8 [480/480 (100%)]\tLoss: 5.530393\n",
      "Train Epoch: 9 [240/480 (50%)]\tLoss: 8.876498\n",
      "Train Epoch: 9 [480/480 (100%)]\tLoss: 20.477846\n",
      "Train Epoch: 10 [240/480 (50%)]\tLoss: 20.719780\n",
      "Train Epoch: 10 [480/480 (100%)]\tLoss: 13.727777\n",
      "Train Epoch: 11 [240/480 (50%)]\tLoss: 13.180855\n",
      "Train Epoch: 11 [480/480 (100%)]\tLoss: 15.701337\n",
      "Train Epoch: 12 [240/480 (50%)]\tLoss: 12.477985\n",
      "Train Epoch: 12 [480/480 (100%)]\tLoss: 21.673122\n",
      "Train Epoch: 13 [240/480 (50%)]\tLoss: 14.739003\n",
      "Train Epoch: 13 [480/480 (100%)]\tLoss: 9.041759\n",
      "Train Epoch: 14 [240/480 (50%)]\tLoss: 17.856373\n",
      "Train Epoch: 14 [480/480 (100%)]\tLoss: 6.141631\n",
      "Train Epoch: 15 [240/480 (50%)]\tLoss: 18.290709\n",
      "Train Epoch: 15 [480/480 (100%)]\tLoss: 4.660990\n",
      "Train Epoch: 16 [240/480 (50%)]\tLoss: 10.164022\n",
      "Train Epoch: 16 [480/480 (100%)]\tLoss: 6.491372\n",
      "Train Epoch: 17 [240/480 (50%)]\tLoss: 17.492020\n",
      "Train Epoch: 17 [480/480 (100%)]\tLoss: 18.998749\n",
      "Train Epoch: 18 [240/480 (50%)]\tLoss: 5.026200\n",
      "Train Epoch: 18 [480/480 (100%)]\tLoss: 6.611211\n",
      "Train Epoch: 19 [240/480 (50%)]\tLoss: 23.633190\n",
      "Train Epoch: 19 [480/480 (100%)]\tLoss: 10.470533\n",
      "Train Epoch: 20 [240/480 (50%)]\tLoss: 3.591997\n",
      "Train Epoch: 20 [480/480 (100%)]\tLoss: 6.002152\n",
      "Train Epoch: 21 [240/480 (50%)]\tLoss: 15.181727\n",
      "Train Epoch: 21 [480/480 (100%)]\tLoss: 11.573339\n",
      "Train Epoch: 22 [240/480 (50%)]\tLoss: 21.713886\n",
      "Train Epoch: 22 [480/480 (100%)]\tLoss: 25.362024\n",
      "Train Epoch: 23 [240/480 (50%)]\tLoss: 22.493444\n",
      "Train Epoch: 23 [480/480 (100%)]\tLoss: 9.995099\n",
      "Train Epoch: 24 [240/480 (50%)]\tLoss: 18.648483\n",
      "Train Epoch: 24 [480/480 (100%)]\tLoss: 6.122647\n",
      "Train Epoch: 25 [240/480 (50%)]\tLoss: 6.678713\n",
      "Train Epoch: 25 [480/480 (100%)]\tLoss: 4.503803\n",
      "Train Epoch: 26 [240/480 (50%)]\tLoss: 9.623961\n",
      "Train Epoch: 26 [480/480 (100%)]\tLoss: 4.498610\n",
      "Train Epoch: 27 [240/480 (50%)]\tLoss: 7.442983\n",
      "Train Epoch: 27 [480/480 (100%)]\tLoss: 13.428119\n",
      "Train Epoch: 28 [240/480 (50%)]\tLoss: 24.823734\n",
      "Train Epoch: 28 [480/480 (100%)]\tLoss: 15.205250\n",
      "Train Epoch: 29 [240/480 (50%)]\tLoss: 25.427151\n",
      "Train Epoch: 29 [480/480 (100%)]\tLoss: 22.750452\n",
      "Train Epoch: 30 [240/480 (50%)]\tLoss: 7.557096\n",
      "Train Epoch: 30 [480/480 (100%)]\tLoss: 12.602680\n",
      "Train Epoch: 31 [240/480 (50%)]\tLoss: 20.022291\n",
      "Train Epoch: 31 [480/480 (100%)]\tLoss: 11.979155\n",
      "Train Epoch: 32 [240/480 (50%)]\tLoss: 5.712117\n",
      "Train Epoch: 32 [480/480 (100%)]\tLoss: 12.034243\n",
      "Train Epoch: 33 [240/480 (50%)]\tLoss: 18.535000\n",
      "Train Epoch: 33 [480/480 (100%)]\tLoss: 14.042570\n",
      "Train Epoch: 34 [240/480 (50%)]\tLoss: 22.749178\n",
      "Train Epoch: 34 [480/480 (100%)]\tLoss: 13.170001\n",
      "Train Epoch: 35 [240/480 (50%)]\tLoss: 3.908292\n",
      "Train Epoch: 35 [480/480 (100%)]\tLoss: 19.917299\n",
      "Train Epoch: 36 [240/480 (50%)]\tLoss: 6.965665\n",
      "Train Epoch: 36 [480/480 (100%)]\tLoss: 20.619425\n",
      "Train Epoch: 37 [240/480 (50%)]\tLoss: 8.352341\n",
      "Train Epoch: 37 [480/480 (100%)]\tLoss: 6.757878\n",
      "Train Epoch: 38 [240/480 (50%)]\tLoss: 3.316098\n",
      "Train Epoch: 38 [480/480 (100%)]\tLoss: 15.326992\n",
      "Train Epoch: 39 [240/480 (50%)]\tLoss: 15.865641\n",
      "Train Epoch: 39 [480/480 (100%)]\tLoss: 13.584851\n",
      "Train Epoch: 40 [240/480 (50%)]\tLoss: 12.540758\n",
      "Train Epoch: 40 [480/480 (100%)]\tLoss: 5.631646\n",
      "Train Epoch: 41 [240/480 (50%)]\tLoss: 24.961239\n",
      "Train Epoch: 41 [480/480 (100%)]\tLoss: 14.694854\n",
      "Train Epoch: 42 [240/480 (50%)]\tLoss: 29.605585\n",
      "Train Epoch: 42 [480/480 (100%)]\tLoss: 45.935856\n",
      "Train Epoch: 43 [240/480 (50%)]\tLoss: 15.078322\n",
      "Train Epoch: 43 [480/480 (100%)]\tLoss: 12.978669\n",
      "Train Epoch: 44 [240/480 (50%)]\tLoss: 7.895719\n",
      "Train Epoch: 44 [480/480 (100%)]\tLoss: 15.321053\n",
      "Train Epoch: 45 [240/480 (50%)]\tLoss: 24.575172\n",
      "Train Epoch: 45 [480/480 (100%)]\tLoss: 6.353898\n",
      "Train Epoch: 46 [240/480 (50%)]\tLoss: 13.348223\n",
      "Train Epoch: 46 [480/480 (100%)]\tLoss: 12.371937\n",
      "Train Epoch: 47 [240/480 (50%)]\tLoss: 2.121755\n",
      "Train Epoch: 47 [480/480 (100%)]\tLoss: 7.989647\n",
      "Train Epoch: 48 [240/480 (50%)]\tLoss: 15.817029\n",
      "Train Epoch: 48 [480/480 (100%)]\tLoss: 4.710874\n",
      "Train Epoch: 49 [240/480 (50%)]\tLoss: 5.326127\n",
      "Train Epoch: 49 [480/480 (100%)]\tLoss: 8.435740\n",
      "Train Epoch: 50 [240/480 (50%)]\tLoss: 25.206589\n",
      "Train Epoch: 50 [480/480 (100%)]\tLoss: 11.800771\n",
      "Train Epoch: 51 [240/480 (50%)]\tLoss: 20.470230\n",
      "Train Epoch: 51 [480/480 (100%)]\tLoss: 2.573524\n",
      "Train Epoch: 52 [240/480 (50%)]\tLoss: 14.157174\n",
      "Train Epoch: 52 [480/480 (100%)]\tLoss: 6.556886\n",
      "Train Epoch: 53 [240/480 (50%)]\tLoss: 10.778465\n",
      "Train Epoch: 53 [480/480 (100%)]\tLoss: 33.896507\n",
      "Train Epoch: 54 [240/480 (50%)]\tLoss: 6.242878\n",
      "Train Epoch: 54 [480/480 (100%)]\tLoss: 9.690268\n",
      "Train Epoch: 55 [240/480 (50%)]\tLoss: 19.665581\n",
      "Train Epoch: 55 [480/480 (100%)]\tLoss: 20.045486\n",
      "Train Epoch: 56 [240/480 (50%)]\tLoss: 15.197690\n",
      "Train Epoch: 56 [480/480 (100%)]\tLoss: 6.741878\n",
      "Train Epoch: 57 [240/480 (50%)]\tLoss: 4.135893\n",
      "Train Epoch: 57 [480/480 (100%)]\tLoss: 15.128201\n",
      "Train Epoch: 58 [240/480 (50%)]\tLoss: 19.042568\n",
      "Train Epoch: 58 [480/480 (100%)]\tLoss: 1.483189\n",
      "Train Epoch: 59 [240/480 (50%)]\tLoss: 17.401020\n",
      "Train Epoch: 59 [480/480 (100%)]\tLoss: 8.441864\n",
      "Train Epoch: 60 [240/480 (50%)]\tLoss: 23.166737\n",
      "Train Epoch: 60 [480/480 (100%)]\tLoss: 12.943762\n",
      "Train Epoch: 61 [240/480 (50%)]\tLoss: 6.065200\n",
      "Train Epoch: 61 [480/480 (100%)]\tLoss: 3.633892\n",
      "Train Epoch: 62 [240/480 (50%)]\tLoss: 15.455395\n",
      "Train Epoch: 62 [480/480 (100%)]\tLoss: 9.042819\n",
      "Train Epoch: 63 [240/480 (50%)]\tLoss: 9.815978\n",
      "Train Epoch: 63 [480/480 (100%)]\tLoss: 32.043491\n",
      "Train Epoch: 64 [240/480 (50%)]\tLoss: 25.390953\n",
      "Train Epoch: 64 [480/480 (100%)]\tLoss: 18.978983\n",
      "Train Epoch: 65 [240/480 (50%)]\tLoss: 1.816743\n",
      "Train Epoch: 65 [480/480 (100%)]\tLoss: 9.653586\n",
      "Train Epoch: 66 [240/480 (50%)]\tLoss: 7.233028\n",
      "Train Epoch: 66 [480/480 (100%)]\tLoss: 3.755289\n",
      "Train Epoch: 67 [240/480 (50%)]\tLoss: 7.171152\n",
      "Train Epoch: 67 [480/480 (100%)]\tLoss: 29.210625\n",
      "Train Epoch: 68 [240/480 (50%)]\tLoss: 3.351708\n",
      "Train Epoch: 68 [480/480 (100%)]\tLoss: 17.667206\n",
      "Train Epoch: 69 [240/480 (50%)]\tLoss: 3.724792\n",
      "Train Epoch: 69 [480/480 (100%)]\tLoss: 1.551067\n",
      "Train Epoch: 70 [240/480 (50%)]\tLoss: 2.901492\n",
      "Train Epoch: 70 [480/480 (100%)]\tLoss: 23.545010\n",
      "Train Epoch: 71 [240/480 (50%)]\tLoss: 15.660251\n",
      "Train Epoch: 71 [480/480 (100%)]\tLoss: 18.302517\n",
      "Train Epoch: 72 [240/480 (50%)]\tLoss: 7.467704\n",
      "Train Epoch: 72 [480/480 (100%)]\tLoss: 12.790071\n",
      "Train Epoch: 73 [240/480 (50%)]\tLoss: 3.505075\n",
      "Train Epoch: 73 [480/480 (100%)]\tLoss: 9.331761\n",
      "Train Epoch: 74 [240/480 (50%)]\tLoss: 4.406782\n",
      "Train Epoch: 74 [480/480 (100%)]\tLoss: 24.936653\n",
      "Train Epoch: 75 [240/480 (50%)]\tLoss: 12.491351\n",
      "Train Epoch: 75 [480/480 (100%)]\tLoss: 8.304719\n",
      "Train Epoch: 76 [240/480 (50%)]\tLoss: 35.908905\n",
      "Train Epoch: 76 [480/480 (100%)]\tLoss: 3.633955\n",
      "Train Epoch: 77 [240/480 (50%)]\tLoss: 16.510862\n",
      "Train Epoch: 77 [480/480 (100%)]\tLoss: 8.285275\n",
      "Train Epoch: 78 [240/480 (50%)]\tLoss: 17.175356\n",
      "Train Epoch: 78 [480/480 (100%)]\tLoss: 6.463766\n",
      "Train Epoch: 79 [240/480 (50%)]\tLoss: 4.301723\n",
      "Train Epoch: 79 [480/480 (100%)]\tLoss: 16.686600\n",
      "Train Epoch: 80 [240/480 (50%)]\tLoss: 3.112872\n",
      "Train Epoch: 80 [480/480 (100%)]\tLoss: 29.310276\n",
      "Train Epoch: 81 [240/480 (50%)]\tLoss: 8.176906\n",
      "Train Epoch: 81 [480/480 (100%)]\tLoss: 1.200612\n",
      "Train Epoch: 82 [240/480 (50%)]\tLoss: 8.096237\n",
      "Train Epoch: 82 [480/480 (100%)]\tLoss: 5.838058\n",
      "Train Epoch: 83 [240/480 (50%)]\tLoss: 7.073902\n",
      "Train Epoch: 83 [480/480 (100%)]\tLoss: 18.892094\n",
      "Train Epoch: 84 [240/480 (50%)]\tLoss: 8.339582\n",
      "Train Epoch: 84 [480/480 (100%)]\tLoss: 6.950900\n",
      "Train Epoch: 85 [240/480 (50%)]\tLoss: 15.569816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 85 [480/480 (100%)]\tLoss: 22.817514\n",
      "Train Epoch: 86 [240/480 (50%)]\tLoss: 19.802544\n",
      "Train Epoch: 86 [480/480 (100%)]\tLoss: 8.191017\n",
      "Train Epoch: 87 [240/480 (50%)]\tLoss: 17.113619\n",
      "Train Epoch: 87 [480/480 (100%)]\tLoss: 15.229954\n",
      "Train Epoch: 88 [240/480 (50%)]\tLoss: 2.465526\n",
      "Train Epoch: 88 [480/480 (100%)]\tLoss: 18.224194\n",
      "Train Epoch: 89 [240/480 (50%)]\tLoss: 14.919038\n",
      "Train Epoch: 89 [480/480 (100%)]\tLoss: 5.710318\n",
      "Train Epoch: 90 [240/480 (50%)]\tLoss: 4.242762\n",
      "Train Epoch: 90 [480/480 (100%)]\tLoss: 17.534740\n",
      "Train Epoch: 91 [240/480 (50%)]\tLoss: 12.048629\n",
      "Train Epoch: 91 [480/480 (100%)]\tLoss: 20.391914\n",
      "Train Epoch: 92 [240/480 (50%)]\tLoss: 5.821526\n",
      "Train Epoch: 92 [480/480 (100%)]\tLoss: 4.698992\n",
      "Train Epoch: 93 [240/480 (50%)]\tLoss: 5.079392\n",
      "Train Epoch: 93 [480/480 (100%)]\tLoss: 5.059673\n",
      "Train Epoch: 94 [240/480 (50%)]\tLoss: 7.060460\n",
      "Train Epoch: 94 [480/480 (100%)]\tLoss: 14.540714\n",
      "Train Epoch: 95 [240/480 (50%)]\tLoss: 9.657681\n",
      "Train Epoch: 95 [480/480 (100%)]\tLoss: 10.363750\n",
      "Train Epoch: 96 [240/480 (50%)]\tLoss: 12.748757\n",
      "Train Epoch: 96 [480/480 (100%)]\tLoss: 22.583393\n",
      "Train Epoch: 97 [240/480 (50%)]\tLoss: 8.581527\n",
      "Train Epoch: 97 [480/480 (100%)]\tLoss: 5.052656\n",
      "Train Epoch: 98 [240/480 (50%)]\tLoss: 15.217214\n",
      "Train Epoch: 98 [480/480 (100%)]\tLoss: 2.431030\n",
      "Train Epoch: 99 [240/480 (50%)]\tLoss: 7.415709\n",
      "Train Epoch: 99 [480/480 (100%)]\tLoss: 9.513530\n",
      "Train Epoch: 100 [240/480 (50%)]\tLoss: 34.056507\n",
      "Train Epoch: 100 [480/480 (100%)]\tLoss: 9.920795\n",
      "Train Epoch: 101 [240/480 (50%)]\tLoss: 19.431704\n",
      "Train Epoch: 101 [480/480 (100%)]\tLoss: 17.833534\n",
      "Train Epoch: 102 [240/480 (50%)]\tLoss: 4.981732\n",
      "Train Epoch: 102 [480/480 (100%)]\tLoss: 24.710232\n",
      "Train Epoch: 103 [240/480 (50%)]\tLoss: 14.071011\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "losses = full_training_loop_monitor_partially(model, loss, optimizer, training_data, \n",
    "                    epochs=total_epochs, print_loss_every_batches=3,\n",
    "                            compute_metrics_every_epochs=compute_metrics_every_epochs,\n",
    "                            plot_metrics_every_epochs=plot_metrics_every_epochs,\n",
    "                           optimizer_step_every_batches=optimizer_step_every_batches,\n",
    "                           per_epoch_use_max_train_batches=per_epoch_use_max_train_batches, \n",
    "                           image_path=save_stuff_path+ f\"/Training_Loss_{datetime.now()}_{exp_name}.png\",\n",
    "                           save_model_every_epochs=save_model_every_epochs, \n",
    "                            model_path=save_stuff_path+f\"/Model_and_Optimizer_{datetime.now()}_{exp_name}.pt\",\n",
    "                            best_model_path=save_stuff_path+f\"/BEST_Model_and_Optimizer_{datetime.now()}_{exp_name}.pt\",\n",
    "                            percentiles=percentiles\n",
    "                           )\n",
    "# Execute the training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the resulting model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }, save_stuff_path+f\"/FINAL_Model_and_Optimizer_{datetime.now()}_{exp_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Validation\n",
    "For the validation of the Embedding, we could check the following ones:\n",
    "    \n",
    "$\\quad$   **(a)** Embedd some random samples and project them to 3D and 2D space using PCA and UMAP, to check the performance visually.\n",
    "   \n",
    "$\\quad$   **(b)** Use it in a simulation fit algorithm as a metric for the image fit.\n",
    "   \n",
    "$\\quad$   **(c)** Use it as a KNN distance metric on a huge library of GT angle images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_to_embedd = 100\n",
    "K_to_embedd = K\n",
    "\n",
    "test_data = ImageDataloader(GT_file_path_test, images_h5_path_test, 1, K_to_embedd, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(training_data[0].to('cpu')) # [KP, 2X+1, 2X+1] uint8 in device\n",
    "\n",
    "fig, ax_array = plt.subplots(P, K)\n",
    "for p in range(P):\n",
    "    ax_array[p][0].set_ylabel(f\"p={p}\")\n",
    "    for k in range(K):\n",
    "        ax_array[p][k].imshow(images[K*p+k])\n",
    "plt.setp(ax_array, xticks=[], yticks=[], frame_on=False)\n",
    "plt.tight_layout(h_pad=0.4, w_pad=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "from umap import UMAP\n",
    "X21=2*X+1\n",
    "# Get the Embeddings and the Images to plot\n",
    "model.eval()\n",
    "embeddings = np.zeros(( P_to_embedd*K_to_embedd, out_fc2), dtype=np.float32)\n",
    "\n",
    "# x,y are only required for the plotting within the 2D PCA and UMAP\n",
    "x = np.zeros( ( P_to_embedd*K_to_embedd, X21**2), dtype=np.uint8)\n",
    "y = np.zeros(( P_to_embedd*K_to_embedd), dtype=np.float32)\n",
    "\n",
    "\n",
    "for p in range(P_to_embedd):\n",
    "    images, label = test_data.get_im_and_label() # [ K_to_embedd, 2X+1, 2X+1]\n",
    "    embeddings[ K_to_embedd*p:(K_to_embedd*(p+1)), :] = np.array(model(images).detach().to('cpu')) # [ K_to_embedd, embedding_dim]\n",
    "    \n",
    "    x[K_to_embedd*p:(K_to_embedd*(p+1)), :] = np.array(images.to('cpu')).reshape(4, X21**2)\n",
    "    y[K_to_embedd*p:(K_to_embedd*(p+1))] = label.item()\n",
    "    print(f\"Embedded {p}\")\n",
    "\n",
    "df = pd.DataFrame({'y':y})\n",
    "df['phiCR'] = df[\"y\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If willing to use an already generated test visualization\n",
    "#df = pd.read_pickle(save_stuff_path+f\"/Test_Visualization_df_TalTal_{exp_name}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "principalComponents = pca.fit_transform(embeddings)\n",
    "# step optimizable!\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "df['principal component 1 2D'] = principalDf['principal component 1']\n",
    "df['principal component 2 2D'] = principalDf['principal component 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, LinearColorMapper\n",
    "from bokeh.palettes import Spectral10\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "def embeddable_image(data):\n",
    "    img_data = data.values.reshape(X21,X21)\n",
    "    image = Image.fromarray(img_data, mode='L').resize((64, 64), Image.BICUBIC)\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format='png')\n",
    "    for_encoding = buffer.getvalue()\n",
    "    return 'data:image/png;base64,' + base64.b64encode(for_encoding).decode()\n",
    "\n",
    "df['image'] = pd.DataFrame(data=x, columns=list(range(x.shape[1]))).apply(embeddable_image, axis=1)\n",
    "\n",
    "datasource = ColumnDataSource(df)\n",
    "color_mapping = LinearColorMapper(\n",
    "    palette='Magma256',\n",
    "    low=y.min(),\n",
    "    high=y.max()\n",
    ")\n",
    "\n",
    "plot_figure = figure(\n",
    "    title='PCA projection of the CR dataset',\n",
    "    plot_width=800,\n",
    "    plot_height=800,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    ")\n",
    "\n",
    "plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "    </div>\n",
    "    <div>\n",
    "        <span style='font-size: 16px; color: #224499'>phiCR:</span>\n",
    "        <span style='font-size: 18px'>@phiCR</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plot_figure.circle(\n",
    "    'principal component 1 2D',\n",
    "    'principal component 2 2D',\n",
    "    source=datasource,\n",
    "    color=dict(field='phiCR', transform=color_mapping),\n",
    "    line_alpha=0.6,\n",
    "    fill_alpha=0.6,\n",
    "    size=7\n",
    ")\n",
    "show(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "principalComponents = pca.fit_transform(embeddings)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "df['principal component 1 3D'] = principalDf['principal component 1']\n",
    "df['principal component 2 3D'] = principalDf['principal component 2']\n",
    "df['principal component 3 3D'] = principalDf['principal component 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter_3d(df, x=\"principal component 1 3D\", y=\"principal component 2 3D\", z=\"principal component 3 3D\", color=\"y\")\n",
    "fig.update_traces(marker={\"size\":3})\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_reducer = UMAP(n_components=2, min_dist=1, n_neighbors=200, metric='euclidean')\n",
    "principalComponents = umap_reducer.fit_transform(embeddings)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['umap_1', 'umap_2'])\n",
    "df['umap_1 2D'] = principalDf['umap_1']\n",
    "df['umap_2 2D'] = principalDf['umap_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, LinearColorMapper\n",
    "from bokeh.palettes import Spectral10\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "def embeddable_image(data):\n",
    "    img_data = data.values.reshape(X21,X21)\n",
    "    image = Image.fromarray(img_data, mode='L').resize((64, 64), Image.BICUBIC)\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format='png')\n",
    "    for_encoding = buffer.getvalue()\n",
    "    return 'data:image/png;base64,' + base64.b64encode(for_encoding).decode()\n",
    "\n",
    "df['image'] = pd.DataFrame(data=x, columns=list(range(x.shape[1]))).apply(embeddable_image, axis=1)\n",
    "\n",
    "\n",
    "datasource = ColumnDataSource(df)\n",
    "color_mapping = LinearColorMapper(\n",
    "    palette='Magma256',\n",
    "    low=y.min(),\n",
    "    high=y.max()\n",
    ")\n",
    "\n",
    "plot_figure = figure(\n",
    "    title='UMAP projection of the CR dataset',\n",
    "    plot_width=800,\n",
    "    plot_height=800,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    ")\n",
    "\n",
    "plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "    </div>\n",
    "    <div>\n",
    "        <span style='font-size: 16px; color: #224499'>phiCR:</span>\n",
    "        <span style='font-size: 18px'>@phiCR</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plot_figure.circle(\n",
    "    'umap_1 2D',\n",
    "    'umap_2 2D',\n",
    "    source=datasource,\n",
    "    color=dict(field='phiCR', transform=color_mapping),\n",
    "    line_alpha=0.6,\n",
    "    fill_alpha=0.6,\n",
    "    size=7\n",
    ")\n",
    "show(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap_reducer = UMAP(n_components=3, min_dist=1, n_neighbors=200, metric='euclidean') #euclidean, canberra, cosine, manhattan, braycurtis, mahalanobis, hamming\n",
    "principalComponents = umap_reducer.fit_transform(embeddings)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['umap_1', 'umap_2', 'umap_3'])\n",
    "df['umap_1 3D'] = principalDf['umap_1']\n",
    "df['umap_2 3D'] = principalDf['umap_2']\n",
    "df['umap_3 3D'] = principalDf['umap_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter_3d(df, x=\"umap_1 3D\", y=\"umap_2 3D\", z=\"umap_3 3D\", color=\"y\")\n",
    "fig.update_traces(marker={\"size\":3})\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the embedding visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(save_stuff_path+f\"/Test_Visualization_df_{datetime.now()}_{exp_name}.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Jungle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.ones((2, 5))\n",
    "b=3*a\n",
    "c=6*a\n",
    "d = torch.stack((a,b,c), axis=0)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = d.reshape(3*2, 5)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = torch.pdist(e, p=2)\n",
    "D = torch.zeros((3*2, 3*2))\n",
    "\n",
    "triu_indices = torch.triu_indices(row=3*2, col=3*2, offset=1)\n",
    "D[triu_indices[0], triu_indices[1]] = distances\n",
    "D.T[triu_indices[0], triu_indices[1]]= distances\n",
    "\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.array([1,2,3])\n",
    "c=np.array([3,4,7])\n",
    "np.linalg.norm(c-2*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3],[3,8,3],[3,4,7], [2,4,6]], dtype=torch.float32)\n",
    "d = torch.pdist(a, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.zeros((4, 4), device=device)\n",
    "\n",
    "triu_indices = torch.triu_indices(row=4, col=4, offset=1)\n",
    "print(triu_indices)\n",
    "m[triu_indices[0], triu_indices[1]] = d.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.T[triu_indices[0], triu_indices[1]]=d.to(device)\n",
    "#m.diagonal=torch.Tensor([1,1,1,1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = m+torch.diag(torch.tensor([float('inf')]*4, device=device))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.min(m))\n",
    "print(torch.min(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=2\n",
    "\n",
    "a=torch.ones((K,K), dtype=torch.bool, device=device)\n",
    "\n",
    "P=2\n",
    "diag_1s = torch.kron(torch.eye(P, dtype=torch.bool, device=device),a)\n",
    "diag_0s = torch.logical_not(diag_1s)\n",
    "print(diag_0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = torch.where(diag_1s, m, torch.tensor([0.0], device=device))\n",
    "negatives = torch.where(diag_1s, torch.tensor([torch.inf], device=device), m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_positives = torch.max(positives, axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_negatives = torch.min(negatives, axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marg=1.0\n",
    "r=nn.ReLU()\n",
    "torch.sum(r(marg+hard_positives-hard_negatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marg+hard_positives-hard_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[diag_1s] = 989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
