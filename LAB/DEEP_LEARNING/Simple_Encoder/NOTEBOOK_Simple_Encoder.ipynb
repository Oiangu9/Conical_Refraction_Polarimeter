{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Encoder CNN to get the $\\phi_{CR}$ parameter of a CR image\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #should be installed by default in any colab notebook\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from IPython import display as display_IPython\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
    "\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the functions and routines for the DL\n",
    "### Define the model and its constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Encoder(nn.Module):\n",
    "    def __init__(self, X=302, feats_1=15, feats_2=20, feats_3=20, feats_4=20,\n",
    "                 prop1=3, prop2=2, prop3=1, av_pool1_div=4, conv4_feat_size=15, av_pool2_div=10, \n",
    "                 out_fc_1=10,\n",
    "                 dropout_p1=0.2, dropout_p2=0.1\n",
    "                ): \n",
    "        # propj is such that the_ image getting out from stage j is propj/prop_{j-1}-ths of the previous (with j=0 being 5)\n",
    "        # clearly, prop_{j-1}>prop_{j}>...\n",
    "        # 2X+1 will be assumed to be divisible by 5\n",
    "        assert((2*X+1)%5==0)\n",
    "        assert(prop1>prop2)\n",
    "        assert(prop2>prop3)\n",
    "        assert((int((prop3*(2*X+1)/5)/av_pool1_div)-conv4_feat_size)>0)\n",
    "        \n",
    "        \n",
    "        super(Simple_Encoder, self).__init__()\n",
    "        # in is [epoch_size, 1, 2X+1, 2X+1]\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=feats_1, \n",
    "                               kernel_size = int((2*X+1)/5*(5-prop1)+1), bias=True) \n",
    "        # out conv1 [epoch_size, feats_1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "        self.conv2 = nn.Conv2d(in_channels=feats_1, out_channels=feats_2, \n",
    "                               kernel_size = int((2*X+1)/5*(prop1-prop2)+1), bias=True) \n",
    "        # out conv1 [epoch_size, feats_2, prop2*(prop1*(2X+1)/5)/prop1, prop2*(prop1*(2X+1)/5)/prop1]\n",
    "        # that is [epoch_size, feats_2, prop2*(2X+1)/5), prop2*(2X+1)/5)]\n",
    "        self.conv3 = nn.Conv2d(in_channels=feats_2, out_channels=feats_3, \n",
    "                               kernel_size = int((2*X+1)/5*(prop2-prop3)+1), bias=True)\n",
    "        # out conv3 is [epoch_size, feats_3, prop3*(2X+1)/5), prop3*(2X+1)/5)]\n",
    "\n",
    "        self.avPool1 = nn.AvgPool2d(kernel_size= int((prop3*(2*X+1)/5)*(1-1/av_pool1_div)) +1, stride=1)\n",
    "        # out avpool1 is [epoch_size, feats_3, prop3*(2X+1)/5)/av_pool1_div, prop3*(2X+1)/5)/av_pool1_div]\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=feats_3, out_channels=feats_4, \n",
    "                              kernel_size= int((prop3*(2*X+1)/5)/av_pool1_div+1)-conv4_feat_size+1, bias=True)\n",
    "        # [epoch_size, feats_4, conv4_feat_size, conv4_feat_size]\n",
    "        \n",
    "        self.avPool2 = nn.AvgPool2d(kernel_size= int(conv4_feat_size*(1-1/av_pool2_div)) +1, stride=1)\n",
    "        # out avpool1 is [epoch_size, feats_4, conv4_feat_size/av_pool2_div+1, conv4_feat_size/av_pool2_div+1]\n",
    "        \n",
    "        #self.in_fc = int(feats_4*(conv4_feat_size/av_pool2_div+1)**2)\n",
    "        self.in_fc = feats_4*((((((2*X+1-int((2*X+1)/5*(5-prop1)+1)+1)\n",
    "                                  -int((2*X+1)/5*(prop1-prop2)+1)+1)\n",
    "                                 -int((2*X+1)/5*(prop2-prop3)+1)+1)\n",
    "                                -int((prop3*(2*X+1)/5)*(1-1/av_pool1_div)) -1+1)\n",
    "                               -int((prop3*(2*X+1)/5)/av_pool1_div+1)+conv4_feat_size-1+1)\n",
    "                              -int(conv4_feat_size*(1-1/av_pool2_div)) -1+1)**2\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=self.in_fc, out_features=out_fc_1, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=out_fc_1, out_features=10, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.fc4 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.fc5 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.fc6 = nn.Linear(in_features=10, out_features=1)\n",
    "        \n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=dropout_p1, inplace=False)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p2, inplace=False)\n",
    "        self.relu = torch.nn.functional.leaky_relu\n",
    "\n",
    "        self.batchNorm2 = nn.BatchNorm2d(num_features=feats_2)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(num_features=feats_4)\n",
    "\n",
    "    def forward(self, x): # [batch_size, 2X+1, 2X+1] or [batch_size, 1, 2X+1, 2X+1]\n",
    "        x = x.view(x.shape[0], 1, x.shape[-2], x.shape[-1]).float() # [batch_size, 1, 2X+1, 2X+1]\n",
    "        # Normalize to unity the float image\n",
    "        x = x/x.amax(dim=(2,3), keepdim=True)[0] # [batch_size, 1, 2X+1, 2X+1]\n",
    "        \n",
    "        x = self.relu( self.conv1(x) ) # [batch_size, feats_1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "        \n",
    "        x = self.batchNorm2( self.relu( self.conv2(self.dropout1(x)) )) # [batch_size, feats_2, prop2*(2X+1)/5, prop2*(2X+1)/5]\n",
    "\n",
    "        \n",
    "        x = self.relu( self.conv3(self.dropout2(x)) ) # [batch_size, feats_3, prop3*(2X+1)/5, prop3*(2X+1)/5]\n",
    "\n",
    "        \n",
    "        x = self.avPool1(x) # [batch_size, feats_3, prop3*(2X+1)/5)/av_pool1_div, prop3*(2X+1)/5)/av_pool1_div]\n",
    "\n",
    "        \n",
    "        x = self.batchNorm4(self.conv4(self.dropout2(x))) # [batch_size, feats_4, conv4_feat_size, conv4_feat_size]\n",
    "\n",
    "        \n",
    "        x = self.relu( self.avPool2(x) ) # [batch_size, feats_4, conv4_feat_size/av_pool2_div, conv4_feat_size/av_pool2_div]\n",
    "\n",
    "        \n",
    "        x = x.view(x.shape[0], self.in_fc) #[batch_size, feats_4*int(conv4_feat_size/av_pool2_div)**2]\n",
    "\n",
    "        \n",
    "        x = self.fc6(self.relu(self.dropout1(self.fc5(self.relu(self.dropout1(self.fc4(self.relu(self.fc3(self.dropout1(self.relu(self.fc2( self.relu( self.fc1(x) ) )))))))))))) #[batch_size, 1]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def print_shapes(self, batch_size=10, X=302):\n",
    "        x = torch.ones((batch_size, 1, 2*X+1, 2*X+1)).to(device)\n",
    "        print(f\"Initial shape {x.shape}\")\n",
    "        x = self.relu( self.conv1(x) ) # [batch_size, feats_1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "        print(f\"Post Conv1+relu shape {x.shape}\")\n",
    "        x = self.batchNorm2( self.relu( self.conv2(self.dropout1(x)) )) # [batch_size, feats_2, prop2*(2X+1)/5, prop2*(2X+1)/5]\n",
    "        print(f\"Post drop1+Conv2+relu+batchnorm shape {x.shape}\")\n",
    "        \n",
    "        x = self.relu( self.conv3(self.dropout2(x)) ) # [batch_size, feats_3, prop3*(2X+1)/5, prop3*(2X+1)/5]\n",
    "        print(f\"Post drop2+Conv3+relu shape {x.shape}\")\n",
    "        \n",
    "        x = self.avPool1(x) # [batch_size, feats_3, prop3*(2X+1)/5)/av_pool1_div, prop3*(2X+1)/5)/av_pool1_div]\n",
    "        print(f\"Post Av Pool1 shape {x.shape}\")\n",
    "        \n",
    "        x = self.batchNorm4(self.conv4(self.dropout2(x))) # [batch_size, feats_4, conv4_feat_size, conv4_feat_size]\n",
    "        print(f\"Post drop2+Conv4+batchnorm shape {x.shape}\")\n",
    "        \n",
    "        x = self.relu( self.avPool2(x) ) # [batch_size, feats_4, conv4_feat_size/av_pool2_div, conv4_feat_size/av_pool2_div]\n",
    "        print(f\"Post Av. Pool2 shape {x.shape}\")\n",
    "        \n",
    "        x = x.view(x.shape[0], self.in_fc) #[batch_size, feats_4*int(conv4_feat_size/av_pool2_div)**2]\n",
    "        print(f\"Post Pre-fc shape {x.shape}\")\n",
    "        \n",
    "        x = self.fc2( self.relu( self.fc1(x) ) ) #[batch_size, 1]\n",
    "        print(f\"Post fc1+relu+fc2 shape {x.shape}\")\n",
    "        \n",
    "        x = self.fc4( self.relu( self.fc3(x) ) ) #[batch_size, 1]\n",
    "        print(f\"Post fc3+relu+fc4 shape {x.shape}\")\n",
    "        \n",
    "        x = self.fc6( self.relu( self.fc5(x) ) ) #[batch_size, 1]\n",
    "        print(f\"Post fc5+relu+fc6 shape {x.shape}\")\n",
    "\n",
    "    def load_my_state_dict(self, state_dict):\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if name not in own_state:\n",
    "                print(f\"Params NOT in own state: {name}\")\n",
    "                continue\n",
    "            if isinstance(param, nn.Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                param = param.data\n",
    "            print(f\"Params YES in own state: {name} shape on external {param.shape} shape on own {own_state[name].shape}\")\n",
    "            if param.shape==own_state[name].shape:\n",
    "                own_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subroutine to count number of parameters in the model\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.numel()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The routines to validate and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # prevent this function from computing gradients \n",
    "def validate_epoch(criterion, model, dataloader, per_epoch_use_max_batches=None): #show_confusion_matrix = False):\n",
    "    if per_epoch_use_max_batches is None:\n",
    "        per_epoch_use_max_batches = len(dataloader)\n",
    "    val_loss = 0\n",
    "    max_abs_error = torch.Tensor([0]).to(device)\n",
    "    mean_abs_error = 0\n",
    "    preds = torch.Tensor().to(device)\n",
    "    targets = torch.Tensor().to(device)\n",
    "\n",
    "    model.eval() # disable the dropout, among others\n",
    "\n",
    "    for batch_id, (data, target) in enumerate(dataloader):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)       \n",
    "        prediction = model(data)\n",
    "        target = target.view(prediction.shape)\n",
    "        loss = criterion(prediction, target)\n",
    "        val_loss += loss.item()                                                              \n",
    "        max_abs_error = torch.maximum(torch.max(torch.abs(prediction-target), 0).values, max_abs_error)\n",
    "        mean_abs_error += torch.sum(torch.abs(prediction-target), 0)\n",
    "        if batch_id % per_epoch_use_max_batches == per_epoch_use_max_batches-1:\n",
    "            break\n",
    "    val_loss /= min(len(dataloader), per_epoch_use_max_batches)\n",
    "    mean_abs_error /= min(len(dataloader), per_epoch_use_max_batches)\n",
    "    #accuracy = 100. * correct / len(loader.dataset)\n",
    "    print(f'\\nValidation set: Average loss: {val_loss:.4f}, Average Abs Error: {np.array(mean_abs_error.cpu())}, Maximum Abs Error: {np.array(max_abs_error.cpu())} \\n')\n",
    "\n",
    "    #if show_confusion_matrix:\n",
    "    #    visualize_confusion_matrix(preds.to(torch.device('cpu')), targets.to(torch.device('cpu')))\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train_epoch(epoch, criterion, model, optimizer, dataloader, print_loss_every_batches=20,\n",
    "                optimizer_step_every_batches=1, per_epoch_use_max_batches=None):\n",
    "    if per_epoch_use_max_batches is None:\n",
    "        per_epoch_use_max_batches = len(dataloader)\n",
    "        \n",
    "    total_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    #t = time()\n",
    "    for batch_id, (data, target) in enumerate(dataloader):        \n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        prediction = model(data) # data is [batch_size, 1, 2X+1, 2X+1]\n",
    "        loss = criterion(prediction, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        if batch_id % optimizer_step_every_batches==optimizer_step_every_batches-1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # print loss every N batches\n",
    "        if batch_id % print_loss_every_batches == print_loss_every_batches-1:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_id+1) * len(data), len(dataloader)*batch_size,\n",
    "                100*(batch_id+1)*len(data) / (len(dataloader)*batch_size), loss.item()))\n",
    "\n",
    "        if batch_id % per_epoch_use_max_batches == per_epoch_use_max_batches-1:\n",
    "            break\n",
    "        #t9=time()\n",
    "        #print(f\"it time {t9-t}\")\n",
    "\n",
    "        total_loss += loss.item()  #.item() is very important here\n",
    "        # In order to avoid having total_loss as a tensor in the gpu\n",
    "        #t = time()\n",
    "\n",
    "    return total_loss / min(len(dataloader), per_epoch_use_max_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_training_loop(model, criterion, optimizer, train_loader, test_loader, epochs=10,\n",
    "                       print_loss_every_batches=20, validate_every_epochs=2, optimizer_step_every_batches=1,\n",
    "                      per_epoch_use_max_train_batches=None, per_epoch_use_max_test_batches=None,\n",
    "                      image_path=None, save_model_every_epochs=1, model_path=None, best_model_path=None, scheduler=None):\n",
    "    losses = {\"train\": [], \"val\": []}\n",
    "    %matplotlib inline\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss = train_epoch(epoch, criterion, model, optimizer, train_loader,\n",
    "                                 print_loss_every_batches=print_loss_every_batches,\n",
    "                                optimizer_step_every_batches=optimizer_step_every_batches,\n",
    "                                per_epoch_use_max_batches=per_epoch_use_max_train_batches)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch%validate_every_epochs==0 and epoch!=0:\n",
    "            val_loss = validate_epoch(criterion, model, test_loader, per_epoch_use_max_test_batches)\n",
    "        else:\n",
    "            try:\n",
    "                val_loss = losses[\"val\"][-1]\n",
    "            except:\n",
    "                val_loss = train_loss\n",
    "        if epoch and train_loss<=min(losses[\"train\"]) and best_model_path:\n",
    "            torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "            }, best_model_path)\n",
    "        \n",
    "        losses[\"train\"].append(train_loss)\n",
    "        losses[\"val\"].append(val_loss)        \n",
    "        plt.plot(losses[\"train\"], label=\"training loss\")\n",
    "        plt.plot(losses[\"val\"], label=\"validation loss\")\n",
    "        #plt.yscale('log')\n",
    "        plt.legend()\n",
    "        if image_path is not None:\n",
    "            plt.savefig(image_path)\n",
    "            plt.clf()\n",
    "        else:\n",
    "            display_IPython.clear_output(wait=True)\n",
    "            plt.pause(0.001)\n",
    "            plt.show()\n",
    "        if epoch % save_model_every_epochs==save_model_every_epochs-1 and model_path:\n",
    "            torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "            }, model_path)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset class and Data Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, GT_file_path, images_dir_path):\n",
    "        self.df_GTs = pd.DataFrame.from_dict(json.load(open(GT_file_path)))\n",
    "        self.images_dir_path = images_dir_path\n",
    "        self.len_data = len(self.df_GTs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = f\"{self.images_dir_path}/IM_{self.df_GTs.iloc[idx,0]}_phiCR_{self.df_GTs.iloc[idx,1]}.png\"\n",
    "        image = read_image(img_path) #[1, 2X+1, 2X+1] torch tensor\n",
    "        label = torch.Tensor([float(self.df_GTs.iloc[idx, 1])]).type(torch.float32) #[1] torch tensor of float32\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Initialize the dataset and sampler (choose the number of batches per epoch, and their length) and fix the artificial noise hyperparameters\n",
    "\n",
    "Note that since in each epoch the dataset shown to the model will be random, we can use the same dataset as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GT_file_path_train = f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/IMAGE_LIBRARY/NON_NOISY/TRAIN/GROUND_TRUTHS.json\"\n",
    "#images_dir_path_train =f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/IMAGE_LIBRARY/NON_NOISY/TRAIN/\" \n",
    "#GT_file_path_test = f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/IMAGE_LIBRARY/NON_NOISY/TEST/GROUND_TRUTHS.json\"\n",
    "#images_dir_path_test = f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/IMAGE_LIBRARY/NON_NOISY/TEST/\"\n",
    "\n",
    "GT_file_path_train = f\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/OUTPUT/NOISY/TRAIN/GROUND_TRUTHS.json\"\n",
    "images_dir_path_train =f\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/OUTPUT/NOISY/TRAIN/\" \n",
    "GT_file_path_test = f\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/OUTPUT/NOISY/TEST/GROUND_TRUTHS.json\"\n",
    "images_dir_path_test =f\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/OUTPUT/NOISY/TEST/\" \n",
    "\n",
    "total_epochs = 100000\n",
    "batch_size = 10\n",
    "validate_every_epochs = 11000000\n",
    "optimizer_step_every_batches = 10\n",
    "per_epoch_use_max_train_batches= 30\n",
    "per_epoch_use_max_test_batches=20\n",
    "save_model_every_epochs = 1\n",
    "\n",
    "worker_num=5\n",
    "\n",
    "save_stuff_path = '/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/Simple_Encoder/'\n",
    "exp_name = 'Simple_Encoder'\n",
    "# 6 segundos por batch de 10 unidades, hacemos que pasen 40.000 batches (400.000 imagenes)\n",
    "# lo haremos de forma que se de una optimizer step cada 4 batches, en 10 epochs de \n",
    "# cada una 4000 batches (para plotear la curva) \n",
    "# y tras cada epoch haremos un save del modelo\n",
    "# deberian ser 2.8 dias de training\n",
    "\n",
    "# todo esto dividido por dos de forma que podemos entrenar simulatnaeamente ambos modelos? (con noisy y non-noisy data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = ImageDataset(GT_file_path_train, images_dir_path_train)\n",
    "test_data = ImageDataset(GT_file_path_test, images_dir_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(211)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=worker_num,\n",
    "                              pin_memory=True, drop_last=False, persistent_workers=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=worker_num, \n",
    "                             pin_memory=True, drop_last=False, persistent_workers=False)\n",
    "\n",
    "\n",
    "assert(len(train_dataloader)%batch_size==0 or per_epoch_use_max_train_batches%batch_size==0 ) # make batch_size an integer proportion of data files\n",
    "batch_number_per_epoch = min(len(train_dataloader)/batch_size, per_epoch_use_max_train_batches)\n",
    "assert(batch_number_per_epoch%optimizer_step_every_batches==0) # make optimizer steps every divisble number of its"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the Hyperparameters and Initialize the Model and the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=302\n",
    "feats_1=20\n",
    "feats_2=20\n",
    "feats_3=20\n",
    "feats_4=5\n",
    "prop1=2.5\n",
    "prop2=1.5\n",
    "prop3=0.6\n",
    "av_pool1_div=2\n",
    "conv4_feat_size=8\n",
    "av_pool2_div=10\n",
    "out_fc_1=5\n",
    "dropout_p1=0.2\n",
    "dropout_p2=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters 12632801\n",
      "Initial shape torch.Size([10, 1, 605, 605])\n",
      "Post Conv1+relu shape torch.Size([10, 20, 303, 303])\n",
      "Post drop1+Conv2+relu+batchnorm shape torch.Size([10, 20, 182, 182])\n",
      "Post drop2+Conv3+relu shape torch.Size([10, 20, 74, 74])\n",
      "Post Av Pool1 shape torch.Size([10, 20, 38, 38])\n",
      "Post drop2+Conv4+batchnorm shape torch.Size([10, 5, 9, 9])\n",
      "Post Av. Pool2 shape torch.Size([10, 5, 2, 2])\n",
      "Post Pre-fc shape torch.Size([10, 20])\n",
      "Post fc1+relu+fc2 shape torch.Size([10, 10])\n",
      "Post fc3+relu+fc4 shape torch.Size([10, 10])\n",
      "Post fc5+relu+fc6 shape torch.Size([10, 1])\n",
      "Params YES in own state: conv1.weight shape on external torch.Size([20, 1, 303, 303]) shape on own torch.Size([20, 1, 303, 303])\n",
      "Params YES in own state: conv1.bias shape on external torch.Size([20]) shape on own torch.Size([20])\n",
      "Params YES in own state: conv2.weight shape on external torch.Size([20, 20, 122, 122]) shape on own torch.Size([20, 20, 122, 122])\n",
      "Params YES in own state: conv2.bias shape on external torch.Size([20]) shape on own torch.Size([20])\n",
      "Params YES in own state: conv3.weight shape on external torch.Size([20, 20, 109, 109]) shape on own torch.Size([20, 20, 109, 109])\n",
      "Params YES in own state: conv3.bias shape on external torch.Size([20]) shape on own torch.Size([20])\n",
      "Params YES in own state: conv4.weight shape on external torch.Size([5, 20, 30, 30]) shape on own torch.Size([5, 20, 30, 30])\n",
      "Params YES in own state: conv4.bias shape on external torch.Size([5]) shape on own torch.Size([5])\n",
      "Params YES in own state: fc1.weight shape on external torch.Size([5, 20]) shape on own torch.Size([5, 20])\n",
      "Params YES in own state: fc1.bias shape on external torch.Size([5]) shape on own torch.Size([5])\n",
      "Params YES in own state: fc2.weight shape on external torch.Size([10, 5]) shape on own torch.Size([10, 5])\n",
      "Params YES in own state: fc2.bias shape on external torch.Size([10]) shape on own torch.Size([10])\n",
      "Params YES in own state: fc3.weight shape on external torch.Size([10, 10]) shape on own torch.Size([10, 10])\n",
      "Params YES in own state: fc3.bias shape on external torch.Size([10]) shape on own torch.Size([10])\n",
      "Params YES in own state: fc4.weight shape on external torch.Size([10, 10]) shape on own torch.Size([10, 10])\n",
      "Params YES in own state: fc4.bias shape on external torch.Size([10]) shape on own torch.Size([10])\n",
      "Params YES in own state: fc5.weight shape on external torch.Size([10, 10]) shape on own torch.Size([10, 10])\n",
      "Params YES in own state: fc5.bias shape on external torch.Size([10]) shape on own torch.Size([10])\n",
      "Params YES in own state: fc6.weight shape on external torch.Size([1, 10]) shape on own torch.Size([1, 10])\n",
      "Params YES in own state: fc6.bias shape on external torch.Size([1]) shape on own torch.Size([1])\n",
      "Params YES in own state: batchNorm2.weight shape on external torch.Size([20]) shape on own torch.Size([20])\n",
      "Params YES in own state: batchNorm2.bias shape on external torch.Size([20]) shape on own torch.Size([20])\n",
      "Params YES in own state: batchNorm2.running_mean shape on external torch.Size([20]) shape on own torch.Size([20])\n",
      "Params YES in own state: batchNorm2.running_var shape on external torch.Size([20]) shape on own torch.Size([20])\n",
      "Params YES in own state: batchNorm2.num_batches_tracked shape on external torch.Size([]) shape on own torch.Size([])\n",
      "Params YES in own state: batchNorm4.weight shape on external torch.Size([5]) shape on own torch.Size([5])\n",
      "Params YES in own state: batchNorm4.bias shape on external torch.Size([5]) shape on own torch.Size([5])\n",
      "Params YES in own state: batchNorm4.running_mean shape on external torch.Size([5]) shape on own torch.Size([5])\n",
      "Params YES in own state: batchNorm4.running_var shape on external torch.Size([5]) shape on own torch.Size([5])\n",
      "Params YES in own state: batchNorm4.num_batches_tracked shape on external torch.Size([]) shape on own torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "model = Simple_Encoder( X=X, feats_1=feats_1, feats_2=feats_2, feats_3=feats_3, feats_4=feats_4,\n",
    "                 prop1=prop1, prop2=prop2, prop3=prop3, av_pool1_div=av_pool1_div, conv4_feat_size=conv4_feat_size, \n",
    "                av_pool2_div=av_pool2_div, \n",
    "                 out_fc_1=out_fc_1,\n",
    "                 dropout_p1=dropout_p1, dropout_p2=dropout_p2 ) \n",
    "\n",
    "print(f\"Number of parameters {get_n_params(model)}\")\n",
    "\n",
    "# In case we wish to transfer the learned parameters of another run\n",
    "#check_file = \"NNs/BEST_Noisy_Model_and_Optimizer_2022-03-01 18:08:09.693062_Simple_Encoder.pt\"\n",
    "input_path = \"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/Simple_Encoder/\"\n",
    "check_file = \"BEST_Noisy_Model_and_Optimizer_2022-06-15 21:21:52.251901_Simple_Encoder.pt\"\n",
    "checkpoint = torch.load(input_path+f\"/{check_file}\")\n",
    "\n",
    "# move model to gpu if available\n",
    "model.to(device)\n",
    "model.print_shapes()\n",
    "\n",
    "model.load_my_state_dict(checkpoint['model'])\n",
    "\n",
    "\n",
    "# Initialize the weights of the model! Default initialization might already be fine!\n",
    "\n",
    "# we can use a MSE loss for the regression task we have in hands\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# we will choose as optimizer the \n",
    "#optimizer = torch.optim.Adagrad(model.parameters(), lr=0.1, lr_decay=0.01, weight_decay=0.3,\n",
    "#                                initial_accumulator_value=0, eps=1e-10)\n",
    "#check_file=\"NNs/Noisy_Model_and_Optimizer_2022-02-22 20:58:41.632916_Simple_Encoder.pt\"\n",
    "#checkpoint = torch.load(save_stuff_path+f\"/{check_file}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.99, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "#optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# we will perform learning rate scheduling at this phase\n",
    "lmbda = lambda epoch: 0.7\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
    "#scheduler.load_state_dict(checkpoint['scheduler'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [100/500000 (0%)]\tLoss: 1.357592\n",
      "Train Epoch: 0 [200/500000 (0%)]\tLoss: 3.206345\n",
      "Train Epoch: 0 [300/500000 (0%)]\tLoss: 0.946850\n",
      "Train Epoch: 1 [100/500000 (0%)]\tLoss: 2.239209\n",
      "Train Epoch: 1 [200/500000 (0%)]\tLoss: 1.775655\n",
      "Train Epoch: 1 [300/500000 (0%)]\tLoss: 0.482742\n",
      "Train Epoch: 2 [100/500000 (0%)]\tLoss: 3.229578\n",
      "Train Epoch: 2 [200/500000 (0%)]\tLoss: 2.605185\n",
      "Train Epoch: 2 [300/500000 (0%)]\tLoss: 3.311521\n",
      "Train Epoch: 3 [100/500000 (0%)]\tLoss: 1.375441\n",
      "Train Epoch: 3 [200/500000 (0%)]\tLoss: 1.665823\n",
      "Train Epoch: 3 [300/500000 (0%)]\tLoss: 1.823316\n",
      "Train Epoch: 4 [100/500000 (0%)]\tLoss: 1.893983\n",
      "Train Epoch: 4 [200/500000 (0%)]\tLoss: 1.701872\n",
      "Train Epoch: 4 [300/500000 (0%)]\tLoss: 1.931157\n",
      "Train Epoch: 5 [100/500000 (0%)]\tLoss: 1.922323\n",
      "Train Epoch: 5 [200/500000 (0%)]\tLoss: 2.298433\n",
      "Train Epoch: 5 [300/500000 (0%)]\tLoss: 1.147761\n",
      "Train Epoch: 6 [100/500000 (0%)]\tLoss: 0.840182\n",
      "Train Epoch: 6 [200/500000 (0%)]\tLoss: 1.039836\n",
      "Train Epoch: 6 [300/500000 (0%)]\tLoss: 1.212553\n",
      "Train Epoch: 7 [100/500000 (0%)]\tLoss: 2.808861\n",
      "Train Epoch: 7 [200/500000 (0%)]\tLoss: 1.768302\n",
      "Train Epoch: 7 [300/500000 (0%)]\tLoss: 1.378033\n",
      "Train Epoch: 8 [100/500000 (0%)]\tLoss: 1.837760\n",
      "Train Epoch: 8 [200/500000 (0%)]\tLoss: 1.307860\n",
      "Train Epoch: 8 [300/500000 (0%)]\tLoss: 0.646572\n",
      "Train Epoch: 9 [100/500000 (0%)]\tLoss: 2.044987\n",
      "Train Epoch: 9 [200/500000 (0%)]\tLoss: 1.915240\n",
      "Train Epoch: 9 [300/500000 (0%)]\tLoss: 2.298893\n",
      "Train Epoch: 10 [100/500000 (0%)]\tLoss: 1.425865\n",
      "Train Epoch: 10 [200/500000 (0%)]\tLoss: 2.129378\n",
      "Train Epoch: 10 [300/500000 (0%)]\tLoss: 1.116278\n",
      "Train Epoch: 11 [100/500000 (0%)]\tLoss: 2.937945\n",
      "Train Epoch: 11 [200/500000 (0%)]\tLoss: 1.174767\n",
      "Train Epoch: 11 [300/500000 (0%)]\tLoss: 1.522442\n",
      "Train Epoch: 12 [100/500000 (0%)]\tLoss: 2.812722\n",
      "Train Epoch: 12 [200/500000 (0%)]\tLoss: 2.113304\n",
      "Train Epoch: 12 [300/500000 (0%)]\tLoss: 1.870319\n",
      "Train Epoch: 13 [100/500000 (0%)]\tLoss: 2.056253\n",
      "Train Epoch: 13 [200/500000 (0%)]\tLoss: 2.033685\n",
      "Train Epoch: 13 [300/500000 (0%)]\tLoss: 2.655399\n",
      "Train Epoch: 14 [100/500000 (0%)]\tLoss: 1.743055\n",
      "Train Epoch: 14 [200/500000 (0%)]\tLoss: 2.377607\n",
      "Train Epoch: 14 [300/500000 (0%)]\tLoss: 1.842999\n",
      "Train Epoch: 15 [100/500000 (0%)]\tLoss: 1.626154\n",
      "Train Epoch: 15 [200/500000 (0%)]\tLoss: 1.368537\n",
      "Train Epoch: 15 [300/500000 (0%)]\tLoss: 1.291176\n",
      "Train Epoch: 16 [100/500000 (0%)]\tLoss: 0.906872\n",
      "Train Epoch: 16 [200/500000 (0%)]\tLoss: 0.686308\n",
      "Train Epoch: 16 [300/500000 (0%)]\tLoss: 1.527354\n",
      "Train Epoch: 17 [100/500000 (0%)]\tLoss: 1.923504\n",
      "Train Epoch: 17 [200/500000 (0%)]\tLoss: 2.742684\n",
      "Train Epoch: 17 [300/500000 (0%)]\tLoss: 1.794194\n",
      "Train Epoch: 18 [100/500000 (0%)]\tLoss: 0.272967\n",
      "Train Epoch: 18 [200/500000 (0%)]\tLoss: 2.471097\n",
      "Train Epoch: 18 [300/500000 (0%)]\tLoss: 1.991001\n",
      "Train Epoch: 19 [100/500000 (0%)]\tLoss: 3.154303\n",
      "Train Epoch: 19 [200/500000 (0%)]\tLoss: 2.826158\n",
      "Train Epoch: 19 [300/500000 (0%)]\tLoss: 1.632053\n",
      "Train Epoch: 20 [100/500000 (0%)]\tLoss: 2.152315\n",
      "Train Epoch: 20 [200/500000 (0%)]\tLoss: 1.031876\n",
      "Train Epoch: 20 [300/500000 (0%)]\tLoss: 1.656281\n",
      "Train Epoch: 21 [100/500000 (0%)]\tLoss: 1.323246\n",
      "Train Epoch: 21 [200/500000 (0%)]\tLoss: 0.939455\n",
      "Train Epoch: 21 [300/500000 (0%)]\tLoss: 1.700165\n",
      "Train Epoch: 22 [100/500000 (0%)]\tLoss: 0.693465\n",
      "Train Epoch: 22 [200/500000 (0%)]\tLoss: 2.567114\n",
      "Train Epoch: 22 [300/500000 (0%)]\tLoss: 0.753445\n",
      "Train Epoch: 23 [100/500000 (0%)]\tLoss: 0.962701\n",
      "Train Epoch: 23 [200/500000 (0%)]\tLoss: 1.200267\n",
      "Train Epoch: 23 [300/500000 (0%)]\tLoss: 1.716960\n",
      "Train Epoch: 24 [100/500000 (0%)]\tLoss: 2.419395\n",
      "Train Epoch: 24 [200/500000 (0%)]\tLoss: 0.946360\n",
      "Train Epoch: 24 [300/500000 (0%)]\tLoss: 2.014457\n",
      "Train Epoch: 25 [100/500000 (0%)]\tLoss: 1.556036\n",
      "Train Epoch: 25 [200/500000 (0%)]\tLoss: 1.274762\n",
      "Train Epoch: 25 [300/500000 (0%)]\tLoss: 2.104646\n",
      "Train Epoch: 26 [100/500000 (0%)]\tLoss: 1.946315\n",
      "Train Epoch: 26 [200/500000 (0%)]\tLoss: 2.555714\n",
      "Train Epoch: 26 [300/500000 (0%)]\tLoss: 1.340317\n",
      "Train Epoch: 27 [100/500000 (0%)]\tLoss: 1.386711\n",
      "Train Epoch: 27 [200/500000 (0%)]\tLoss: 0.802997\n",
      "Train Epoch: 27 [300/500000 (0%)]\tLoss: 2.700835\n",
      "Train Epoch: 28 [100/500000 (0%)]\tLoss: 1.110482\n",
      "Train Epoch: 28 [200/500000 (0%)]\tLoss: 3.028906\n",
      "Train Epoch: 28 [300/500000 (0%)]\tLoss: 0.753438\n",
      "Train Epoch: 29 [100/500000 (0%)]\tLoss: 2.044214\n",
      "Train Epoch: 29 [200/500000 (0%)]\tLoss: 3.694763\n",
      "Train Epoch: 29 [300/500000 (0%)]\tLoss: 1.214413\n",
      "Train Epoch: 30 [100/500000 (0%)]\tLoss: 0.877797\n",
      "Train Epoch: 30 [200/500000 (0%)]\tLoss: 0.733320\n",
      "Train Epoch: 30 [300/500000 (0%)]\tLoss: 1.240753\n",
      "Train Epoch: 31 [100/500000 (0%)]\tLoss: 1.438666\n",
      "Train Epoch: 31 [200/500000 (0%)]\tLoss: 2.316565\n",
      "Train Epoch: 31 [300/500000 (0%)]\tLoss: 1.207652\n",
      "Train Epoch: 32 [100/500000 (0%)]\tLoss: 1.824702\n",
      "Train Epoch: 32 [200/500000 (0%)]\tLoss: 2.016689\n",
      "Train Epoch: 32 [300/500000 (0%)]\tLoss: 1.390279\n",
      "Train Epoch: 33 [100/500000 (0%)]\tLoss: 3.971999\n",
      "Train Epoch: 33 [200/500000 (0%)]\tLoss: 2.729506\n",
      "Train Epoch: 33 [300/500000 (0%)]\tLoss: 0.867732\n",
      "Train Epoch: 34 [100/500000 (0%)]\tLoss: 3.207355\n",
      "Train Epoch: 34 [200/500000 (0%)]\tLoss: 0.260516\n",
      "Train Epoch: 34 [300/500000 (0%)]\tLoss: 3.760931\n",
      "Train Epoch: 35 [100/500000 (0%)]\tLoss: 3.732311\n",
      "Train Epoch: 35 [200/500000 (0%)]\tLoss: 2.029242\n",
      "Train Epoch: 35 [300/500000 (0%)]\tLoss: 2.467613\n",
      "Train Epoch: 36 [100/500000 (0%)]\tLoss: 2.766593\n",
      "Train Epoch: 36 [200/500000 (0%)]\tLoss: 1.370589\n",
      "Train Epoch: 36 [300/500000 (0%)]\tLoss: 2.427594\n",
      "Train Epoch: 37 [100/500000 (0%)]\tLoss: 1.379300\n",
      "Train Epoch: 37 [200/500000 (0%)]\tLoss: 1.367152\n",
      "Train Epoch: 37 [300/500000 (0%)]\tLoss: 0.769124\n",
      "Train Epoch: 38 [100/500000 (0%)]\tLoss: 0.889775\n",
      "Train Epoch: 38 [200/500000 (0%)]\tLoss: 1.971856\n",
      "Train Epoch: 38 [300/500000 (0%)]\tLoss: 1.023674\n",
      "Train Epoch: 39 [100/500000 (0%)]\tLoss: 1.747144\n",
      "Train Epoch: 39 [200/500000 (0%)]\tLoss: 0.788695\n",
      "Train Epoch: 39 [300/500000 (0%)]\tLoss: 1.591367\n",
      "Train Epoch: 40 [100/500000 (0%)]\tLoss: 3.217886\n",
      "Train Epoch: 40 [200/500000 (0%)]\tLoss: 2.586393\n",
      "Train Epoch: 40 [300/500000 (0%)]\tLoss: 2.903877\n",
      "Train Epoch: 41 [100/500000 (0%)]\tLoss: 1.265341\n",
      "Train Epoch: 41 [200/500000 (0%)]\tLoss: 1.007995\n",
      "Train Epoch: 41 [300/500000 (0%)]\tLoss: 2.206522\n",
      "Train Epoch: 42 [100/500000 (0%)]\tLoss: 0.787280\n",
      "Train Epoch: 42 [200/500000 (0%)]\tLoss: 1.850373\n",
      "Train Epoch: 42 [300/500000 (0%)]\tLoss: 1.396623\n",
      "Train Epoch: 43 [100/500000 (0%)]\tLoss: 1.869035\n",
      "Train Epoch: 43 [200/500000 (0%)]\tLoss: 1.210304\n",
      "Train Epoch: 43 [300/500000 (0%)]\tLoss: 2.624624\n",
      "Train Epoch: 44 [100/500000 (0%)]\tLoss: 2.009974\n",
      "Train Epoch: 44 [200/500000 (0%)]\tLoss: 1.472639\n",
      "Train Epoch: 44 [300/500000 (0%)]\tLoss: 2.716889\n",
      "Train Epoch: 45 [100/500000 (0%)]\tLoss: 0.313849\n",
      "Train Epoch: 45 [200/500000 (0%)]\tLoss: 1.551825\n",
      "Train Epoch: 45 [300/500000 (0%)]\tLoss: 4.942903\n",
      "Train Epoch: 46 [100/500000 (0%)]\tLoss: 1.206959\n",
      "Train Epoch: 46 [200/500000 (0%)]\tLoss: 1.019199\n",
      "Train Epoch: 46 [300/500000 (0%)]\tLoss: 1.682974\n",
      "Train Epoch: 47 [100/500000 (0%)]\tLoss: 3.169479\n",
      "Train Epoch: 47 [200/500000 (0%)]\tLoss: 1.475466\n",
      "Train Epoch: 47 [300/500000 (0%)]\tLoss: 1.008142\n",
      "Train Epoch: 48 [100/500000 (0%)]\tLoss: 0.396371\n",
      "Train Epoch: 48 [200/500000 (0%)]\tLoss: 0.718542\n",
      "Train Epoch: 48 [300/500000 (0%)]\tLoss: 1.264193\n",
      "Train Epoch: 49 [100/500000 (0%)]\tLoss: 1.401594\n",
      "Train Epoch: 49 [200/500000 (0%)]\tLoss: 2.174226\n",
      "Train Epoch: 49 [300/500000 (0%)]\tLoss: 2.904826\n",
      "Train Epoch: 50 [100/500000 (0%)]\tLoss: 3.050442\n",
      "Train Epoch: 50 [200/500000 (0%)]\tLoss: 0.865722\n",
      "Train Epoch: 50 [300/500000 (0%)]\tLoss: 1.277369\n",
      "Train Epoch: 51 [100/500000 (0%)]\tLoss: 1.594891\n",
      "Train Epoch: 51 [200/500000 (0%)]\tLoss: 2.043686\n",
      "Train Epoch: 51 [300/500000 (0%)]\tLoss: 1.457303\n",
      "Train Epoch: 52 [100/500000 (0%)]\tLoss: 2.491594\n",
      "Train Epoch: 52 [200/500000 (0%)]\tLoss: 1.413551\n",
      "Train Epoch: 52 [300/500000 (0%)]\tLoss: 0.643872\n",
      "Train Epoch: 53 [100/500000 (0%)]\tLoss: 2.311724\n",
      "Train Epoch: 53 [200/500000 (0%)]\tLoss: 1.196325\n",
      "Train Epoch: 53 [300/500000 (0%)]\tLoss: 2.110508\n",
      "Train Epoch: 54 [100/500000 (0%)]\tLoss: 3.246439\n",
      "Train Epoch: 54 [200/500000 (0%)]\tLoss: 0.522853\n",
      "Train Epoch: 54 [300/500000 (0%)]\tLoss: 1.245275\n",
      "Train Epoch: 55 [100/500000 (0%)]\tLoss: 1.887674\n",
      "Train Epoch: 55 [200/500000 (0%)]\tLoss: 2.496910\n",
      "Train Epoch: 55 [300/500000 (0%)]\tLoss: 3.657432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 56 [100/500000 (0%)]\tLoss: 1.150874\n",
      "Train Epoch: 56 [200/500000 (0%)]\tLoss: 1.767504\n",
      "Train Epoch: 56 [300/500000 (0%)]\tLoss: 1.214169\n",
      "Train Epoch: 57 [100/500000 (0%)]\tLoss: 0.292935\n",
      "Train Epoch: 57 [200/500000 (0%)]\tLoss: 1.003236\n",
      "Train Epoch: 57 [300/500000 (0%)]\tLoss: 2.603117\n",
      "Train Epoch: 58 [100/500000 (0%)]\tLoss: 1.214593\n",
      "Train Epoch: 58 [200/500000 (0%)]\tLoss: 1.292982\n",
      "Train Epoch: 58 [300/500000 (0%)]\tLoss: 3.311053\n",
      "Train Epoch: 59 [100/500000 (0%)]\tLoss: 1.135336\n",
      "Train Epoch: 59 [200/500000 (0%)]\tLoss: 1.364320\n",
      "Train Epoch: 59 [300/500000 (0%)]\tLoss: 1.005600\n",
      "Train Epoch: 60 [100/500000 (0%)]\tLoss: 1.803591\n",
      "Train Epoch: 60 [200/500000 (0%)]\tLoss: 1.573347\n",
      "Train Epoch: 60 [300/500000 (0%)]\tLoss: 0.950828\n",
      "Train Epoch: 61 [100/500000 (0%)]\tLoss: 1.425338\n",
      "Train Epoch: 61 [200/500000 (0%)]\tLoss: 1.379806\n",
      "Train Epoch: 61 [300/500000 (0%)]\tLoss: 1.783750\n",
      "Train Epoch: 62 [100/500000 (0%)]\tLoss: 2.000505\n",
      "Train Epoch: 62 [200/500000 (0%)]\tLoss: 1.787033\n",
      "Train Epoch: 62 [300/500000 (0%)]\tLoss: 1.406247\n",
      "Train Epoch: 63 [100/500000 (0%)]\tLoss: 1.304410\n",
      "Train Epoch: 63 [200/500000 (0%)]\tLoss: 2.584842\n",
      "Train Epoch: 63 [300/500000 (0%)]\tLoss: 1.687514\n",
      "Train Epoch: 64 [100/500000 (0%)]\tLoss: 0.553181\n",
      "Train Epoch: 64 [200/500000 (0%)]\tLoss: 2.324893\n",
      "Train Epoch: 64 [300/500000 (0%)]\tLoss: 2.135692\n",
      "Train Epoch: 65 [100/500000 (0%)]\tLoss: 2.996736\n",
      "Train Epoch: 65 [200/500000 (0%)]\tLoss: 1.821996\n",
      "Train Epoch: 65 [300/500000 (0%)]\tLoss: 1.242076\n",
      "Train Epoch: 66 [100/500000 (0%)]\tLoss: 1.231366\n",
      "Train Epoch: 66 [200/500000 (0%)]\tLoss: 1.874805\n",
      "Train Epoch: 66 [300/500000 (0%)]\tLoss: 2.913490\n",
      "Train Epoch: 67 [100/500000 (0%)]\tLoss: 1.660919\n",
      "Train Epoch: 67 [200/500000 (0%)]\tLoss: 1.938111\n",
      "Train Epoch: 67 [300/500000 (0%)]\tLoss: 2.400881\n",
      "Train Epoch: 68 [100/500000 (0%)]\tLoss: 1.727702\n",
      "Train Epoch: 68 [200/500000 (0%)]\tLoss: 0.446474\n",
      "Train Epoch: 68 [300/500000 (0%)]\tLoss: 1.043991\n",
      "Train Epoch: 69 [100/500000 (0%)]\tLoss: 0.780694\n",
      "Train Epoch: 69 [200/500000 (0%)]\tLoss: 1.698784\n",
      "Train Epoch: 69 [300/500000 (0%)]\tLoss: 2.723653\n",
      "Train Epoch: 70 [100/500000 (0%)]\tLoss: 1.516379\n",
      "Train Epoch: 70 [200/500000 (0%)]\tLoss: 0.917799\n",
      "Train Epoch: 70 [300/500000 (0%)]\tLoss: 3.050277\n",
      "Train Epoch: 71 [100/500000 (0%)]\tLoss: 0.811352\n",
      "Train Epoch: 71 [200/500000 (0%)]\tLoss: 2.413933\n",
      "Train Epoch: 71 [300/500000 (0%)]\tLoss: 5.503826\n",
      "Train Epoch: 72 [100/500000 (0%)]\tLoss: 2.353017\n",
      "Train Epoch: 72 [200/500000 (0%)]\tLoss: 0.910920\n",
      "Train Epoch: 72 [300/500000 (0%)]\tLoss: 1.689863\n",
      "Train Epoch: 73 [100/500000 (0%)]\tLoss: 1.918329\n",
      "Train Epoch: 73 [200/500000 (0%)]\tLoss: 2.419432\n",
      "Train Epoch: 73 [300/500000 (0%)]\tLoss: 2.094043\n",
      "Train Epoch: 74 [100/500000 (0%)]\tLoss: 1.942010\n",
      "Train Epoch: 74 [200/500000 (0%)]\tLoss: 3.810605\n",
      "Train Epoch: 74 [300/500000 (0%)]\tLoss: 1.357915\n",
      "Train Epoch: 75 [100/500000 (0%)]\tLoss: 1.966862\n",
      "Train Epoch: 75 [200/500000 (0%)]\tLoss: 0.789745\n",
      "Train Epoch: 75 [300/500000 (0%)]\tLoss: 1.353204\n",
      "Train Epoch: 76 [100/500000 (0%)]\tLoss: 1.361290\n",
      "Train Epoch: 76 [200/500000 (0%)]\tLoss: 1.326209\n",
      "Train Epoch: 76 [300/500000 (0%)]\tLoss: 3.463272\n",
      "Train Epoch: 77 [100/500000 (0%)]\tLoss: 2.803599\n",
      "Train Epoch: 77 [200/500000 (0%)]\tLoss: 1.467001\n",
      "Train Epoch: 77 [300/500000 (0%)]\tLoss: 1.257781\n",
      "Train Epoch: 78 [100/500000 (0%)]\tLoss: 2.057935\n",
      "Train Epoch: 78 [200/500000 (0%)]\tLoss: 0.976727\n",
      "Train Epoch: 78 [300/500000 (0%)]\tLoss: 2.032610\n",
      "Train Epoch: 79 [100/500000 (0%)]\tLoss: 0.881064\n",
      "Train Epoch: 79 [200/500000 (0%)]\tLoss: 0.790784\n",
      "Train Epoch: 79 [300/500000 (0%)]\tLoss: 1.139590\n",
      "Train Epoch: 80 [100/500000 (0%)]\tLoss: 0.575566\n",
      "Train Epoch: 80 [200/500000 (0%)]\tLoss: 1.761942\n",
      "Train Epoch: 80 [300/500000 (0%)]\tLoss: 1.646839\n",
      "Train Epoch: 81 [100/500000 (0%)]\tLoss: 3.298839\n",
      "Train Epoch: 81 [200/500000 (0%)]\tLoss: 1.403250\n",
      "Train Epoch: 81 [300/500000 (0%)]\tLoss: 1.326063\n",
      "Train Epoch: 82 [100/500000 (0%)]\tLoss: 1.751021\n",
      "Train Epoch: 82 [200/500000 (0%)]\tLoss: 2.515891\n",
      "Train Epoch: 82 [300/500000 (0%)]\tLoss: 0.320523\n",
      "Train Epoch: 83 [100/500000 (0%)]\tLoss: 1.351028\n",
      "Train Epoch: 83 [200/500000 (0%)]\tLoss: 1.332885\n",
      "Train Epoch: 83 [300/500000 (0%)]\tLoss: 1.571358\n",
      "Train Epoch: 84 [100/500000 (0%)]\tLoss: 1.462373\n",
      "Train Epoch: 84 [200/500000 (0%)]\tLoss: 3.474090\n",
      "Train Epoch: 84 [300/500000 (0%)]\tLoss: 2.459283\n",
      "Train Epoch: 85 [100/500000 (0%)]\tLoss: 1.651321\n",
      "Train Epoch: 85 [200/500000 (0%)]\tLoss: 0.747619\n",
      "Train Epoch: 85 [300/500000 (0%)]\tLoss: 2.196095\n",
      "Train Epoch: 86 [100/500000 (0%)]\tLoss: 2.460334\n",
      "Train Epoch: 86 [200/500000 (0%)]\tLoss: 1.101986\n",
      "Train Epoch: 86 [300/500000 (0%)]\tLoss: 2.399408\n",
      "Train Epoch: 87 [100/500000 (0%)]\tLoss: 0.900166\n",
      "Train Epoch: 87 [200/500000 (0%)]\tLoss: 3.394331\n",
      "Train Epoch: 87 [300/500000 (0%)]\tLoss: 3.101332\n",
      "Train Epoch: 88 [100/500000 (0%)]\tLoss: 1.595722\n",
      "Train Epoch: 88 [200/500000 (0%)]\tLoss: 1.939575\n",
      "Train Epoch: 88 [300/500000 (0%)]\tLoss: 1.452015\n",
      "Train Epoch: 89 [100/500000 (0%)]\tLoss: 1.894052\n",
      "Train Epoch: 89 [200/500000 (0%)]\tLoss: 1.440738\n",
      "Train Epoch: 89 [300/500000 (0%)]\tLoss: 1.034269\n",
      "Train Epoch: 90 [100/500000 (0%)]\tLoss: 1.463762\n",
      "Train Epoch: 90 [200/500000 (0%)]\tLoss: 0.960578\n",
      "Train Epoch: 90 [300/500000 (0%)]\tLoss: 1.208229\n",
      "Train Epoch: 91 [100/500000 (0%)]\tLoss: 2.898913\n",
      "Train Epoch: 91 [200/500000 (0%)]\tLoss: 1.443735\n",
      "Train Epoch: 91 [300/500000 (0%)]\tLoss: 1.935033\n",
      "Train Epoch: 92 [100/500000 (0%)]\tLoss: 2.237150\n",
      "Train Epoch: 92 [200/500000 (0%)]\tLoss: 2.714618\n",
      "Train Epoch: 92 [300/500000 (0%)]\tLoss: 2.604136\n",
      "Train Epoch: 93 [100/500000 (0%)]\tLoss: 2.975349\n",
      "Train Epoch: 93 [200/500000 (0%)]\tLoss: 2.078957\n",
      "Train Epoch: 93 [300/500000 (0%)]\tLoss: 2.316241\n",
      "Train Epoch: 94 [100/500000 (0%)]\tLoss: 0.989924\n",
      "Train Epoch: 94 [200/500000 (0%)]\tLoss: 1.272120\n",
      "Train Epoch: 94 [300/500000 (0%)]\tLoss: 0.761447\n",
      "Train Epoch: 95 [100/500000 (0%)]\tLoss: 1.129887\n",
      "Train Epoch: 95 [200/500000 (0%)]\tLoss: 1.250574\n",
      "Train Epoch: 95 [300/500000 (0%)]\tLoss: 0.874377\n",
      "Train Epoch: 96 [100/500000 (0%)]\tLoss: 1.863160\n",
      "Train Epoch: 96 [200/500000 (0%)]\tLoss: 1.350568\n",
      "Train Epoch: 96 [300/500000 (0%)]\tLoss: 0.982486\n",
      "Train Epoch: 97 [100/500000 (0%)]\tLoss: 0.583706\n",
      "Train Epoch: 97 [200/500000 (0%)]\tLoss: 1.523898\n",
      "Train Epoch: 97 [300/500000 (0%)]\tLoss: 1.101073\n",
      "Train Epoch: 98 [100/500000 (0%)]\tLoss: 1.087689\n",
      "Train Epoch: 98 [200/500000 (0%)]\tLoss: 3.548863\n",
      "Train Epoch: 98 [300/500000 (0%)]\tLoss: 2.869913\n",
      "Train Epoch: 99 [100/500000 (0%)]\tLoss: 0.453746\n",
      "Train Epoch: 99 [200/500000 (0%)]\tLoss: 0.775947\n",
      "Train Epoch: 99 [300/500000 (0%)]\tLoss: 1.676665\n",
      "Train Epoch: 100 [100/500000 (0%)]\tLoss: 2.314305\n",
      "Train Epoch: 100 [200/500000 (0%)]\tLoss: 1.102860\n",
      "Train Epoch: 100 [300/500000 (0%)]\tLoss: 0.730466\n",
      "Train Epoch: 101 [100/500000 (0%)]\tLoss: 0.922016\n",
      "Train Epoch: 101 [200/500000 (0%)]\tLoss: 1.610907\n",
      "Train Epoch: 101 [300/500000 (0%)]\tLoss: 1.873178\n",
      "Train Epoch: 102 [100/500000 (0%)]\tLoss: 2.653948\n",
      "Train Epoch: 102 [200/500000 (0%)]\tLoss: 1.985377\n",
      "Train Epoch: 102 [300/500000 (0%)]\tLoss: 3.549527\n",
      "Train Epoch: 103 [100/500000 (0%)]\tLoss: 2.197147\n",
      "Train Epoch: 103 [200/500000 (0%)]\tLoss: 2.357270\n",
      "Train Epoch: 103 [300/500000 (0%)]\tLoss: 1.228838\n",
      "Train Epoch: 104 [100/500000 (0%)]\tLoss: 1.135175\n",
      "Train Epoch: 104 [200/500000 (0%)]\tLoss: 1.076433\n",
      "Train Epoch: 104 [300/500000 (0%)]\tLoss: 2.127740\n",
      "Train Epoch: 105 [100/500000 (0%)]\tLoss: 3.177670\n",
      "Train Epoch: 105 [200/500000 (0%)]\tLoss: 1.925897\n",
      "Train Epoch: 105 [300/500000 (0%)]\tLoss: 0.875511\n",
      "Train Epoch: 106 [100/500000 (0%)]\tLoss: 2.262120\n",
      "Train Epoch: 106 [200/500000 (0%)]\tLoss: 0.374898\n",
      "Train Epoch: 106 [300/500000 (0%)]\tLoss: 1.380770\n",
      "Train Epoch: 107 [100/500000 (0%)]\tLoss: 1.687714\n",
      "Train Epoch: 107 [200/500000 (0%)]\tLoss: 1.296143\n",
      "Train Epoch: 107 [300/500000 (0%)]\tLoss: 1.592497\n",
      "Train Epoch: 108 [100/500000 (0%)]\tLoss: 1.743598\n",
      "Train Epoch: 108 [200/500000 (0%)]\tLoss: 0.976108\n",
      "Train Epoch: 108 [300/500000 (0%)]\tLoss: 2.415577\n",
      "Train Epoch: 109 [100/500000 (0%)]\tLoss: 0.598608\n",
      "Train Epoch: 109 [200/500000 (0%)]\tLoss: 2.209874\n",
      "Train Epoch: 109 [300/500000 (0%)]\tLoss: 3.110216\n",
      "Train Epoch: 110 [100/500000 (0%)]\tLoss: 2.379449\n",
      "Train Epoch: 110 [200/500000 (0%)]\tLoss: 0.920696\n",
      "Train Epoch: 110 [300/500000 (0%)]\tLoss: 1.803353\n",
      "Train Epoch: 111 [100/500000 (0%)]\tLoss: 0.706611\n",
      "Train Epoch: 111 [200/500000 (0%)]\tLoss: 1.825133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 111 [300/500000 (0%)]\tLoss: 4.118855\n",
      "Train Epoch: 112 [100/500000 (0%)]\tLoss: 1.258179\n",
      "Train Epoch: 112 [200/500000 (0%)]\tLoss: 2.224427\n",
      "Train Epoch: 112 [300/500000 (0%)]\tLoss: 3.756156\n",
      "Train Epoch: 113 [100/500000 (0%)]\tLoss: 1.643981\n",
      "Train Epoch: 113 [200/500000 (0%)]\tLoss: 2.644030\n",
      "Train Epoch: 113 [300/500000 (0%)]\tLoss: 0.592080\n",
      "Train Epoch: 114 [100/500000 (0%)]\tLoss: 1.604908\n",
      "Train Epoch: 114 [200/500000 (0%)]\tLoss: 1.870565\n",
      "Train Epoch: 114 [300/500000 (0%)]\tLoss: 1.865474\n",
      "Train Epoch: 115 [100/500000 (0%)]\tLoss: 1.782297\n",
      "Train Epoch: 115 [200/500000 (0%)]\tLoss: 1.666469\n",
      "Train Epoch: 115 [300/500000 (0%)]\tLoss: 2.271550\n",
      "Train Epoch: 116 [100/500000 (0%)]\tLoss: 2.660149\n",
      "Train Epoch: 116 [200/500000 (0%)]\tLoss: 3.353587\n",
      "Train Epoch: 116 [300/500000 (0%)]\tLoss: 0.537111\n",
      "Train Epoch: 117 [100/500000 (0%)]\tLoss: 2.111864\n",
      "Train Epoch: 117 [200/500000 (0%)]\tLoss: 1.127614\n",
      "Train Epoch: 117 [300/500000 (0%)]\tLoss: 2.043292\n",
      "Train Epoch: 118 [100/500000 (0%)]\tLoss: 1.853668\n",
      "Train Epoch: 118 [200/500000 (0%)]\tLoss: 3.003067\n",
      "Train Epoch: 118 [300/500000 (0%)]\tLoss: 2.708506\n",
      "Train Epoch: 119 [100/500000 (0%)]\tLoss: 4.105047\n",
      "Train Epoch: 119 [200/500000 (0%)]\tLoss: 0.858807\n",
      "Train Epoch: 119 [300/500000 (0%)]\tLoss: 2.720846\n",
      "Train Epoch: 120 [100/500000 (0%)]\tLoss: 1.968691\n",
      "Train Epoch: 120 [200/500000 (0%)]\tLoss: 1.598198\n",
      "Train Epoch: 120 [300/500000 (0%)]\tLoss: 0.683828\n",
      "Train Epoch: 121 [100/500000 (0%)]\tLoss: 1.280820\n",
      "Train Epoch: 121 [200/500000 (0%)]\tLoss: 0.937558\n",
      "Train Epoch: 121 [300/500000 (0%)]\tLoss: 0.570374\n",
      "Train Epoch: 122 [100/500000 (0%)]\tLoss: 2.214957\n",
      "Train Epoch: 122 [200/500000 (0%)]\tLoss: 1.619567\n",
      "Train Epoch: 122 [300/500000 (0%)]\tLoss: 2.714402\n",
      "Train Epoch: 123 [100/500000 (0%)]\tLoss: 1.447279\n",
      "Train Epoch: 123 [200/500000 (0%)]\tLoss: 2.514861\n",
      "Train Epoch: 123 [300/500000 (0%)]\tLoss: 1.211460\n",
      "Train Epoch: 124 [100/500000 (0%)]\tLoss: 1.789190\n",
      "Train Epoch: 124 [200/500000 (0%)]\tLoss: 0.873487\n",
      "Train Epoch: 124 [300/500000 (0%)]\tLoss: 0.945727\n",
      "Train Epoch: 125 [100/500000 (0%)]\tLoss: 1.472392\n",
      "Train Epoch: 125 [200/500000 (0%)]\tLoss: 0.486164\n",
      "Train Epoch: 125 [300/500000 (0%)]\tLoss: 1.713206\n",
      "Train Epoch: 126 [100/500000 (0%)]\tLoss: 2.966724\n",
      "Train Epoch: 126 [200/500000 (0%)]\tLoss: 1.595118\n",
      "Train Epoch: 126 [300/500000 (0%)]\tLoss: 2.953805\n",
      "Train Epoch: 127 [100/500000 (0%)]\tLoss: 1.559616\n",
      "Train Epoch: 127 [200/500000 (0%)]\tLoss: 1.272306\n",
      "Train Epoch: 127 [300/500000 (0%)]\tLoss: 0.884959\n",
      "Train Epoch: 128 [100/500000 (0%)]\tLoss: 0.465761\n",
      "Train Epoch: 128 [200/500000 (0%)]\tLoss: 0.987682\n",
      "Train Epoch: 128 [300/500000 (0%)]\tLoss: 1.161955\n",
      "Train Epoch: 129 [100/500000 (0%)]\tLoss: 1.106360\n",
      "Train Epoch: 129 [200/500000 (0%)]\tLoss: 0.926113\n",
      "Train Epoch: 129 [300/500000 (0%)]\tLoss: 2.166160\n",
      "Train Epoch: 130 [100/500000 (0%)]\tLoss: 1.301383\n",
      "Train Epoch: 130 [200/500000 (0%)]\tLoss: 1.007200\n",
      "Train Epoch: 130 [300/500000 (0%)]\tLoss: 0.840571\n",
      "Train Epoch: 131 [100/500000 (0%)]\tLoss: 2.332879\n",
      "Train Epoch: 131 [200/500000 (0%)]\tLoss: 2.199459\n",
      "Train Epoch: 131 [300/500000 (0%)]\tLoss: 2.357115\n",
      "Train Epoch: 132 [100/500000 (0%)]\tLoss: 1.715164\n",
      "Train Epoch: 132 [200/500000 (0%)]\tLoss: 2.893107\n",
      "Train Epoch: 132 [300/500000 (0%)]\tLoss: 2.712084\n",
      "Train Epoch: 133 [100/500000 (0%)]\tLoss: 2.927813\n",
      "Train Epoch: 133 [200/500000 (0%)]\tLoss: 2.150265\n",
      "Train Epoch: 133 [300/500000 (0%)]\tLoss: 3.004158\n",
      "Train Epoch: 134 [100/500000 (0%)]\tLoss: 0.809874\n",
      "Train Epoch: 134 [200/500000 (0%)]\tLoss: 1.534701\n",
      "Train Epoch: 134 [300/500000 (0%)]\tLoss: 1.720668\n",
      "Train Epoch: 135 [100/500000 (0%)]\tLoss: 1.690281\n",
      "Train Epoch: 135 [200/500000 (0%)]\tLoss: 0.871353\n",
      "Train Epoch: 135 [300/500000 (0%)]\tLoss: 1.651197\n",
      "Train Epoch: 136 [100/500000 (0%)]\tLoss: 1.422686\n",
      "Train Epoch: 136 [200/500000 (0%)]\tLoss: 3.257535\n",
      "Train Epoch: 136 [300/500000 (0%)]\tLoss: 2.173511\n",
      "Train Epoch: 137 [100/500000 (0%)]\tLoss: 3.072109\n",
      "Train Epoch: 137 [200/500000 (0%)]\tLoss: 1.939862\n",
      "Train Epoch: 137 [300/500000 (0%)]\tLoss: 1.452126\n",
      "Train Epoch: 138 [100/500000 (0%)]\tLoss: 2.397304\n",
      "Train Epoch: 138 [200/500000 (0%)]\tLoss: 1.455850\n",
      "Train Epoch: 138 [300/500000 (0%)]\tLoss: 2.057488\n",
      "Train Epoch: 139 [100/500000 (0%)]\tLoss: 3.217763\n",
      "Train Epoch: 139 [200/500000 (0%)]\tLoss: 1.715775\n",
      "Train Epoch: 139 [300/500000 (0%)]\tLoss: 2.592772\n",
      "Train Epoch: 140 [100/500000 (0%)]\tLoss: 0.187828\n",
      "Train Epoch: 140 [200/500000 (0%)]\tLoss: 2.653262\n",
      "Train Epoch: 140 [300/500000 (0%)]\tLoss: 1.049163\n",
      "Train Epoch: 141 [100/500000 (0%)]\tLoss: 2.068292\n",
      "Train Epoch: 141 [200/500000 (0%)]\tLoss: 2.860663\n",
      "Train Epoch: 141 [300/500000 (0%)]\tLoss: 1.984910\n",
      "Train Epoch: 142 [100/500000 (0%)]\tLoss: 0.572640\n",
      "Train Epoch: 142 [200/500000 (0%)]\tLoss: 1.930993\n",
      "Train Epoch: 142 [300/500000 (0%)]\tLoss: 2.060711\n",
      "Train Epoch: 143 [100/500000 (0%)]\tLoss: 2.361271\n",
      "Train Epoch: 143 [200/500000 (0%)]\tLoss: 1.777300\n",
      "Train Epoch: 143 [300/500000 (0%)]\tLoss: 0.883896\n",
      "Train Epoch: 144 [100/500000 (0%)]\tLoss: 2.897938\n",
      "Train Epoch: 144 [200/500000 (0%)]\tLoss: 1.548333\n",
      "Train Epoch: 144 [300/500000 (0%)]\tLoss: 0.723544\n",
      "Train Epoch: 145 [100/500000 (0%)]\tLoss: 2.554904\n",
      "Train Epoch: 145 [200/500000 (0%)]\tLoss: 2.033179\n",
      "Train Epoch: 145 [300/500000 (0%)]\tLoss: 1.261581\n",
      "Train Epoch: 146 [100/500000 (0%)]\tLoss: 1.358027\n",
      "Train Epoch: 146 [200/500000 (0%)]\tLoss: 3.899340\n",
      "Train Epoch: 146 [300/500000 (0%)]\tLoss: 1.355563\n",
      "Train Epoch: 147 [100/500000 (0%)]\tLoss: 1.821634\n",
      "Train Epoch: 147 [200/500000 (0%)]\tLoss: 3.735613\n",
      "Train Epoch: 147 [300/500000 (0%)]\tLoss: 2.222789\n",
      "Train Epoch: 148 [100/500000 (0%)]\tLoss: 0.994650\n",
      "Train Epoch: 148 [200/500000 (0%)]\tLoss: 2.095792\n",
      "Train Epoch: 148 [300/500000 (0%)]\tLoss: 1.397609\n",
      "Train Epoch: 149 [100/500000 (0%)]\tLoss: 1.276170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-747:\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "losses = full_training_loop(model, criterion, optimizer, train_dataloader, test_dataloader, \n",
    "                    epochs=total_epochs, print_loss_every_batches=10,\n",
    "                            validate_every_epochs=validate_every_epochs,\n",
    "                           optimizer_step_every_batches=optimizer_step_every_batches,\n",
    "                           per_epoch_use_max_train_batches=per_epoch_use_max_train_batches, \n",
    "                            per_epoch_use_max_test_batches=per_epoch_use_max_test_batches,\n",
    "                           image_path=save_stuff_path+f\"Noisy_Training_{datetime.now()}_{exp_name}.png\",\n",
    "                           save_model_every_epochs=save_model_every_epochs, \n",
    "                            model_path=save_stuff_path+f\"/Noisy_Model_and_Optimizer_{datetime.now()}_{exp_name}.pt\",\n",
    "                            best_model_path=save_stuff_path+f\"/BEST_Noisy_Model_and_Optimizer_{datetime.now()}_{exp_name}.pt\",\n",
    "                            scheduler=scheduler\n",
    "                           )\n",
    "# Execute the training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale, el pb es exclusivamente el Backjprop, ni es el data transfer (ya ke el crazy no improvea el tiempo practicamente), ni es nigun print. Tb un poco es el model inference, ke es la mitad del tiempo de cada batch! \n",
    "\n",
    "\n",
    "Turns out ke unos 7 segdos y medio son inferencia y 25 en total si haces backward -backpropagate-.\n",
    "Asike todo el tiempo gordo, no es ni el retrieval (image library o in situ generation), ni el sending al device (ya ke sendeas o no en cada batch no cambia tpco casi), ni los prints:\n",
    "Son 1/3 del tiempo el inference, y 2/3 del tiempo el backpropagate!\n",
    "\n",
    "LA cosa es en si ppodrias hacer backward solo cuando hagas el step del optimizer y no en cada batch. Lo ke pasa es ke tienes el trade off de ke iras acumulando cada vez un computational graph mas grande! Asike kizas no cunda, porke la memoria se ira llenando mas, y en realidad kizas sea secuencial el backprop ke ejekutes?\n",
    "\n",
    "Pues lo he probado y efectivamente, las batches sin backprop son de 7 y pico secs, pero luego la cuarta (cadda 4 se hace backw pa hacer step), es de 80 secs, en total, 100 secs, como si cada uno hubiese tardado 25 secs. En fin, no hay solucion magica: Solo una-> hacer el modelo mas pequeo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the resulting model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }, f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/Simple_Encoder/Noisy_Model_opt_Adam_it_1_8sperbatchx10batchperepx20epochs_samples_feats_1={feats_1}_feats_2={feats_2}_feats_3={feats_3}_feats_4={feats_4}_prop1={prop1}_prop2={prop2}_prop3={prop3}_av_pool1_div={av_pool1_div}_conv4_feat_size={conv4_feat_size}_av_pool2_div={av_pool2_div}_out_fc_1={out_fc_1}_dropout_p1={dropout_p1}_dropout_p2={dropout_p2}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\\nFINAL VALIDATION! ####################################################\\n\\n\")\n",
    "print(\"Train Set\")\n",
    "#validate_epoch(nn.MSELoss(), model, sampler, dataset, per_epoch_use_max_train_batches)\n",
    "print(\"Test Set\")\n",
    "validate_epoch(nn.MSELoss(), model, test_dataloader, per_epoch_use_max_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "path = \"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/UMAP_Regressor/TEST_IMAGES/\"\n",
    "image_names = os.listdir(f\"{path}\")\n",
    "\n",
    "predictions={}\n",
    "\n",
    "for im_n in image_names:\n",
    "    model.eval()\n",
    "    im = cv2.imread(path+im_n, cv2.IMREAD_ANYDEPTH)\n",
    "    im = compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))\n",
    "    predictions[im_n] = model(im)[0].item()\n",
    "print(\"DEGREES-----------------------------------------------------\")\n",
    "print(\"EXPERIMENTALES ###########\")\n",
    "print(f\"Positivo-Ref deberian ser {13.85} deg son {-(predictions['sin_el_negativo.png']-predictions['antes_de_la_estandar.png'])*180/np.pi/2} deg\")\n",
    "print(f\"Negativo-Ref deberian ser {9.45} deg son {(predictions['sin_el_positivo.png']-predictions['antes_de_la_estandar.png'])*180/np.pi/2} deg\")\n",
    "print(f\"Ambos-Ref deberian ser {4.4} deg son {-(predictions['con_los_dos.png']-predictions['antes_de_la_estandar.png'])*180/np.pi/2} deg\\n\")\n",
    "\n",
    "print(f\"Positivo-Ref2 deberian ser {13.85} deg son {-(predictions['sin_el_negativo.png']-predictions['sin_los_dos_solo_tubo.png'])*180/np.pi/2} deg\")\n",
    "print(f\"Negativo-Ref2 deberian ser {9.45} deg son {(predictions['sin_el_positivo.png']-predictions['sin_los_dos_solo_tubo.png'])*180/np.pi/2} deg\")\n",
    "print(f\"Ambos-Ref2 deberian ser {4.4} deg son {-(predictions['con_los_dos.png']-predictions['sin_los_dos_solo_tubo.png'])*180/np.pi/2} deg\\n\")\n",
    "\n",
    "print(f\"Ref2-Ref deberian ser {0} deg son {(predictions['antes_de_la_estandar.png']-predictions['sin_los_dos_solo_tubo.png'])*180/np.pi/2} deg\\n\")\n",
    "\n",
    "print(f\"El de noventa deberian ser {90} deg son {(predictions['Reference__100.png']-predictions['90__100.png'])*180/np.pi/2} deg\")\n",
    "\n",
    "print(\"TEORICOS NOISY #############\")\n",
    "print(f\"Should be {(2.6544740200042725+1.57120680809021)*180/np.pi/2} deg predicted {(predictions['IM_44_phiCR_2.6544740200042725.png']-predictions['IM_43_phiCR_-1.57120680809021.png'])*180/np.pi/2}\")\n",
    "print(f\"Should be {(-0.6731816530227661+2.4470927715301514)*180/np.pi/2} deg predicted {(predictions['IM_40870_phiCR_-0.6731816530227661.png']-predictions['IM_40871_phiCR_-2.4470927715301514.png'])*180/np.pi/2}\")\n",
    "print(f\"Should be {(0.6789670586585999-0.9714600443840027)*180/np.pi/2} deg predicted {(predictions['IM_52928_phiCR_0.6789670586585999.png']-predictions['IM_52929_phiCR_0.9714600443840027.png'])*180/np.pi/2}\")\n",
    "print(f\"Should be {(0.659442126750946+2.2813968658447266)*180/np.pi/2} deg predicted {(predictions['IM_53017_phiCR_0.659442126750946.png']-predictions['IM_53018_phiCR_-2.2813968658447266.png'])*180/np.pi/2}\")\n",
    "print(f\"Should be {(-2.2813968658447266+2.679948091506958)*180/np.pi/2} deg predicted {(predictions['IM_53018_phiCR_-2.2813968658447266.png']-predictions['IM_53019_phiCR_-2.679948091506958.png'])*180/np.pi/2}\")\n",
    "print(\"TEORICOS NON NOISY #########\")\n",
    "print(f\"Should be {(-2.6049387454986572+1.7562638521194458)*180/np.pi/2} deg predicted {(predictions['IM_5_phiCR_-2.6049387454986572.png']-predictions['IM_6_phiCR_-1.7562638521194458.png'])*180/np.pi/2}\")\n",
    "print(f\"Should be {(-2.946422576904297-1.33404541015625)*180/np.pi/2} deg predicted {(predictions['IM_72_phiCR_-2.946422576904297.png']-predictions['IM_73_phiCR_1.33404541015625.png'])*180/np.pi/2}\")\n",
    "\n",
    "print(\"\\n\\nRADIANS-----------------------------------------------------\")\n",
    "print(\"EXPERIMENTALES ###########\")\n",
    "print(f\"Positivo-Ref deberian ser {13.85*np.pi/180} rad son {-(predictions['sin_el_negativo.png']-predictions['antes_de_la_estandar.png'])/2} rad\")\n",
    "print(f\"Negativo-Ref deberian ser {9.45*np.pi/180} rad son {(predictions['sin_el_positivo.png']-predictions['antes_de_la_estandar.png'])/2} rad\")\n",
    "print(f\"Ambos-Ref deberian ser {4.4*np.pi/180} rad son {-(predictions['con_los_dos.png']-predictions['antes_de_la_estandar.png'])/2} rad\\n\")\n",
    "\n",
    "print(f\"Positivo-Ref2 deberian ser {13.85*np.pi/180} rad son {-(predictions['sin_el_negativo.png']-predictions['sin_los_dos_solo_tubo.png'])/2} rad\")\n",
    "print(f\"Negativo-Ref2 deberian ser {9.45*np.pi/180} rad son {(predictions['sin_el_positivo.png']-predictions['sin_los_dos_solo_tubo.png'])/2} rad\")\n",
    "print(f\"Ambos-Ref2 deberian ser {4.4*np.pi/180} rad son {-(predictions['con_los_dos.png']-predictions['sin_los_dos_solo_tubo.png'])/2} rad\\n\")\n",
    "\n",
    "print(f\"Ref2-Ref deberian ser {0} rad son {(predictions['antes_de_la_estandar.png']-predictions['sin_los_dos_solo_tubo.png'])/2} rad\\n\")\n",
    "\n",
    "print(f\"El de noventa deberian ser {np.pi/2} rad son {(predictions['Reference__100.png']-predictions['90__100.png'])/2} rad\")\n",
    "\n",
    "print(\"TEORICOS NOISY #############\")\n",
    "print(f\"Should be {(2.6544740200042725+1.57120680809021)/2} rad predicted {(predictions['IM_44_phiCR_2.6544740200042725.png']-predictions['IM_43_phiCR_-1.57120680809021.png'])/2}\")\n",
    "print(f\"Should be {(-0.6731816530227661+2.4470927715301514)/2} rad predicted {(predictions['IM_40870_phiCR_-0.6731816530227661.png']-predictions['IM_40871_phiCR_-2.4470927715301514.png'])/2}\")\n",
    "print(f\"Should be {(0.6789670586585999-0.9714600443840027)/2} rad predicted {(predictions['IM_52928_phiCR_0.6789670586585999.png']-predictions['IM_52929_phiCR_0.9714600443840027.png'])/2}\")\n",
    "print(f\"Should be {(0.659442126750946+2.2813968658447266)/2} rad predicted {(predictions['IM_53017_phiCR_0.659442126750946.png']-predictions['IM_53018_phiCR_-2.2813968658447266.png'])/2}\")\n",
    "print(f\"Should be {(-2.2813968658447266+2.679948091506958)/2} rad predicted {(predictions['IM_53018_phiCR_-2.2813968658447266.png']-predictions['IM_53019_phiCR_-2.679948091506958.png'])/2}\")\n",
    "print(\"TEORICOS NON NOISY #########\")\n",
    "print(f\"Should be {(-2.6049387454986572+1.7562638521194458)/2} rad predicted {(predictions['IM_5_phiCR_-2.6049387454986572.png']-predictions['IM_6_phiCR_-1.7562638521194458.png'])/2}\")\n",
    "print(f\"Should be {(-2.946422576904297-1.33404541015625)/2} rad predicted {(predictions['IM_72_phiCR_-2.946422576904297.png']-predictions['IM_73_phiCR_1.33404541015625.png'])/2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charge models and do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/Simple_Encoder/Noisy_Model_and_Optimizer_2022-01-24 19:57:31.886991.pt\")\n",
    "\n",
    "model = Simple_Encoder( X=X, feats_1=feats_1, feats_2=feats_2, feats_3=feats_3, feats_4=feats_4,\n",
    "                 prop1=prop1, prop2=prop2, prop3=prop3, av_pool1_div=av_pool1_div, conv4_feat_size=conv4_feat_size, av_pool2_div=av_pool2_div, \n",
    "                 out_fc_1=out_fc_1,\n",
    "                 dropout_p1=dropout_p1, dropout_p2=dropout_p2 ) \n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intensity_gravity_centers(images):\n",
    "    \"\"\"\n",
    "        Expects input image to be an array of dimensions [N_imgs, h, w].\n",
    "        It will return an array of gravity centers [N_imgs, 2(h,w)] in pixel coordinates\n",
    "        Remember that pixel coordinates are set equal to array indices\n",
    "\n",
    "    \"\"\"\n",
    "    # image wise total intensity and marginalized inensities for weighted sum\n",
    "    intensity_in_w = torch.sum(images, dim=1) # weights for x [N_images, raw_width]\n",
    "    intensity_in_h = torch.sum(images, dim=2) # weights for y [N_images, raw_height]\n",
    "    total_intensity = intensity_in_h.sum(dim=1) # [N_images]\n",
    "\n",
    "    # Compute mass center for intensity\n",
    "    # [N_images, 2] (h_center,w_center)\n",
    "    return torch.nan_to_num( torch.stack(\n",
    "        (torch.matmul(intensity_in_h.float(), torch.arange(images.shape[1], \n",
    "                                    dtype=torch.float32, device=device))/total_intensity,\n",
    "         torch.matmul(intensity_in_w.float(), torch.arange(images.shape[2], \n",
    "                                    dtype=torch.float32, device=device))/total_intensity),\n",
    "        dim=1\n",
    "        ), nan=0.0, posinf=None, neginf=None)\n",
    "\n",
    "def compute_raw_to_centered_iX(images, X=302):\n",
    "\n",
    "        g_raw = compute_intensity_gravity_centers(images) # [ N_images, 2]\n",
    "\n",
    "        # crop the iamges with size (X+1+X)^2 leaving the gravity center in\n",
    "        # the central pixel of the image. In case the image is not big enough for the cropping,\n",
    "        # a 0 padding will be made.\n",
    "        centered_images = torch.zeros( ( images.shape[0], 2*X+1, 2*X+1),  dtype = images.dtype, \n",
    "                                      device=device)\n",
    "\n",
    "        # we round the gravity centers to the nearest pixel indices\n",
    "        g_index_raw = torch.round(g_raw).int() #[ N_images, 2]\n",
    "\n",
    "        # obtain the slicing indices around the center of gravity\n",
    "        # TODO -> make all this with a single array operation by stacking the lower and upper in\n",
    "        # a new axis!!\n",
    "        # [ N_images, 2 (h,w)]\n",
    "        unclipped_lower = g_index_raw-X\n",
    "        unclipped_upper = g_index_raw+X+1\n",
    "\n",
    "        # unclipped could get out of bounds for the indices, so we clip them\n",
    "        lower_bound = torch.clip( unclipped_lower.float(), min=torch.Tensor([[0,0]]).to(device),\n",
    "                                 max=torch.Tensor(list(images.shape[1:])).unsqueeze(0).to(device)).int()\n",
    "        upper_bound = torch.clip( unclipped_upper.float(), min=torch.Tensor([[0,0]]).to(device),\n",
    "                                 max=torch.Tensor(list(images.shape[1:])).unsqueeze(0).to(device)).int()\n",
    "        # we use the difference between the clipped and unclipped to get the necessary padding\n",
    "        # such that the center of gravity is left still in the center of the image\n",
    "        padding_lower = lower_bound-unclipped_lower\n",
    "        padding_upper = upper_bound-unclipped_upper\n",
    "\n",
    "        # crop the image\n",
    "        for im in range(g_raw.shape[0]):\n",
    "            centered_images[im, padding_lower[ im, 0]:padding_upper[ im, 0] or None,\n",
    "                                        padding_lower[ im, 1]:padding_upper[ im, 1] or None] = \\\n",
    "                      images[im, lower_bound[ im, 0]:upper_bound[ im, 0],\n",
    "                                          lower_bound[ im, 1]:upper_bound[ im, 1]]\n",
    "\n",
    "        return centered_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display a FileChooser widget\n",
    "from ipyfilechooser import FileChooser\n",
    "path=\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter\"\n",
    "fc = FileChooser(path+'/LAB/EXPERIMENTAL/Fotos_Turpin/Day2/laser_gaussian_thesis/')\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a single experimental image to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image_full_path=fc.selected\n",
    "#image_full_path=\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/Experimental_Stuff/Fotos_Turpin/Day2/laser_gaussian_thesis/All_Taken_Photos/sin_el_positivo.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "if im is None:\n",
    "    print(f\" Unable to import image {image_full_path}\")\n",
    "    raise ValueError\n",
    "# Center in gravicenter, generating iX\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot its Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d_resolution=0.7\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "prof_x=np.sum(im, axis=0)\n",
    "prof_y=np.sum(im, axis=1)\n",
    "fig = plt.figure(figsize=(2*4.5, 2*4.5))\n",
    "axes=fig.subplots(2,2)\n",
    "\n",
    "cm=axes[0, 0].imshow(im, cmap='viridis')\n",
    "axes[0,0].grid(True)\n",
    "axes[0,1].scatter(prof_y, np.arange(len(prof_y)), s=1, label=f'Intensity profile in y')\n",
    "axes[0,1].set_ylim((0,len(prof_y)))\n",
    "axes[0,1].invert_yaxis()\n",
    "axes[1,0].scatter(np.arange(len(prof_x)), prof_x, s=1, label=f'Intensity profile in y')\n",
    "axes[1,0].set_xlim((0,len(prof_x)))\n",
    "axes[1,0].invert_yaxis()\n",
    "axes[0,0].set_xlabel(\"x (pixels)\")\n",
    "#axes[0,0].set_ylabel(\"y (pixels)\")\n",
    "axes[0,1].set_xlabel(\"Cummulative Intensity\")\n",
    "axes[0,1].set_ylabel(\"y (pixels)\")\n",
    "axes[1,0].set_ylabel(\"Cummulative Intensity\")\n",
    "axes[1,0].set_xlabel(\"x (pixels)\")\n",
    "axes[1,0].grid(True)\n",
    "axes[0,1].grid(True)\n",
    "axes[1,1].set_visible(False)\n",
    "ax = fig.add_subplot(224, projection='3d')\n",
    "Xs,Ys = np.meshgrid(np.arange(len(prof_y)),np.arange(len(prof_x)))\n",
    "fig.suptitle(f\"Intesity Profiles for Image\\n{image_full_path.split('/')[-1]}\")\n",
    "files_for_gif=[]\n",
    "cbax=fig.add_axes([0.54,0.05,0.4,0.01])\n",
    "fig.colorbar(cm, ax=axes[0,0], cax=cbax, orientation='horizontal')\n",
    "theta=25\n",
    "phi=30\n",
    "ax.plot_surface(Xs, Ys, im.T, rcount=int(len(prof_y)*plot3d_resolution), ccount=int(len(prof_x)*plot3d_resolution), cmap='viridis') # rstride=1, cstride=1, linewidth=0\n",
    "#cset = ax.contourf(X, Y, im, 2, zdir='z', offset=-20, cmap='viridis', alpha=0.5)\n",
    "#cset = ax.contourf(X, Y, im, 1, zdir='x', offset=-8, cmap='viridis')\n",
    "#cset = ax.contourf(X, Y, im, 1, zdir='y', offset=0, cmap='viridis')\n",
    "ax.set_xlabel('Y')\n",
    "#ax.set_xlim(-8, 8)\n",
    "ax.set_ylabel('X')\n",
    "#ax.set_ylim(-10, 8)\n",
    "ax.set_zlabel('Intensity')\n",
    "ax.set_zlim(-0.078*np.max(im), np.max(im))\n",
    "ax.set_title(\"Image intensity 3D plot\")\n",
    "ax.view_init(10, theta)\n",
    "#ax.get_proj = lambda: np.dot(Axes3D.get_proj(ax), np.diag([1.3, 1.3, 1.3, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NN predictions for $R_0, w_0, \\phi_{CR}, Z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Custom\")\n",
    "model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "print(f\"Predicted phi_CR {predictions[0]} rad {predictions[0]*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {predictions[0]/2} rad {predictions[0]*180/np.pi/2} deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "print(\"Referencia sin nada\\n\")\n",
    "%matplotlib inline\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day2/laser_gaussian_thesis/All_Taken_Photos/sin_los_dos_solo_tubo.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "#model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "ref=predictions[0].item()\n",
    "print(f\"Predicted phi_CR {ref} rad {ref*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {ref/2} rad {ref*180/np.pi/2} deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Referencia sin nada\\n\")\n",
    "%matplotlib inline\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day2/laser_gaussian_thesis/All_Taken_Photos/antes_de_la_estandar.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "#model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "ref2=predictions[0].item()\n",
    "print(f\"Predicted phi_CR {ref2} rad {ref2*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {ref2/2} rad {ref2*180/np.pi/2} deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sin el negativo\\n\")\n",
    "%matplotlib inline\n",
    "\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day2/laser_gaussian_thesis/All_Taken_Photos/sin_el_negativo.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "#model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "pos=predictions[0].item()\n",
    "print(f\"Predicted phi_CR {pos} rad {pos*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {pos/2} rad {pos*180/np.pi/2} deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sin el positivo\\n\")\n",
    "%matplotlib inline\n",
    "\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day2/laser_gaussian_thesis/All_Taken_Photos/sin_el_positivo.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "#model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "neg = predictions[0].item()\n",
    "print(f\"Predicted phi_CR {neg} rad {neg*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {neg/2} rad {neg*180/np.pi/2} deg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Con ambos\\n\")\n",
    "%matplotlib inline\n",
    "\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day2/laser_gaussian_thesis/All_Taken_Photos/con_los_dos.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "#model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "both=predictions[0].item()\n",
    "print(f\"Predicted phi_CR {both} rad {both*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {both/2} rad {both*180/np.pi/2} deg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ref Ort\\n\")\n",
    "%matplotlib inline\n",
    "\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day3/Reference/Reference__100.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "#model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "ref_ort=predictions[0].item()\n",
    "print(f\"Predicted phi_CR {ref_ort} rad {ref_ort*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {ref_ort/2} rad {ref_ort*180/np.pi/2} deg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ref Ort\\n\")\n",
    "%matplotlib inline\n",
    "\n",
    "image_full_path = \"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/LAB/EXPERIMENTAL/Fotos_Turpin/Day3/Problem/90__100.png\"\n",
    "im = cv2.imread(image_full_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "im = np.asarray((compute_raw_to_centered_iX(torch.from_numpy(im).unsqueeze(0).to(device))).to('cpu').squeeze(0))\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "#model.eval()\n",
    "predictions = model(torch.from_numpy(im).to(device).unsqueeze(0))[0]\n",
    "ort=predictions[0].item()\n",
    "print(f\"Predicted phi_CR {ref_ort} rad {ort*180/np.pi} deg\")\n",
    "print(f\"\\n\\nPredicted Polarization plane -relative to the image plane w axis- is {ort/2} rad {ort*180/np.pi/2} deg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Positivo-Ref deberian ser {13.85} deg son {-(pos-ref)*180/np.pi/2} deg\")\n",
    "print(f\"Negativo-Ref deberian ser {9.45} deg son {(neg-ref)*180/np.pi/2} deg\")\n",
    "print(f\"Ambos-Ref deberian ser {4.4} deg son {(both-ref)*180/np.pi/2} deg\\n\")\n",
    "\n",
    "print(f\"Positivo-Ref2 deberian ser {13.85} deg son {-(pos-ref2)*180/np.pi/2} deg\")\n",
    "print(f\"Negativo-Ref2 deberian ser {9.45} deg son {(neg-ref2)*180/np.pi/2} deg\")\n",
    "print(f\"Ambos-Ref2 deberian ser {4.4} deg son {(both-ref2)*180/np.pi/2} deg\\n\")\n",
    "\n",
    "print(f\"Ref2-Ref deberian ser {0} deg son {(ref2-ref)*180/np.pi/2} deg\\n\")\n",
    "\n",
    "print(f\"El de noventa deberian ser {90} deg son {(ref_ort-ort)*180/np.pi/2} deg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the non-black-box algorithm estimate for $\\phi_{CR}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(f\"../../..\")\n",
    "import sys\n",
    "from SOURCE.CLASS_CODE_GPU_Classes import *\n",
    "from SOURCE.CLASS_CODE_Image_Manager import *\n",
    "from SOURCE.CLASS_CODE_Polarization_Obtention_Algorithms import Rotation_Algorithm, Mirror_Flip_Algorithm, Gradient_Algorithm\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image=im.copy()\n",
    "saturation=0.9\n",
    "pol_or_CR=\"pol\" \n",
    "deg_or_rad=\"deg\" # for the final output\n",
    "image_depth=8 # or 16 bit per pixel\n",
    "image_shortest_side=540\n",
    "randomization_seed=666\n",
    "recenter_average_image=False\n",
    "\n",
    "\n",
    "# 5. POLARIZATION RELATIVE ANGLES ###################################\n",
    "# Mirror with affine interpolation & Rotation Algorithms will be employed\n",
    "# Each using both Fibonacci and Quadratic Fit Search\n",
    "# Also a gradient algorithm\n",
    "theta_min_Rot=-np.pi\n",
    "theta_max_Rot=np.pi\n",
    "rad_min_Grav=3\n",
    "rad_max_Grav=image_shortest_side\n",
    "theta_min_Mir=0\n",
    "theta_max_Mir=np.pi\n",
    "initial_guess_delta_rad=0.1\n",
    "initial_guess_delta_pix=10\n",
    "use_exact_gravicenter=True\n",
    "precision_quadratic=1e-10\n",
    "max_it_quadratic=100\n",
    "cost_tolerance_quadratic=1e-14\n",
    "precision_fibonacci=1e-10\n",
    "max_points_fibonacci=100\n",
    "cost_tolerance_fibonacci=1e-14\n",
    "\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "im_type=np.uint16 if image_depth==16 else np.uint8\n",
    "max_intensity=65535 if image_depth==16 else 255\n",
    "np.random.seed(randomization_seed)\n",
    "polCR=1 if pol_or_CR=='CR' else 0.5\n",
    "\n",
    "# 6. POLARIZATION RELATIVE ANGLES ###################################\n",
    "# Mirror with affine interpolation & Rotation Algorithms will be employed\n",
    "# Each using both Fibonacci and Quadratic Fit Search\n",
    "# Results will be gathered in a table and outputed as an excel csv\n",
    "# Mock Image Loader\n",
    "# Computar el angulo de cada uno en un dataframe donde una de las entradas sea results y haya un result per fibo qfs y per rotation y mirror affine. Y luego procesar en un 7 paso estos angulos para obtener los angulos relativos etc y perhaps hacer tablucha con ground truth menos el resulting delta angle medido por el algoritmo\n",
    "image_loader = Image_Manager(mode=X, interpolation_flag=None)\n",
    "# Define the ROTATION ALGORITHM\n",
    "rotation_algorithm = Rotation_Algorithm(image_loader,\n",
    "    theta_min_Rot, theta_max_Rot, None,\n",
    "    initial_guess_delta_rad, use_exact_gravicenter, initialize_it=False)\n",
    "\n",
    "# Define the Affine Mirror algorithm\n",
    "mirror_algorithm = Mirror_Flip_Algorithm(image_loader,\n",
    "    theta_min_Mir, theta_max_Mir, None,\n",
    "    initial_guess_delta_rad, method=\"aff\", left_vs_right=True, use_exact_gravicenter=use_exact_gravicenter, initialize_it=False)\n",
    "\n",
    "# Define the Gradient algorithm\n",
    "gradient_algorithm = Gradient_Algorithm(image_loader,\n",
    "        rad_min_Grav, rad_max_Grav,\n",
    "        initial_guess_delta_pix,\n",
    "        use_exact_gravicenter)\n",
    "\n",
    "# A dictionary to gather all the resulting angles for each image\n",
    "\n",
    "individual_image_results = { 'polarization_method':[], 'optimization_1d':[], 'found_phiCR':[], 'predicted_opt_precision':[] }\n",
    "\n",
    "def to_result_dict(result_dict, alg, alg_name, opt_name, im_names):\n",
    "    for key, name in zip(alg.times.keys(), im_names):\n",
    "        result_dict['polarization_method'].append(alg_name)\n",
    "        result_dict['optimization_1d'].append(opt_name)\n",
    "        result_dict['found_phiCR'].append(alg.angles[key])\n",
    "        result_dict['predicted_opt_precision'].append(alg.precisions[key])\n",
    "image_container=np.zeros( (1, 2*X+1, 2*X+1), dtype=np.float64)\n",
    "image_names=[]\n",
    "# charge the image\n",
    "image_container[0]=image.astype(np.float64)\n",
    "image_names.append(f\"{fc.selected_filename}\")\n",
    "\n",
    "# charge the image loader:\n",
    "image_loader.import_converted_images_as_array(image_container, image_names)\n",
    "# Execute the Rotation and Mirror Algorithms:\n",
    "# ROTATION ######\n",
    "interpolation_flag=None\n",
    "# the interpolation algorithm used in case we disbale its usage for the iX image obtention will be the Lanczos one\n",
    "rotation_algorithm.interpolation_flag=interpolation_flag if interpolation_flag is not None else cv2.INTER_CUBIC\n",
    "rotation_algorithm.reInitialize(image_loader)\n",
    "rotation_algorithm.quadratic_fit_search(precision_quadratic, max_it_quadratic, cost_tolerance_quadratic)\n",
    "to_result_dict( individual_image_results, rotation_algorithm, \"Rotation\", \"Quadratic\", image_names)\n",
    "rotation_algorithm.reInitialize(image_loader)\n",
    "rotation_algorithm.fibonacci_ratio_search(precision_fibonacci, max_points_fibonacci, cost_tolerance_fibonacci)\n",
    "to_result_dict( individual_image_results, rotation_algorithm, \"Rotation\", \"Fibonacci\", image_names)\n",
    "\n",
    "# MIRROR #######\n",
    "mirror_algorithm.interpolation_flag=interpolation_flag if interpolation_flag is not None else cv2.INTER_CUBIC\n",
    "mirror_algorithm.reInitialize(image_loader)\n",
    "mirror_algorithm.quadratic_fit_search(precision_quadratic, max_it_quadratic, cost_tolerance_quadratic)\n",
    "to_result_dict( individual_image_results, rotation_algorithm, \"Mirror\", \"Quadratic\", image_names)\n",
    "mirror_algorithm.reInitialize(image_loader)\n",
    "mirror_algorithm.fibonacci_ratio_search(precision_fibonacci, max_points_fibonacci, cost_tolerance_fibonacci)\n",
    "to_result_dict( individual_image_results, rotation_algorithm, \"Mirror\", \"Fibonacci\", image_names)\n",
    "\n",
    "# GRADIENT #######\n",
    "def compute_intensity_gravity_center(image):\n",
    "    \"\"\"\n",
    "        Expects input image to be an array of dimensions [h, w].\n",
    "        It will return an array of gravity centers [2(h,w)] in pixel coordinates\n",
    "        Remember that pixel coordinates are set equal to numpy indices\n",
    "\n",
    "    \"\"\"\n",
    "    # image wise total intensity and marginalized inensities for weighted sum\n",
    "    intensity_in_w = np.sum(image, axis=0) # weights for x [raw_width]\n",
    "    intensity_in_h = np.sum(image, axis=1) # weights for y [raw_height]\n",
    "    total_intensity = intensity_in_h.sum()\n",
    "\n",
    "    # Compute mass center for intensity\n",
    "    # [2] (h_center,w_center)\n",
    "    return np.nan_to_num( np.stack(\n",
    "        (np.dot(intensity_in_h, np.arange(image.shape[0]))/total_intensity,\n",
    "         np.dot(intensity_in_w, np.arange(image.shape[1]))/total_intensity)\n",
    "        ) )\n",
    "\n",
    "optimal_masked_gravs={}\n",
    "optimal_radii={}\n",
    "grav=compute_intensity_gravity_center(image)\n",
    "\n",
    "gradient_algorithm.interpolation_flag=interpolation_flag if interpolation_flag is not None else cv2.INTER_CUBIC\n",
    "gradient_algorithm.reInitialize(image_loader)\n",
    "gradient_algorithm.quadratic_fit_search(precision_quadratic, max_it_quadratic, cost_tolerance_quadratic)\n",
    "to_result_dict( individual_image_results, gradient_algorithm, \"Gradient\", \"Quadratic\", image_names)\n",
    "#optimal_masked_gravs['quad'] = gradient_algorithm.masked_gravs[f\"Quadratic_Search_{fc.selected_filename}\"]\n",
    "#optimal_radii['quad'] = gradient_algorithm.optimals[f\"Quadratic_Search_{fc.selected_filename}\"]\n",
    "\n",
    "gradient_algorithm.reInitialize(image_loader)\n",
    "gradient_algorithm.fibonacci_ratio_search(precision_fibonacci, max_points_fibonacci, cost_tolerance_fibonacci)\n",
    "to_result_dict( individual_image_results, gradient_algorithm, \"Gradient\", \"Fibonacci\", image_names)\n",
    "\n",
    "#optimal_masked_gravs['fibo'] = gradient_algorithm.masked_gravs[f\"Fibonacci_Search_{fc.selected_filename}\"]\n",
    "#optimal_radii['fibo'] = gradient_algorithm.optimals[f\"Fibonacci_Search_{fc.selected_filename}\"]\n",
    "\n",
    "#masked_grav=(optimal_masked_gravs['quad']+optimal_masked_gravs['fibo'])/2.0\n",
    "#optimal_radi = (optimal_radii['quad']+optimal_radii['fibo'])/2\n",
    "#print(f\"\\n\\nOptimal masked gravs: {optimal_masked_gravs}\\nOptimal radii: {optimal_radii}\\n\\n\\n\")\n",
    "print(pd.DataFrame.from_dict(individual_image_results))\n",
    "\n",
    "# 7. PROCESS FINAL RESULTS ##########################################\n",
    "def angle_to_pi_pi( angle): # convert any angle to range ()-pi,pi]\n",
    "    angle= angle%(2*np.pi) # take it to [-2pi, 2pi]\n",
    "    return angle-np.sign(angle)*2*np.pi if abs(angle)>np.pi else angle    \n",
    "\n",
    "average_found_phiCR=np.mean([angle_to_pi_pi(phi) for i,phi in enumerate(individual_image_results['found_phiCR']) if individual_image_results['polarization_method'][i]!='Gradient'])\n",
    "print(\"Average found phiCR:\", average_found_phiCR)\n",
    "#print(f\"\\n\\nPredicted slope for main axis: by Gradient {(masked_grav[0]-grav[0])/(masked_grav[1]-grav[1])} and by the others averaged {np.tan(-average_found_phiCR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def forward(self, x): # [batch_size, 2X+1, 2X+1] or [batch_size, 1, 2X+1, 2X+1]\n",
    "    x = x.view(x.shape[0], 1, x.shape[-2], x.shape[-1]) # [batch_size, 1, 2X+1, 2X+1]\n",
    "    X=302\n",
    "    feats_1=15\n",
    "    feats_2=20\n",
    "    feats_3=20\n",
    "    feats_4=20\n",
    "    prop1=3\n",
    "    prop2=2\n",
    "    prop3=1\n",
    "    av_pool1_div=4\n",
    "    conv4_feat_size=15\n",
    "    av_pool2_div=10\n",
    "    out_fc_1=10 \n",
    "    print(x.shape, 2*X+1)\n",
    "\n",
    "    x = self.relu( self.conv1(x) ) # [batch_size, feats_1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "    print(\"conv1\",x.shape, prop1*(2*X+1)/5)\n",
    "\n",
    "\n",
    "    x = self.batchNorm2( self.relu( self.conv2(self.dropout1(x)) )) # [batch_size, feats_2, prop2*(2X+1)/5, prop2*(2X+1)/5]\n",
    "    print(\"conv2\",x.shape,  prop2*(2*X+1)/5)\n",
    "\n",
    "\n",
    "    x = self.relu( self.conv3(self.dropout2(x)) ) # [batch_size, feats_3, prop3*(2X+1)/5, prop3*(2X+1)/5]\n",
    "    print(\"conv3\",x.shape,  prop3*(2*X+1)/5)\n",
    "\n",
    "\n",
    "    x = self.avPool1(x) # [batch_size, feats_3, prop3*(2X+1)/5)/av_pool1_div, prop3*(2X+1)/5)/av_pool1_div]\n",
    "    print(\"av_pool1\",x.shape, int((prop3*(2*X+1)/5)/av_pool1_div))\n",
    "\n",
    "\n",
    "    x = self.batchNorm4(self.conv4(self.dropout2(x))) # [batch_size, feats_4, conv4_feat_size, conv4_feat_size]\n",
    "    print(\"conv4+batchn\",x.shape, conv4_feat_size)\n",
    "\n",
    "\n",
    "    x = self.relu( self.avPool2(x) ) # [batch_size, feats_4, conv4_feat_size/av_pool2_div, conv4_feat_size/av_pool2_div]\n",
    "    print(\"av_pool2\",x.shape, int(conv4_feat_size/av_pool2_div)+1)\n",
    "\n",
    "\n",
    "    x = x.view(x.shape[0], self.in_fc) #[batch_size, feats_4*int(conv4_feat_size/av_pool2_div)**2]\n",
    "    print(\"view_change\",x.shape, feats_4*int(conv4_feat_size/av_pool2_div+1)**2)\n",
    "\n",
    "\n",
    "    x = self.fc2( self.relu( self.fc1(x) ) ) #[batch_size, 4]\n",
    "    print(x.shape, 4)\n",
    "\n",
    "        return x\n",
    "a = Simple_Encoder().to(device)\n",
    "a(torch.ones(2,1, 605,605).to(device))\n",
    "del a\n",
    "torch.cuda.empty_cache()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crazy_epoch(epoch, criterion, model, optimizer, datas, targets, batch_number, batch_size,\n",
    "                      print_loss_every_batches=20,\n",
    "                    optimizer_step_every_batches=1):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    t2 = time()\n",
    "    for k in range(batch_number):        \n",
    "        \n",
    "        prediction = model(datas[k*batch_size:(k+1)*batch_size]) # data is [batch_size, 1, 2X+1, 2X+1]\n",
    "        loss = criterion(prediction, targets[k*batch_size:(k+1)*batch_size])\n",
    "        loss.backward()\n",
    "        \n",
    "        if k % optimizer_step_every_batches==optimizer_step_every_batches-1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # print loss every N batches\n",
    "        if k % print_loss_every_batches == print_loss_every_batches-1:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (k+1) * batch_size, len(datas),\n",
    "                100*(k+1)*batch_size / len(datas), loss.item()))\n",
    "\n",
    "        #total_loss += loss.item()  #.item() is very important here\n",
    "        # Why?-> In order to avoid having total_loss as a tensor in the gpu\n",
    "        t1= time()\n",
    "        print(f\"Iteration time{t1-t2}\")\n",
    "        t2 = time()\n",
    "\n",
    "    return total_loss / len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_crazy_training_loop(model, criterion, optimizer_generator, train_loader,\n",
    "                             batch_number, batch_size, epochs=10,\n",
    "                       print_loss_every_batches=20, optimizer_step_every_batches=1,\n",
    "                            meta_epoch_number=1):\n",
    "    %matplotlib inline\n",
    "    for meta_epoch in range(meta_epoch_number):\n",
    "        for meta_batch_id, (datas, targets) in enumerate(train_loader):        \n",
    "            datas, targets = datas.to(device), targets.to(device) # pero muuh gordos\n",
    "            losses = {\"train\": []}\n",
    "            optimizer = optimizer_generator(model)\n",
    "            for epoch in range(epochs): # que overfitee el muuh gordo este\n",
    "                train_loss = train_crazy_epoch(epoch, criterion, model, optimizer, datas,\n",
    "                                         targets, batch_number, batch_size,\n",
    "                                          print_loss_every_batches=20,\n",
    "                                            optimizer_step_every_batches=1)\n",
    "\n",
    "                display_IPython.clear_output(wait=True)\n",
    "                losses[\"train\"].append(train_loss)\n",
    "                plt.plot(losses[\"train\"], label=f\"log training loss- MetaBatch {meta_batch_id/len(train_loader)*100}%\")\n",
    "                plt.yscale('log')\n",
    "                plt.legend()\n",
    "                plt.pause(0.001)\n",
    "                plt.show()   \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_epoch_number = 1\n",
    "meta_batch_size = 100\n",
    "batch_size = 10\n",
    "batch_number = int(meta_batch_size/batch_size)\n",
    "assert(meta_batch_size%batch_size==0)\n",
    "\n",
    "crazy_loader = DataLoader(training_data, batch_size=meta_batch_size, shuffle=True, num_workers=worker_num,\n",
    "                              pin_memory=True, drop_last=False, persistent_workers=False)\n",
    "\n",
    "def adam_generator(model):\n",
    "    return torch.optim.Adam(model.parameters(), lr=0.05, betas=(0.99, 0.9999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "full_crazy_training_loop(model, criterion, \n",
    "                         adam_generator, \n",
    "                         crazy_loader,\n",
    "                             batch_number, batch_size, epochs=10,\n",
    "                       print_loss_every_batches=10, optimizer_step_every_batches=2, \n",
    "                         meta_epoch_number=meta_epoch_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time()\n",
    "for datas, targets in train_dataloader:\n",
    "    datas, targets = datas.to(device), targets.to(device)\n",
    "    pred = model(datas)\n",
    "    t2=time()\n",
    "    print(f\"inf time {t2-t1}\")\n",
    "    loss = criterion(pred, targets)\n",
    "    loss.backward()\n",
    "    t3=time()\n",
    "    print(f\"with backward {t3-t1}\")\n",
    "    t1=time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
