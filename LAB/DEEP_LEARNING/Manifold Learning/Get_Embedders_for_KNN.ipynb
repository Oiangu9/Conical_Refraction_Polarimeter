{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart and Run All Function to do the whole pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arregleu dill eta picklen problemie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Javascript\n",
    "\n",
    "def restart_run_all():\n",
    "    display(HTML(\n",
    "        '''\n",
    "            <script>\n",
    "                code_show = false;\n",
    "                IPython.notebook.kernel.restart();\n",
    "                setTimeout(function(){\n",
    "                        IPython.notebook.execute_all_cells();\n",
    "                    }, 1000)\n",
    "                \n",
    "            </script>\n",
    "        '''\n",
    "    ))\n",
    "#restart_run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import gc\n",
    "\n",
    "def free():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    libc.malloc_trim(0)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to jump cells if a condition is met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip(line, cell=None):\n",
    "    '''Skips execution of the current line/cell if line evaluates to True.'''\n",
    "    if eval(line):\n",
    "        return\n",
    "\n",
    "    get_ipython().run_cell(cell) \n",
    "\n",
    "def load_ipython_extension(shell):\n",
    "    '''Registers the skip magic when the extension loads.'''\n",
    "    shell.register_magic_function(skip, 'line_cell')\n",
    "\n",
    "def unload_ipython_extension(shell):\n",
    "    '''Unregisters the skip magic when the extension unloads.'''\n",
    "    del shell.magics_manager.magics['cell']['skip']\n",
    "    \n",
    "    \n",
    "load_ipython_extension(get_ipython())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get state of pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "exp_name=\"Noisy_Dataset_Embedders\"\n",
    "\n",
    "try:\n",
    "    f = open(f\"state_{exp_name}_Embedders.txt\", \"r\")\n",
    "    current_state = int(f.read())\n",
    "    f.close()\n",
    "    \n",
    "    current_state+=1\n",
    "except:\n",
    "    current_state = 0\n",
    "\n",
    "print(current_state)\n",
    "skip_it = current_state==6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Image Dataset to train the embedders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import sklearn.decomposition\n",
    "import sklearn.manifold\n",
    "import umap\n",
    "import dill\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, GT_file_path, images_dir_path):\n",
    "        self.df_GTs = pd.DataFrame.from_dict(json.load(open(GT_file_path)))\n",
    "        self.images_dir_path = images_dir_path\n",
    "        self.len_data = len(self.df_GTs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = f\"{self.images_dir_path}/IM_{self.df_GTs.iloc[idx,0]}_phiCR_{self.df_GTs.iloc[idx,1]}.png\"\n",
    "        image = read_image(img_path) #[1, 2X+1, 2X+1] torch tensor\n",
    "        label = torch.Tensor([float(self.df_GTs.iloc[idx, 1])]).type(torch.float32) #[1] torch tensor of float32\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose Dataset and **training** hyperparameters for the Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Train set!\n",
    "GT_file_path_train_noisy = f\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/OUTPUT/NOISY/TRAIN/GROUND_TRUTHS.json\"\n",
    "images_dir_path_train_noisy =f\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/OUTPUT/NOISY/TRAIN\"\n",
    "\n",
    "# Non-Noisy Train set!\n",
    "GT_file_path_train_non_noisy = f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/IMAGE_LIBRARY/NON_NOISY/TEST/GROUND_TRUTHS.json\"\n",
    "images_dir_path_train_non_noisy = f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/IMAGE_LIBRARY/NON_NOISY/TEST\"\n",
    "\n",
    "# Test set\n",
    "GT_file_path_test_noisy = f\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/OUTPUT/NOISY/TEST/GROUND_TRUTHS.json\"\n",
    "images_dir_path_test_noisy =f\"/home/oiangu/Desktop/Conical_Refraction_Polarimeter/OUTPUT/NOISY/TEST\"\n",
    "\n",
    "# Non-Noisy Train set!\n",
    "GT_file_path_test_non_noisy = f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/IMAGE_LIBRARY/NON_NOISY/TEST/GROUND_TRUTHS.json\"\n",
    "images_dir_path_test_non_noisy = f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/IMAGE_LIBRARY/NON_NOISY/TEST\"\n",
    "\n",
    "use_noisy=True\n",
    "num_images= 5000\n",
    "num_decimals = 3\n",
    "random_seed = 666\n",
    "n_jobs=11\n",
    "\n",
    "emb_dims=10\n",
    "\n",
    "save_stuff_path = f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/Embedders/Embedders_and_KNN/{exp_name}/\"\n",
    "os.makedirs(save_stuff_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_noisy:\n",
    "    training_data = ImageDataset(GT_file_path_train_noisy, images_dir_path_train_noisy)\n",
    "    #test_data = ImageDataset(GT_file_path_test_noisy, images_dir_path_test_noisy)\n",
    "else:\n",
    "    training_data = ImageDataset(GT_file_path_train_non_noisy, images_dir_path_train_non_noisy)\n",
    "    #test_data = ImageDataset(GT_file_path_test_non_noisy, images_dir_path_test_non_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate proper finite dataset for Embedders! X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "random_indices = np.random.choice(range(len(training_data)), num_images, replace=False)\n",
    "#random_indices = np.random.choice(range(2850), num_images, replace=False)\n",
    "X21 = training_data[0][0].shape[1]\n",
    "X = np.zeros( (num_images, X21**2), dtype=np.float32)\n",
    "y = np.zeros((num_images), dtype=np.float64)\n",
    "\n",
    "for j,idx in enumerate(random_indices):\n",
    "    im, lab = training_data[idx]\n",
    "    X[j, :] = im[0].flatten()\n",
    "    y[j] = lab   \n",
    "\n",
    "y_categoric = (np.around(y+np.pi, num_decimals)*10**num_decimals).astype(int)\n",
    "\n",
    "free()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data for KNN tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrandom_indices_test = np.random.choice(range(len(test_data)), num_images_test, replace=False)\\n#random_indices = np.random.choice(range(2850), num_images, replace=False)\\nX_test = np.zeros( (num_images_test, X21**2), dtype=np.float32)\\ny_test= np.zeros((num_images_test), dtype=np.float64)\\n\\nfor j,idx in enumerate(random_indices_test):\\n    im, lab = test_data[idx]\\n    X_test[j, :] = im[0].flatten()\\n    y_test[j] = lab   \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "random_indices_test = np.random.choice(range(len(test_data)), num_images_test, replace=False)\n",
    "#random_indices = np.random.choice(range(2850), num_images, replace=False)\n",
    "X_test = np.zeros( (num_images_test, X21**2), dtype=np.float32)\n",
    "y_test= np.zeros((num_images_test), dtype=np.float64)\n",
    "\n",
    "for j,idx in enumerate(random_indices_test):\n",
    "    im, lab = test_data[idx]\n",
    "    X_test[j, :] = im[0].flatten()\n",
    "    y_test[j] = lab   \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (B) Class to Use Embedding Spaces for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_sk_KNN = {'n_neighbors':3, 'weights':'distance', 'algorithm':'auto', 'leaf_size':50, 'p':2,\n",
    "               'metric':'minkowski', 'n_jobs':n_jobs}\n",
    "\n",
    "class KNN_Regressor():\n",
    "    def __init__(self, embedder_func, args_sk_KNN):\n",
    "        # ‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’\n",
    "        # ‘uniform’, ‘distance’\n",
    "        self.embedder_func = embedder_func\n",
    "        self.KNN = sk.neighbors.KNeighborsRegressor(n_neighbors=args_sk_KNN['n_neighbors'],\n",
    "                    weights=args_sk_KNN['weights'], algorithm=args_sk_KNN['algorithm'],\n",
    "                    leaf_size=args_sk_KNN['leaf_size'], p=args_sk_KNN['p'], \n",
    "                    metric=args_sk_KNN['metric'], n_jobs=args_sk_KNN['n_jobs'])\n",
    "        self.fitted = False\n",
    "    \n",
    "    def fit(self, X, y, already_embedded_X=False): # X [N_samples, dim_feats], y [N_samples] # y can be to regression floats!\n",
    "        if not already_embedded_X:\n",
    "            X = self.embedder_func(X) # [N_samples, dim_feats]\n",
    "        self.KNN = self.KNN.fit(X, y)\n",
    "        self.fitted = True\n",
    "    \n",
    "    def score(self, X, y, already_embedded_X=False):\n",
    "        if not already_embedded_X:\n",
    "            X = self.embedder_func(X)\n",
    "        return self.KNN.score(X,y)\n",
    "        \n",
    "    def predict(self, X, already_embedded_X=False):\n",
    "        if not already_embedded_X:\n",
    "            X = self.embedder_func(X)\n",
    "        return self.KNN.predict(X)\n",
    "    \n",
    "class Sklearn_embedder():\n",
    "    def __init__(self, embedder, preprocess_fct):\n",
    "        self.preprocess_fct = preprocess_fct\n",
    "        self.embedder = embedder\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.embedder.transform( self.preprocess_fct(X) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (C) Generate Embedding Space Transformers\n",
    "## Choose Embedder Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_state==0:\n",
    "    # PCA -> Existe la incremental PCA por si es massa grande el dataset!\n",
    "    args = {'exp':'PCA','emb_dims':emb_dims, \"whiten\":True}\n",
    "    embedder = sk.decomposition.PCA(n_components=args['emb_dims'], whiten=args['whiten'], random_state=random_seed)\n",
    "\n",
    "elif current_state==1:\n",
    "    # KPCA\n",
    "    args = {'exp':'KPCA_rbf', 'emb_dims':emb_dims, 'kernel':'rbf', 'fit_inverse':True, 'max_iter':100}\n",
    "    # kernels: linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘cosine’, ‘precomputed’\n",
    "    embedder = sk.decomposition.KernelPCA(n_components=args['emb_dims'], kernel=args['kernel'], \n",
    "                    fit_inverse_transform=args['fit_inverse'], max_iter=args['max_iter'], \n",
    "                            random_state=random_seed, n_jobs=n_jobs)\n",
    "elif current_state==2:\n",
    "    # LLE \n",
    "    args = {'exp':'LLE_standard',\"method\":\"standard\", \"n_neighbors\": 200, \"emb_dims\": emb_dims, 'max_iter':100}\n",
    "    # Methods: standard, hessian, ltsa, modified (modified_tol) \n",
    "    embedder = sk.manifold.LocallyLinearEmbedding(method=args['method'], n_neighbors=args['n_neighbors'],\n",
    "                  n_components=args['emb_dims'], max_iter=args['max_iter'], random_state=random_seed, n_jobs=n_jobs)\n",
    "\n",
    "elif current_state==3:\n",
    "    # UMAP -> uses y continous\n",
    "    args = {'exp':'UMAP', 'emb_dims':emb_dims, 'min_dist':0.1, 'n_neighbors':300, 'metric':'hamming', 'n_epochs':None,\n",
    "           'target_metric':'l2'}\n",
    "    # Metrics: euclidean, canberra, cosine, manhattan, braycurtis, mahalanobis, hamming\n",
    "    embedder = umap.UMAP(n_components=args['emb_dims'], min_dist=args['min_dist'], n_epochs=args['n_epochs'],\n",
    "                n_neighbors=args['n_neighbors'], metric=args['metric'], random_state=random_seed, n_jobs=n_jobs,\n",
    "                        target_metric=args['target_metric']) \n",
    "\n",
    "elif current_state==4:\n",
    "    # ISOMAP\n",
    "    args = {'exp':'ISOMAP', 'n_neighbors':200, 'emb_dims':emb_dims, 'max_iter':100, 'neighbors_algorithm':'auto', 'metric':'minkowski' }\n",
    "    embedder = sk.manifold.Isomap( n_neighbors=args['n_neighbors'],n_components=args['emb_dims'],\n",
    "                        max_iter=args['max_iter'], neighbors_algorithm=args['neighbors_algorithm'], n_jobs=n_jobs,\n",
    "                        metric=args['metric'], p=2)\n",
    "\n",
    "elif current_state==5:\n",
    "    # NCA -> uses y categorical\n",
    "    args = {'exp':'NCA', 'emb_dims':emb_dims, 'init':'auto', 'max_iter':100 }\n",
    "    # init ‘auto’, ‘pca’, ‘lda’, ‘identity’, ‘random’\n",
    "    embedder = sk.neighbors.NeighborhoodComponentsAnalysis(n_components=args['emb_dims'], init=args['init'],\n",
    "                                    max_iter=args['max_iter'], random_state=random_seed)\n",
    "\n",
    "elif current_state==6:\n",
    "    args = {'exp':'RAW'}\n",
    "    embedder = lambda X : X\n",
    "    \n",
    "\n",
    "elif current_state!=7:\n",
    "    raise ValueError\n",
    "# Triplet\n",
    "#args = {'exp':'TRIPLET_CNN', 'emb_dims':10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply pre-embedding pre-process function to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "%%skip $skip_it\n",
    "\n",
    "pre_process_name = \"normalize_to_max_and_iX\"\n",
    "\n",
    "import torch\n",
    "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
    "\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def compute_intensity_gravity_centers_torch( images):\n",
    "    \"\"\"\n",
    "        Expects input image to be an array of dimensions [N_imgs, h, w].\n",
    "        It will return an array of gravity centers [N_imgs, 2(h,w)] in pixel coordinates\n",
    "        Remember that pixel coordinates are set equal to array indices\n",
    "\n",
    "    \"\"\"\n",
    "    # image wise total intensity and marginalized inensities for weighted sum\n",
    "    intensity_in_w = torch.sum(images, dim=1) # weights for x [N_images, raw_width]\n",
    "    intensity_in_h = torch.sum(images, dim=2) # weights for y [N_images, raw_height]\n",
    "    total_intensity = intensity_in_h.sum(dim=1) # [N_images]\n",
    "\n",
    "    # Compute mass center for intensity\n",
    "    # [N_images, 2] (h_center,w_center)\n",
    "    return torch.nan_to_num( torch.stack(\n",
    "        (torch.matmul(intensity_in_h.float(), torch.arange(images.shape[1], \n",
    "                                    dtype=torch.float32, device=device))/total_intensity,\n",
    "         torch.matmul(intensity_in_w.float(), torch.arange(images.shape[2], \n",
    "                                    dtype=torch.float32, device=device))/total_intensity),\n",
    "        dim=1\n",
    "        ), nan=0.0, posinf=None, neginf=None)\n",
    "\n",
    "\n",
    "def compute_raws_to_centered_iXs_torch( images, X, device):\n",
    "\n",
    "    g_raw = compute_intensity_gravity_centers_torch(images) # [ N_images, 2]\n",
    "\n",
    "    # crop the iamges with size (X+1+X)^2 leaving the gravity center in\n",
    "    # the central pixel of the image. In case the image is not big enough for the cropping,\n",
    "    # a 0 padding will be made.\n",
    "    centered_images = torch.zeros( ( images.shape[0], 2*X+1, 2*X+1),  dtype = images.dtype, \n",
    "                                  device=device)\n",
    "\n",
    "    # we round the gravity centers to the nearest pixel indices\n",
    "    g_index_raw = torch.round(g_raw).int() #[ N_images, 2]\n",
    "\n",
    "    # obtain the slicing indices around the center of gravity\n",
    "    # TODO -> make all this with a single array operation by stacking the lower and upper in\n",
    "    # a new axis!!\n",
    "    # [ N_images, 2 (h,w)]\n",
    "    unclipped_lower = g_index_raw-X\n",
    "    unclipped_upper = g_index_raw+X+1\n",
    "\n",
    "    # unclipped could get out of bounds for the indices, so we clip them\n",
    "    lower_bound = torch.clip( unclipped_lower.float(), min=torch.Tensor([[0,0]]).to(device),\n",
    "                             max=torch.Tensor(list(images.shape[1:])).unsqueeze(0).to(device)).int()\n",
    "    upper_bound = torch.clip( unclipped_upper.float(), min=torch.Tensor([[0,0]]).to(device),\n",
    "                             max=torch.Tensor(list(images.shape[1:])).unsqueeze(0).to(device)).int()\n",
    "    # we use the difference between the clipped and unclipped to get the necessary padding\n",
    "    # such that the center of gravity is left still in the center of the image\n",
    "    padding_lower = lower_bound-unclipped_lower\n",
    "    padding_upper = upper_bound-unclipped_upper\n",
    "\n",
    "    # crop the image\n",
    "    for im in range(g_raw.shape[0]):\n",
    "        centered_images[im, padding_lower[ im, 0]:padding_upper[ im, 0] or None,\n",
    "                                    padding_lower[ im, 1]:padding_upper[ im, 1] or None] = \\\n",
    "                  images[im, lower_bound[ im, 0]:upper_bound[ im, 0],\n",
    "                                      lower_bound[ im, 1]:upper_bound[ im, 1]]\n",
    "\n",
    "    return centered_images\n",
    "\n",
    "import gc\n",
    "def normalize_to_max_and_iX_input_output_flatten(images, dtype=np.float64,\n",
    "                    iX_dev='cpu', out_dev='cpu', X=302, batch_size=100): # images expected to be [N_images, h, w]\n",
    "    out = np.zeros((images.shape[0], images.shape[-1]), dtype=np.float64)\n",
    "    images= images.reshape(-1, X*2+1, X*2+1).astype(dtype)/np.expand_dims( np.amax(images, axis=(-2,-1) ), (-2,-1) )\n",
    "    for j in range(0, images.shape[0], batch_size):\n",
    "        out[j:(j+batch_size)] = compute_raws_to_centered_iXs_torch( torch.from_numpy(images[j:(j+batch_size)]).to(device), X, device).to('cpu').numpy().reshape(batch_size, -1)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return out\n",
    "\n",
    "\n",
    "preprocess_fct = normalize_to_max_and_iX_input_output_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $skip_it\n",
    "# the training images do not need to be normalized and iX, only normalized to max (already iX)\n",
    "\n",
    "#X = preprocess_fct(X) # apply preprocess funct\n",
    "X = X/np.expand_dims(np.amax(X, axis=(-1) ), 1)\n",
    "free()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Embedding Training and KNN Training after it\n",
    "Note that the embedders alone can be also used as metric for the simulation coordinate descent or Nedler Mead.\n",
    "\n",
    "And note also that the embedder is still required for inference in KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $skip_it\n",
    "\n",
    "date = datetime.now().strftime(\"%d_%m_%Y_%Hh%Mm%s\")\n",
    "f_name_emb = f\"{args['exp']}_EMBEDDER_n_images_{num_images}_emb_dims_{emb_dims}_pre_process_{pre_process_name}_n_decimals_{num_decimals}_seed_{random_seed}_date_{date}.sav\"\n",
    "f_name_knn = f\"{args['exp']}_KNN_n_images_{num_images}_emb_dims_{emb_dims}_pre_process_{pre_process_name}_n_decimals_{num_decimals}_seed_{random_seed}_date_{date}.sav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oiangu/anaconda3/lib/python3.8/site-packages/umap/umap_.py:1802: UserWarning: gradient function is not yet implemented for hamming distance metric; inverse_transform will be unavailable\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "%%skip $skip_it\n",
    "\n",
    "if args['exp']=='NCA': # then y categorical\n",
    "    X_embedded = embedder.fit_transform(X, y=y_categoric)\n",
    "elif args['exp']=='RAW':\n",
    "    X_embedded = embedder(X)\n",
    "elif args['exp']=='UMAP':\n",
    "    embedder = embedder.fit(X, y=y)\n",
    "    X_embedded = embedder.embedding_\n",
    "else: # in relaity only UMAP uses y from the rest, but it can handle y continous\n",
    "    X_embedded = embedder.fit_transform(X, y=y)\n",
    "\n",
    "del X\n",
    "free()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the embedder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_stuff_path+f_name_emb, 'wb') as f:\n",
    "    dill.dump(embedder, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $skip_it\n",
    "free()\n",
    "\n",
    "knn_regressor = KNN_Regressor( Sklearn_embedder( embedder, preprocess_fct), args_sk_KNN)\n",
    "knn_regressor.fit(X_embedded, y, already_embedded_X=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trained embedder and knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $skip_it\n",
    "\n",
    "with open(save_stuff_path+f_name_knn, 'wb') as f:\n",
    "    dill.dump(knn_regressor, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Trained Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedder = dill.load((open(save_stuff_path+\"NCA_EMBEDDER_n_images_500_emb_dims_10_pre_process_normalize_to_max_and_iX_n_decimals_3_seed_666_date_13_06_2022_18-M-28.sav\", 'rb')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize resulting embedding in lower dimensions using PCA, first 2d PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_embedded = embedder.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $skip_it\n",
    "\n",
    "free()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "principalComponents = pca.fit_transform(X_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bokeh and px see 2d PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $skip_it\n",
    "\n",
    "import plotly.express as px\n",
    "fig=px.scatter(np.concatenate((principalComponents, y[:,np.newaxis]), 1), x=0, y=1, color=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"{args['exp']} -> PCA to 2D\",\n",
    "    xaxis_title=\"Principal component 1\",\n",
    "    yaxis_title=\"Principal component 2\",\n",
    "    coloraxis_colorbar=dict(title=\"phi CR\",)\n",
    "    )\n",
    "fig.update_traces(marker={\"size\":5})\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "\n",
    "fig.write_image(f\"{save_stuff_path}/{f_name_emb}_2d.png\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, LinearColorMapper\n",
    "from bokeh.palettes import Spectral10\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "def embeddable_image(data):\n",
    "    img_data = data.values.reshape(X21,X21).astype(np.uint8)\n",
    "    image = Image.fromarray(img_data, mode='L').resize((64, 64), Image.BICUBIC)\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format='png')\n",
    "    for_encoding = buffer.getvalue()\n",
    "    return 'data:image/png;base64,' + base64.b64encode(for_encoding).decode()\n",
    "\n",
    "df['image'] = pd.DataFrame(data=X, columns=list(range(X.shape[1]))).apply(embeddable_image, axis=1)\n",
    "\n",
    "datasource = ColumnDataSource(df)\n",
    "color_mapping = LinearColorMapper(\n",
    "    palette='Magma256',\n",
    "    low=y.min(),\n",
    "    high=y.max()\n",
    ")\n",
    "\n",
    "plot_figure = figure(\n",
    "    title='PCA projection of the CR dataset',\n",
    "    plot_width=800,\n",
    "    plot_height=800,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    ")\n",
    "\n",
    "plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "    </div>\n",
    "    <div>\n",
    "        <span style='font-size: 16px; color: #224499'>phiCR:</span>\n",
    "        <span style='font-size: 18px'>@phiCR</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plot_figure.circle(\n",
    "    'principal component 1',\n",
    "    'principal component 2',\n",
    "    source=datasource,\n",
    "    color=dict(field='phiCR', transform=color_mapping),\n",
    "    line_alpha=0.6,\n",
    "    fill_alpha=0.6,\n",
    "    size=7\n",
    ")\n",
    "show(plot_figure)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now 3d PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $skip_it\n",
    "free()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "principalComponents = pca.fit_transform(X_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%skip $skip_it\n",
    "\n",
    "fig=px.scatter_3d(np.concatenate((principalComponents, y[:,np.newaxis]), 1), x=0, y=1, z=2, color=3)\n",
    "fig.update_layout(\n",
    "    title=f\"{args['exp']} -> PCA to 3D\",\n",
    "    scene = dict(\n",
    "                    xaxis_title='princ. comp 1',\n",
    "                    yaxis_title='princ. comp 2',\n",
    "                    zaxis_title='princ. comp 3'),\n",
    "    coloraxis_colorbar=dict(title=\"phi CR\",)\n",
    "    )\n",
    "\n",
    "fig.update_traces(marker={\"size\":3})\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "\n",
    "fig.write_image(f\"{save_stuff_path}/{f_name_emb}_3d.png\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "free()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save just in case the generated embedding and the employed indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Initialize the hdf5 dataset saver\n",
    "h5f = h5py.File(f\"{save_stuff_path}/{args['exp_name']}_Training_Embedding_date_{datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}.h5\", 'a') # append if exists, create if not\n",
    "h5f.create_dataset(\"Embeddings\", data= embedded_training, compression=\"lzf\", shuffle=True)\n",
    "h5f.create_dataset(\"phiCRs\", data= y, compression=\"lzf\", shuffle=True)\n",
    "h5f.create_dataset(\"ImageIndexes\", data= random_indices, compression=\"lzf\", shuffle=True)\n",
    "\n",
    "h5f.flush()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If everything fine until here then updte state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $skip_it\n",
    "\n",
    "f = open(f\"state_{exp_name}_Embedders.txt\", \"w\")\n",
    "f.write(str(current_state))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart kernel and re-run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $skip_it\n",
    "\n",
    "restart_run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (D.2) Using Torch Embedder of Triplet Loss and train KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #should be installed by default in any colab notebook\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
    "\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "class Proximity_Metric_Based_On_Simple_Encoder(nn.Module):\n",
    "    def __init__(self, X=302, feats_1=15, feats_2=20, feats_3=20, feats_4=20,\n",
    "                 prop1=3, prop2=2, prop3=1, av_pool1_div=4, conv4_feat_size=15, av_pool2_div=10, \n",
    "                 out_fc_1=10, out_fc_2=10,\n",
    "                 dropout_p1=0.2, dropout_p2=0.1\n",
    "                ): \n",
    "        # propj is such that the_ image getting out from stage j is propj/prop_{j-1}-ths of the previous (with j=0 being 5)\n",
    "        # clearly, prop_{j-1}>prop_{j}>...\n",
    "        # 2X+1 will be assumed to be divisible by 5\n",
    "        assert((2*X+1)%5==0)\n",
    "        assert(prop1>prop2)\n",
    "        assert(prop2>prop3)\n",
    "        assert((int((prop3*(2*X+1)/5)/av_pool1_div)-conv4_feat_size)>0)\n",
    "        \n",
    "        \n",
    "        super(Proximity_Metric_Based_On_Simple_Encoder, self).__init__()\n",
    "        # in is [epoch_size, 1, 2X+1, 2X+1]\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=feats_1, \n",
    "                               kernel_size = int((2*X+1)/5*(5-prop1)+1), bias=True) \n",
    "        # out conv1 [epoch_size, feats_1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "        self.conv2 = nn.Conv2d(in_channels=feats_1, out_channels=feats_2, \n",
    "                               kernel_size = int((2*X+1)/5*(prop1-prop2)+1), bias=True) \n",
    "        # out conv1 [epoch_size, feats_2, prop2*(prop1*(2X+1)/5)/prop1, prop2*(prop1*(2X+1)/5)/prop1]\n",
    "        # that is [epoch_size, feats_2, prop2*(2X+1)/5), prop2*(2X+1)/5)]\n",
    "        self.conv3 = nn.Conv2d(in_channels=feats_2, out_channels=feats_3, \n",
    "                               kernel_size = int((2*X+1)/5*(prop2-prop3)+1), bias=True)\n",
    "        # out conv3 is [epoch_size, feats_3, prop3*(2X+1)/5), prop3*(2X+1)/5)]\n",
    "\n",
    "        self.avPool1 = nn.AvgPool2d(kernel_size= int((prop3*(2*X+1)/5)*(1-1/av_pool1_div)) +1, stride=1)\n",
    "        # out avpool1 is [epoch_size, feats_3, prop3*(2X+1)/5)/av_pool1_div, prop3*(2X+1)/5)/av_pool1_div]\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=feats_3, out_channels=feats_4, \n",
    "                              kernel_size= int((prop3*(2*X+1)/5)/av_pool1_div+1)-conv4_feat_size+1, bias=True)\n",
    "        # [epoch_size, feats_4, conv4_feat_size, conv4_feat_size]\n",
    "        \n",
    "        self.avPool2 = nn.AvgPool2d(kernel_size= int(conv4_feat_size*(1-1/av_pool2_div)) +1, stride=1)\n",
    "        # out avpool1 is [epoch_size, feats_4, conv4_feat_size/av_pool2_div+1, conv4_feat_size/av_pool2_div+1]\n",
    "        \n",
    "        #self.in_fc = int(feats_4*(conv4_feat_size/av_pool2_div+1)**2)\n",
    "        self.in_fc = feats_4*((((((2*X+1-int((2*X+1)/5*(5-prop1)+1)+1)\n",
    "                                  -int((2*X+1)/5*(prop1-prop2)+1)+1)\n",
    "                                 -int((2*X+1)/5*(prop2-prop3)+1)+1)\n",
    "                                -int((prop3*(2*X+1)/5)*(1-1/av_pool1_div)) -1+1)\n",
    "                               -int((prop3*(2*X+1)/5)/av_pool1_div+1)+conv4_feat_size-1+1)\n",
    "                              -int(conv4_feat_size*(1-1/av_pool2_div)) -1+1)**2\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=self.in_fc, out_features=out_fc_1, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=out_fc_1, out_features=out_fc_2, bias=True)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=dropout_p1, inplace=False)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p2, inplace=False)\n",
    "        self.relu = torch.nn.functional.leaky_relu\n",
    "\n",
    "        self.batchNorm2 = nn.BatchNorm2d(num_features=feats_2)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(num_features=feats_4)\n",
    "\n",
    "    def forward(self, x): # [batch_size, 2X+1, 2X+1] or [batch_size, 1, 2X+1, 2X+1]\n",
    "        x = x.view(x.shape[0], 1, x.shape[-2], x.shape[-1]).float() # [batch_size, 1, 2X+1, 2X+1]\n",
    "        # Normalize to unity the float image\n",
    "        x = x/x.amax(dim=(2,3), keepdim=True)[0] # [batch_size, 1, 2X+1, 2X+1]\n",
    "        \n",
    "        x = self.relu( self.conv1(x) ) # [batch_size, feats_1, prop1*(2X+1)/5, prop1*(2X+1)/5]\n",
    "        \n",
    "        x = self.batchNorm2( self.relu( self.conv2(self.dropout1(x)) )) # [batch_size, feats_2, prop2*(2X+1)/5, prop2*(2X+1)/5]\n",
    "\n",
    "        \n",
    "        x = self.relu( self.conv3(self.dropout2(x)) ) # [batch_size, feats_3, prop3*(2X+1)/5, prop3*(2X+1)/5]\n",
    "\n",
    "        \n",
    "        x = self.avPool1(x) # [batch_size, feats_3, prop3*(2X+1)/5)/av_pool1_div, prop3*(2X+1)/5)/av_pool1_div]\n",
    "\n",
    "        \n",
    "        x = self.batchNorm4(self.conv4(self.dropout2(x))) # [batch_size, feats_4, conv4_feat_size, conv4_feat_size]\n",
    "\n",
    "        \n",
    "        x = self.relu( self.avPool2(x) ) # [batch_size, feats_4, conv4_feat_size/av_pool2_div, conv4_feat_size/av_pool2_div]\n",
    "\n",
    "        \n",
    "        x = x.view(x.shape[0], self.in_fc) #[batch_size, feats_4*int(conv4_feat_size/av_pool2_div)**2]\n",
    "\n",
    "        \n",
    "        x = self.fc2( self.relu( self.fc1(x) ) ) #[batch_size, out_fc_2]\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Proximity_Metric_Based_On_Corrector(nn.Module):\n",
    "    def __init__(self, S0=2*302+1, S1=2*290+1, S2=2*250+1, S3=2*200+1, S4 = 2*10+1,\n",
    "                 S5 = 2*1+1, S6 =2,\n",
    "                 feats_S1=10, feats_S2=10, feats_S3=20, feats_S4=20, feats_S5 = 20,\n",
    "                 out_fc1=100, out_fc2=10,\n",
    "                 feats_S6 = 25,\n",
    "                 dropout_p=0.1\n",
    "                ): \n",
    "       \n",
    "        super(Proximity_Metric_Based_On_Corrector, self).__init__()\n",
    "        self.Ss = [S0, S1, S2, S3, S4, S5, S6]\n",
    "        self.feats = [1, feats_S1, feats_S2, feats_S3, feats_S4, feats_S5, feats_S6]\n",
    "        self.out_fc1 = out_fc1\n",
    "        self.out_fc2 = out_fc2\n",
    "        # in is [batch_size, 1, S0, S0]\n",
    "        self.conv_S01 = nn.Conv2d(in_channels=1, out_channels=feats_S1, \n",
    "                               kernel_size = S0-S1+1, bias=True) \n",
    "        # out conv_S01 [batch_size, feats_S1, S1, S1]\n",
    "        self.conv_S12 = nn.Conv2d(in_channels=feats_S1, out_channels=feats_S2, \n",
    "                               kernel_size = S1-S2+1, bias=True) \n",
    "        # out conv_S12 [batch_size, feats_S2, S2, S2]\n",
    "        self.conv_S23 = nn.Conv2d(in_channels=feats_S2, out_channels=feats_S3, \n",
    "                               kernel_size = S2-S3+1, bias=True) \n",
    "        # out conv_S23 [batch_size, feats_S3, S3, S3]\n",
    "        \n",
    "        self.conv_S33 = nn.Conv2d(in_channels=feats_S3, out_channels=feats_S3, \n",
    "                               kernel_size = 1, bias=True) \n",
    "        # out conv_S33 [batch_size, feats_S3, S3, S3]\n",
    "        \n",
    "        self.conv_S34 = nn.Conv2d(in_channels=feats_S3, out_channels=feats_S4, \n",
    "                               kernel_size = S3-S4+1, bias=True) \n",
    "        # out conv_S34 [batch_size, feats_S4, S4, S4]\n",
    "        \n",
    "        self.conv_S45 = nn.Conv2d(in_channels=feats_S4, out_channels=feats_S5, \n",
    "                               kernel_size = S4-S5+1, bias=True) \n",
    "        # out conv_S45 [batch_size, feats_S5, S5, S5]\n",
    "        self.conv_S56 = nn.Conv2d(in_channels=feats_S5, out_channels=feats_S6, \n",
    "                               kernel_size = S5-S6+1, bias=True) \n",
    "        # out conv_S56 [batch_size, feats_S6, S6, S6]\n",
    "        \n",
    "        self.in_fc1 = S6*S6*feats_S6\n",
    "        self.fc1 = nn.Linear(in_features=self.in_fc1, out_features=out_fc1, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=out_fc1, out_features=out_fc2, bias=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_p, inplace=False)\n",
    "        self.relu = torch.nn.functional.leaky_relu\n",
    "\n",
    "        self.batchNorm1 = nn.BatchNorm2d(num_features=feats_S3)\n",
    "        self.batchNorm2 = nn.BatchNorm1d(num_features=out_fc1)\n",
    "        \n",
    "\n",
    "    def forward(self, x): # [batch_size, 2X+1, 2X+1] or [batch_size, 1, 2X+1, 2X+1]\n",
    "        x = x.view(x.shape[0], 1, x.shape[-2], x.shape[-1]).float() # [batch_size, 1, 2X+1, 2X+1]\n",
    "        # Normalize to unity the float image\n",
    "        x = x/x.amax(dim=(2,3), keepdim=True)[0] # [batch_size, 1, 2X+1, 2X+1]\n",
    "        \n",
    "        # Conv layers\n",
    "        x = self.relu(self.conv_S01(x)) # [batch_size, feats_S1, S1, S1]\n",
    "        x = self.dropout( self.relu(self.conv_S12(x)) ) # [batch_size, feats_S2, S2, S2]\n",
    "        x = self.relu(self.conv_S23(x)) # [batch_size, feats_S3, S3, S3]\n",
    "        x = self.batchNorm1(self.relu(self.conv_S33(x)))\n",
    "        x = self.relu(self.conv_S34(x))\n",
    "        x = self.relu(self.conv_S45(x))\n",
    "        x = self.relu(self.conv_S56(x))\n",
    "        \n",
    "        x = x.view(x.shape[0], self.in_fc1)\n",
    "        x = self.dropout( self.relu(self.batchNorm2(self.fc1(self.dropout(x)))) )\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_NN_encoder = {'X':302, 'feats_1':20, 'feats_2':20, 'feats_3':20, 'feats_4':5, 'prop1':2.5, 'prop2':1.5,\n",
    "                  'prop3':0.6, 'av_pool1_div':2, 'conv4_feat_size':8, 'av_pool2_div':10, 'out_fc_1':5,\n",
    "                  'dropout_p1':0.2, 'dropout_p2':0.1, 'out_fc2':10} # out_fc_2 is the output dim of the embedding space\n",
    "args_NN_denoiser = {'X':302, 'S0':2*302+1, 'S1':2*250+1, 'S2':2*200+1, 'S3':2*150+1, 'S4':2*10+1,\n",
    "                    'S5':2*1+1, 'S6':2, 'feats_S1':5, 'feats_S2':5, 'feats_S3':10, 'feats_S4':20,\n",
    "                    'feats_S5':20, 'feats_S6':25, 'out_fc1':100, 'dropout_p':0.1, 'out_fc_2':10}\n",
    "\n",
    "saved_NN_path=f\"/home/oiangu/Hippocampus/Conical_Refraction_Polarimeter/OUTPUT/LIBRARIES_OF_THEORETICAL_D/Basler_like_R0_300x_w0_300x_Z_50x_64bit/SIMULATIONS/Proximity_Metric/\"\n",
    "\n",
    "check_file=\"BEEEST1_BEST_Model_and_Optimizer_2022-05-03 17:24:03.205978_Proximity_Metric_from_Simple_Encoder_Batch_Hard_Soft_Margin.pt\"\n",
    "checkpoint = torch.load(saved_NN_path+f\"/NNs/{check_file}\")\n",
    "\n",
    "class Triplet_NN_embedder(): # INPUT DATA IS ASSUMED TO BE NUMPY\n",
    "    def __init__(self, args_NN_embedder, checkpoint_path, device, batch_size=200,\n",
    "                 encoder_or_denoiser_based=\"encoder\", output_to=\"numpy\"):\n",
    "        self.device = device\n",
    "        self.output_to = output_to\n",
    "        self.args_NN_embedder = args_NN_embedder\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.iX = args_NN_embedder['X']\n",
    "        self.out_dim = args_NN_embedder['out_fc2']\n",
    "        if encoder_or_denoiser_based==\"encoder\":\n",
    "            self.model = Proximity_Metric_Based_On_Simple_Encoder( X=args_NN_embedder['X'], \n",
    "                feats_1=args_NN_embedder['feats_1'], feats_2=args_NN_embedder['feats_2'], \n",
    "                feats_3=args_NN_embedder['feats_3'], feats_4=args_NN_embedder['feats_4'],\n",
    "                 prop1=args_NN_embedder['prop1'], prop2=args_NN_embedder['prop2'], prop3=args_NN_embedder['prop3'], \n",
    "                av_pool1_div=args_NN_embedder['av_pool1_div'], conv4_feat_size=args_NN_embedder['conv4_feat_size'], \n",
    "                av_pool2_div=args_NN_embedder['av_pool2_div'], \n",
    "                 out_fc_1=args_NN_embedder['out_fc_1'], out_fc_2=args_NN_embedder['out_fc2'],\n",
    "                 dropout_p1=args_NN_embedder['dropout_p1'], dropout_p2=args_NN_embedder['dropout_p2'] )\n",
    "        else:\n",
    "            self.model = Proximity_Metric_Based_On_Corrector(S0=S0, S1=S1, S2=S2, S3=S3, S4=S4, S5=S5, S6=S6,\n",
    "                 feats_S1=feats_S1, feats_S2=feats_S2, feats_S3=feats_S3, feats_S4=feats_S4,\n",
    "                 feats_S5=feats_S5, feats_S6=feats_S6,\n",
    "                 out_fc1=out_fc1, out_fc2=out_fc2,\n",
    "                 dropout_p=dropout_p ) \n",
    "        \n",
    "        self.preprocess = lambda X: (torch.tensor(X.reshape(X.shape[0],self.iX*2+1,self.iX*2+1)).to(device)) if len(X.shape)<3 else (torch.tensor(X).to(device))\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # move model to gpu if available\n",
    "        self.model.to(device)\n",
    "        self.model.load_state_dict(checkpoint['model'])\n",
    "        self.model.eval()\n",
    "    \n",
    "    @torch.no_grad() \n",
    "    def __call__(self, X):\n",
    "        self.model.eval()\n",
    "        if self.output_to==\"numpy\":\n",
    "            Xout = np.zeros((X.shape[0], self.out_dim), dtype=np.float64)\n",
    "            for j in range(0, X.shape[0], self.batch_size):\n",
    "                Xout[j:(j+self.batch_size)] = self.model(self.preprocess(X[j:(j+self.batch_size)])).detach().to('cpu').numpy()\n",
    "            return Xout\n",
    "        else:\n",
    "            return self.model(self.preprocess(X))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train KNN for CNN embedder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_embedder = Triplet_NN_embedder( args_NN_encoder, \n",
    "                checkpoint_path=save_stuff_path+check_file, \n",
    "                device=device, encoder_or_denoiser_based=\"encoder\", output_to=\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_regressor = KNN_Regressor( -1, args_sk_KNN) # se introduzca el embedder en el código final, porke sino pesaría gigas este objeto en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb = triplet_embedder(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free()\n",
    "knn_regressor.fit(  X_emb, y, already_embedded_X=True)\n",
    "free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_emb = triplet_embedder(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn_regressor.score( X_test_emb, y_test, already_embedded_X=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = knn_regressor.predict(X_test_emb, already_embedded_X=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean(np.abs(y_pred-y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%d_%m_%Y_%Hh%Mm%Ss\")\n",
    "f_name_knn = f'Triplet_CNN_KNN_n_images_{num_images}_emb_dims_{10}_seed_{random_seed}_date_{date}.sav'\n",
    "with open(save_stuff_path+f_name_knn, 'wb') as f:\n",
    "    dill.dump(knn_regressor, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets see generated embedding space after PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "free()\n",
    "# for plotting purposes\n",
    "df = pd.DataFrame({'y':y})\n",
    "df['phiCR'] = df[\"y\"].astype(str)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "principalComponents = pca.fit_transform(X_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bokeh and px see 2d PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig=px.scatter(np.concatenate((principalComponents, y[:,np.newaxis]), 1), x=0, y=1, color=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"{args['exp']} -> PCA to 2D\",\n",
    "    xaxis_title=\"Principal component 1\",\n",
    "    yaxis_title=\"Principal component 2\",\n",
    "    coloraxis_colorbar=dict(title=\"phi CR\",)\n",
    "    )\n",
    "fig.update_traces(marker={\"size\":5})\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "\n",
    "fig.write_image(f\"{save_stuff_path}/{f_name_emb}_2d.png\")\n",
    "\n",
    "fig.show()\n",
    "free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, LinearColorMapper\n",
    "from bokeh.palettes import Spectral10\n",
    "import plotly.express as px\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "def embeddable_image(data):\n",
    "    img_data = data.values.reshape(X21,X21).astype(np.uint8)\n",
    "    image = Image.fromarray(img_data, mode='L').resize((64, 64), Image.BICUBIC)\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format='png')\n",
    "    for_encoding = buffer.getvalue()\n",
    "    return 'data:image/png;base64,' + base64.b64encode(for_encoding).decode()\n",
    "\n",
    "df['image'] = pd.DataFrame(data=X, columns=list(range(X.shape[1]))).apply(embeddable_image, axis=1)\n",
    "\n",
    "datasource = ColumnDataSource(df)\n",
    "color_mapping = LinearColorMapper(\n",
    "    palette='Magma256',\n",
    "    low=y.min(),\n",
    "    high=y.max()\n",
    ")\n",
    "\n",
    "plot_figure = figure(\n",
    "    title='PCA projection of the CR dataset',\n",
    "    plot_width=800,\n",
    "    plot_height=800,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    ")\n",
    "\n",
    "plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "    </div>\n",
    "    <div>\n",
    "        <span style='font-size: 16px; color: #224499'>phiCR:</span>\n",
    "        <span style='font-size: 18px'>@phiCR</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plot_figure.circle(\n",
    "    'principal component 1',\n",
    "    'principal component 2',\n",
    "    source=datasource,\n",
    "    color=dict(field='phiCR', transform=color_mapping),\n",
    "    line_alpha=0.6,\n",
    "    fill_alpha=0.6,\n",
    "    size=7\n",
    ")\n",
    "show(plot_figure)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now 3d PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "free()\n",
    "principalComponents = pca.fit_transform(X_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter_3d(np.concatenate((principalComponents, y[:,np.newaxis]), 1), x=0, y=1, z=2, color=3)\n",
    "fig.update_layout(\n",
    "    title=f\"{args['exp']} -> PCA to 3D\",\n",
    "    scene = dict(\n",
    "                    xaxis_title='princ. comp 1',\n",
    "                    yaxis_title='princ. comp 2',\n",
    "                    zaxis_title='princ. comp 3'),\n",
    "    coloraxis_colorbar=dict(title=\"phi CR\",)\n",
    "    )\n",
    "\n",
    "fig.update_traces(marker={\"size\":3})\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "\n",
    "fig.write_image(f\"{save_stuff_path}/{f_name_knn}_NN.png\")\n",
    "\n",
    "fig.show()\n",
    "free()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
